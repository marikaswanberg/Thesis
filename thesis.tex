 % This is the Reed College LaTeX thesis template. Most of the work 
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a 
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment. 
% They won't show up in the document, and are useful for notes 
% to yourself and explaining commands. 
% Commenting also removes a line from the document; 
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in 
% the 2002-2003 Senior Handbook. Ask a librarian to check the 
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}


% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace} 
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
\usepackage{imakeidx}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{mathtools}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

\usepackage{amssymb}
\usepackage{xspace}
\usepackage{color}
\usepackage[total={6in,8in}]{geometry}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{centernot}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% theorem, definition, remark, and example formatting
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}

%Commands
\newlength{\arrow}
\settowidth{\arrow}{\scriptsize$k-\mathcal{PQ}$}
\newcommand*{\myrightarrow}[1]{\xrightarrow{\mathmakebox[\arrow]{#1}}}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\ketz}{\ensuremath{\lvert 0\rangle}\xspace}
\newcommand{\keto}{\ensuremath{\lvert 1\rangle}\xspace}
\newcommand{\ket}[1]{\ensuremath{\lvert #1\rangle}\xspace}
\newcommand{\ketpsi}{\ensuremath{|\psi\rangle}\xspace}
\newcommand{\bra}[1]{\ensuremath{\langle #1\vert}\xspace}
\newcommand{\Hplus}{\ensuremath{\lvert + \rangle}\xspace}
\newcommand{\Hminus}{\ensuremath{\lvert- \rangle}\xspace}
\newcommand{\inner}[2]{\ensuremath{\langle #1 \mid #2 \rangle}\xspace}
\newcommand{\suchthat}{\;\ifnum\currentgrouptype=16 \middle\fi|\;}
\newlength{\minuslength}
\settowidth{\minuslength}{$-$}
\newcommand{\hadamard}{
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & \hspace{\minuslength}1\\
1 & -1 
\end{bmatrix}
}

\input{notation.tex}

\title{Noisy Quantum Oracles: A Study of Algorithmic Robustness}
\author{Marika Swanberg}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2019}
\division{Mathematics and Natural Sciences}
\advisor{James Pommersheim}
%If you have two advisors for some reason, you can use the following
%\altadvisor{Adam Groce}
%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%%
\makeindex
%% End Preamble
%%
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
 %   \chapter*{Acknowledgements}

	

% The preface is optional
% To remove it, comment it out or delete it.
   \chapter*{Preface}
   \emph{``It's not a Turing machine, but a machine of a different kind''} \citep{feynman1982simulating}. Quantum computing was proposed by Richard Feynman in 1981 for simulating quantum physical phenomena. He was the first to suggest that a computer based upon quantum mechanics could potentially have a computational advantage to classical computers. 

This idea became relevant to mainstream computer science theorists in 1994 when Peter Shor challenged the Extended Church-Turing thesis\footnote{The Extended Church-Turing thesis (previously called the quantitative Church-Turing thesis) is stated by Shor as follows: \emph{any physical computing device can be simulated by a Turing machine in a number of steps polynomial in the resources used by the computing device }\citep{shor1999polynomial}. In most cases, ``resources'' refers to either time (CPU cycles) or space (memory).} which they held so dearly. He proposed a quantum algorithm that could compute the prime factors of integers in polynomial time, a large speedup compared to classical computers \citep{shor1999polynomial}. This algorithm demonstrated the great potential of quantum computation and spurred on the research community.

Although Shor's algorithm was provocative \emph{in theory}, actual implementations of quantum computers lagged behind. In 2001, scientists published their success at physically implementing Shor's algorithm to compute the prime factors of the number 15  \citep{experimentalshor2001}. Clearly, there was a large gap to bridge between theory and practice. 

Noise and inaccuracy are still among the largest obstacles to making quantum computation feasible. Quantum physical systems are plagued by quantum decoherence, the tendency of particles to be influenced by their environment and thus be difficult to preserve properly. In this thesis, we study error-correcting codes and the robustness of quantum algorithms to noise. Managing errors and noise will be key to realizing the full potential of quantum computing.
 
 

    \tableofcontents
% if you want a list of tables, optional
 %   \listoftables
% if you want a list of figures, also optional
%    \listoffigures

% The abstract is not required if you're writing a creative thesis (but aren't they all?)
% If your abstract is longer than a page, there may be a formatting issue.
  \chapter*{Abstract}
This thesis is an analysis of the query complexity of quantum decoding algorithms for geometric error-correcting codes. Many quantum decoding algorithms for error-correcting codes have already been devised, and we study the robustness of such algorithms to noisy or unreliable oracles, given a fixed query complexity. In chapter 4, we define one type of noisy quantum oracle and proceed with an original analysis of the quantum decoding algorithm for the binary simplex code. The remainder of the thesis is dedicated to exploring and reinterpreting a class of generalized simplex codes, called Reed-Muller codes, in two ways: as finite geometries and as polynomials. Chapter 7, the main result, presents the transformation of Reed-Muller decoding into multivariate polynomial interpolation and the  successful simplification of a generalized quantum multivariate polynomial interpolation algorithm for decoding Reed-Muller codes. 


\chapter*{Dedication}
\textit{Till Carina,} 

\textit{min allra k\"{a}raste syster.}


  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

%The \introduction command is provided as a convenience.
%if you want special chapter formatting, you'll probably want to avoid using it altogether

	
\chapter{Quantum Computing}


In the following chapter, we will delve into the basics of quantum computing, much of which is taken from \cite{nielsenchuang} and  \cite{de2001quantum} \footnote{Eager students of quantum computation should undoubtedly look to \cite{nielsenchuang} for an in-depth treatment of the subject.}.  Before proceeding, it is important to realize that quantum computing describes a  fundamentally different \textit{model of computation} than the computer systems currently on the market. Much like the first programmable computer was realized by Alan Turing far before anyone built his remarkable invention, we too study quantum computing at a time when only the most rudimentary quantum computers are available in practice.


Due to the complex nature of quantum mechanics, there are a plethora of misconceptions about quantum computing. One of the most widespread is that quantum computers gain computational speedups by \emph{trying all possible solutions at once}. This is simply not true; the state of the quantum bits may be unknown, but they are still in only one state. We will go into this more later. Perhaps my favorite that I have heard upon starting my thesis is that \emph{quantum computers are like classical computers but in trinary}. False--we cannot efficiently simulate a quantum computer on a classical one. Lastly, probably the most optimistic is that \emph{quantum computers are faster at all tasks compared to classical computers}. Quantum computers are faster than classical computers at some, \emph{but not all}, computations.

I outline these common misconceptions not to criticize them, but rather to encourage the reader to exert patience and care with the following section, as quantum computing is so fundamentally different from classical computing.

The quantum mechanical properties upon which quantum computers are based have been studied extensively by physicists; we will avoid discussing such details and instead take these properties for granted in order to focus on the information-theoretic behavior of this new model of computation. 

\section{Information Representation}
Classical computers, i.e.~the computers that we all know and love, run on \textit{bits} or 1's and 0's. This is the fundamental unit of information in classical computers. In quantum computers, information is built upon an analagous concept, the \textit{quantum bit} or \textit{qubit}. We will discuss the defining properties of qubits\index{qubit}, some of which may seem very different from those of bits.

In classical computing, each bit can be in one of two states--1 or 0.  We denote \textit{quantum states} by \ketz and \keto, pronounced ``\textit{ket zero}'' and ``\textit{ket one},'' respectively. This follows traditional \textit{Dirac notation}. Unlike classical bits, qubits can be in a \textit{superposition} between these two states. That is, a qubit can lie in one of the infinite states ``between'' \ketz and \keto. We describe a quantum state as follows: $\ketpsi = \alpha \ketz + \beta \keto$, where $\alpha$ and $\beta$ are complex numbers and $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1$. This describes a probability distribution over the quantum states \ketz and \keto with \textit{amplitudes} $\alpha$ and $\beta$ that each take values from $\C$, the complex field. In this way, we can think of \ketpsi as a vector in $\C^2$. The states \ketz and \keto form an orthonomal basis in this complex vector space and are called the \textit{computational basis states}.
\begin{definition} (Qubit) A qubit is a unit vector of $\C^2$, denoted
\begin{equation*}
\ket{\psi} = \alpha\ketz + \beta\keto 
\end{equation*}
where $\alpha, \beta \in \C$ and $\lvert \alpha \rvert^2 + \lvert \beta \rvert^2 = 1$. Equivalently, $\ket{\psi}$ can also be represented as 
\begin{equation*}
\ket{\psi} = 
\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix}.
\end{equation*}
\end{definition}
So how can we know which state a bit or qubit is in? In classical computing, we can simply read the bit to determine whether it's a 0 or a 1. Qubits are a little trickier. We cannot read \ketpsi to determine its exact amplitudes, $\alpha$ and $\beta$. As soon as we measure \ketpsi, the superposition will collapse to either \ketz with probability $ \lvert \alpha \rvert ^2$ or \keto with probability $ \lvert \beta \rvert ^2$. As soon as \ketpsi has been measured to reveal some quantum state, \ketpsi will permanently collapse to that state, i.e.~$\ketpsi = 1\ketz + 0 \keto$ or $\ketpsi = 0\ketz + 1\keto$. Every time we measure it thereafter, we will observe the same state that we measured the first time.

\begin{remark}
Suppose you are given a weighted coin and you are asked to determine, through measurement, the weights on heads and tails. Obviously, with enough flips, the law of large numbers states that your measurements would approach the true weights. Now, suppose you are only allowed to flip the coin once. The task of determining the weights becomes impossible. This is why we cannot measure the amplitudes of qubits only through measurement.
\end{remark}
You may be wondering why quantum phenomenon are useful in the context of computation. It seems impossible to build a model of computation on a fundamental unit of information that is unknowable, immeasurable. The beauty of quantum computation lies in the \textit{manipulation} of these immeasurable qubits such that by the time we measure them at the end, the result will inform us of the state they started in. Simply measuring the qubits before doing any transformations is fundamentally the same as  running a random number generator on a classical computer and trying a random possible solution. Obviously, this is not very useful, so we must \textit{transform} the qubits to say anything intelligent about the end result that we measure.

\section{Quantum Logic Gates}

As we discussed earlier, $\ketz = 1 \ketz + 0  \keto$ and $\keto = 0 \ketz + 1 \keto$. We sometimes represent these quantum states as column vectors of the amplitudes: 
\begin{align*}
\ketz = \begin{bmatrix}
1\\
0
\end{bmatrix}
\text{ and }
\keto = \begin{bmatrix}
0\\
1
\end{bmatrix}
\end{align*}

Thus, we can represent a single-qubit transformation by a 2-by-2 matrix where the first column is the image of \ketz and the second column is the image of \keto under the transformation. For example, the quantum NOT gate can be realized as a matrix:
\begin{align*}
X = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\end{align*}
This takes a state $\ketpsi = \alpha \ketz + \beta \keto$ and transforms it into $X \ketpsi  = \beta \ketz + \alpha \keto$. Equivalently, by matrix multiplication we see that
\begin{align*}
X \ketpsi = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix} 
= 
\begin{bmatrix}
\beta \\
\alpha
\end{bmatrix}.
\end{align*}
So, any transformation on a single qubit can be represented by a 2-by-2 matrix. What about the transformation that, when given \ketz or \keto, outputs a state that is in \textit{equal superposition} between \ketz and \keto ? By this, we mean that there is a 1/2 probability of measuring \ketz or \keto. This would essentially give us a fair coin. We might represent that as follows:
\begin{align*}
\widehat{H} = \begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
There are a few problems with this transformation. First, applying this transformation, we get $\widehat{H} \ketz = 1\ketz + 1\keto$, which means that $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1^2 + 1^2 \neq 1.$ Since we view a quantum state as a probability distribution, the squares of the amplitudes must sum to 1. So, we must \textit{normalize} the matrix, to get 
\begin{align*}
\widehat{H} = 
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
This transformation still isn't \textit{unitary}, meaning it doesn't preserve the magnitude of the input vector. For example, applying this transformation to the state $\ket{\phi} =( \ketz - \keto)/\sqrt{2}$ gives us $\widehat{H}\ket{\phi} =  0\ketz + 0 \keto$. This is not a valid state, because the squares of the amplitudes do not add to 1. So, we must fix this transformation to the following:

\begin{align*}
H = \hadamard
\end{align*}

The quantum gate above ($H$) is called the \textit{Hadamard transform} or \textit{Hadamard gate}. We will precisely define this quantum gate later in this chapter, as it is absolutely critical for work in quantum algorithms. $H$ acts on the basis vectors as follows: $H\ketz = (\ketz + \keto)/\sqrt{2}$ and $H\keto = (\ketz - \keto)/\sqrt{2}$. 

Both of these states are in an equal superposition between \ketz and \keto, meaning that for both states, there is an equal probability (1/2) of measuring \ketz and \keto; however, they are still distinct states. These states come up rather often, so they have been given the special shorthand notations \Hplus and \Hminus, respectively. 

\begin{definition}[Plus and Minus States] The following states (pronounced ``plus'' and ``minus'') will be used ubiquitously throughout this thesis in their shorthand form:
$$\Hplus = \frac{1}{\sqrt{2}}(\ketz + \keto)$$
$$\Hminus = \frac{1}{\sqrt{2}}(\ketz - \keto)$$
\end{definition}
The requirements that quantum transformations be reversible and normalized are encapsulated by the property that the matrix representation for any transformation must be \textit{unitary}. That is, $U^{\dagger} U = I$ where $U^{\dagger}$ is the transpose of the complex conjugate of $U$ and $I$ is the 2-by-2 identity matrix. Within those requirements, we can construct any transformation we like. What about transforming multiple qubits?

\section{Multiple Qubits}

As we discussed, single qubits live in $\C^2$. In order to express a superposition over multiple qubits, say $n$ qubits, we need to take some \emph{tensor products}. An $n$-qubit state can be represented as vector in $(\C^2)^{\otimes n} \cong \C^{2^n}$. For example, in a two qubit system, we have the following basis states:
$$ \ketz \otimes \ketz,~\ketz \otimes \keto,~\keto \otimes \ketz, \text{ and } \keto \otimes \keto.$$ 
All two-qubit states can be thought of as a vector in $\C^2 \otimes \C^2 = \C^4$. With that in mind, the four basis states above can also be written as\footnote{In this thesis, the following notations are equivalent: $\ket{xy}, \ket{x, y},$ and $ \ket{x} \otimes \ket{y}$.}:
$$ \ket{00},~\ket{01},~\ket{10}, \text{ and } \ket{11}.$$
Thus, any two-qubit state can be represented as a linear combination of these basis states, namely $\ket{\psi} = \alpha_{00}\ket{00}+ \alpha_{01}\ket{01} + \alpha_{10}\ket{10} + \alpha_{11}\ket{11}.$ In general, an $n$-qubit state can be represented as a linear combination of basis states, 
\begin{equation*}
\ket{\psi} = \sum_{i \in \Z_2^n} \alpha_{i}\ket{i}, \quad \alpha_i \in \C
\end{equation*}
where
\begin{equation*}
\sum_{i \in \Z_2^n} \lvert \alpha_{i}\rvert^2 = 1.
\end{equation*}
The probability of observing a state \ket{i} upon measuring \ket{\psi} is $\lvert \alpha_{i} \rvert ^2.$ 

We may measure qubits individually as well. For example, measuring just the first qubit gives us 0 with probability 
\begin{equation*}
 \sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2,
\end{equation*}
leaving the post-measurement state
\begin{equation} \label{post_measurement}
 \ket{\psi'} = \frac{\sum_{i \in \Z_2^{n-1}} \alpha_{0i} \ket{\alpha_{0i}}}{\sqrt{\sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2}}.
\end{equation}

\section{Entangled States}

Something that follows from the post-measurement equation (\ref{post_measurement}) but which is not self-evident is the concept of \textit{entangled states}. Consider the following state:
\begin{equation*}
\ket{\beta_{00}} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}
\end{equation*}
Suppose we measured the first qubit. We will observe \ket{0} with probability 1/2, and the resulting post-measurement state is: \ket{\beta_{00}'} = \ket{00} (by \ref{post_measurement}). How can this be? We only measured the first qubit, and yet, we now have information about both qubits. This is precisely because the state is entangled. Another example of an entangled state is:
\begin{equation*}
\ket{\beta_{01}} = \frac{\ket{01} + \ket{10}}{\sqrt{2}}
\end{equation*}
Because these two states famously demonstrate quantum entanglement, \ket{\beta_{00}} and \ket{\beta_{01}}, are called the first two \textit{Bell states} or \textit{EPR pairs}\footnote{There are four EPR pairs or Bell states, the four most basic examples of quantum entanglement.}. Quantum entanglement is a powerful computational tool and will be used in many quantum algorithms in the rest of the text. 

Now that we have the means to represent an $n$-qubit state, the state transformations can be represented by $2^n$-by-$2^n$ matrices. One particularly useful transformation is the $n$-qubit Hadamard transform.

\section{Hadamard Transform}

As promised, a more precise definition of the Hadamard transform follows. Note that $H^{\otimes n} = H \otimes H \otimes \ldots \otimes H, n$ tensor products with itself. 

\begin{definition}[Hadamard Transform] The $n$-qubit Hadamard transform, $H^{\otimes n}$ acts on a basis state $\ket{k}$ as follows:
\begin{equation*}
H^{\otimes n} \ket{k} = \frac{1}{\sqrt{2^n}} \sum_{j \in \Z_2^n} (-1)^{k \cdot j} \ket{j}.
\end{equation*}
\end{definition}
Upon first glance, this formula is quite uninviting, so I will provide a quick guide for some common states. 
\begin{align*}
& H^{\otimes n} \ketz^{\otimes n} = \Hplus^{\otimes n} \\
& H^{\otimes n} \keto^{\otimes n} = \Hminus^{\otimes n} \\
\end{align*}
Since $H = H^{-1}$, we also have that
\begin{align*}
& H^{\otimes n} \Hplus^{\otimes n} =  \ketz^{\otimes n} \\
& H^{\otimes n} \Hminus^{\otimes n} = \keto^{\otimes n}. \\
\end{align*}
This concludes the section on the Hadamard transform, though the careful (or confused) reader will find that they will have to refer back to this section in the coming chapters as the Hadamard transform is central to this thesis.

\section{Measuring in Other Bases}
Thus far, all of the measurements we have performed have been done in the \textit{computational basis}. Given a state $\ket{\psi} = \alpha\ketz + \beta\keto$, upon measurement in the computational basis $\{\ketz, \keto\}$ this state will collapse to $\ketz$ with probability $\lvert \alpha \rvert^2$ and $\keto$ with probability  $\lvert \beta \rvert^2.$ But, this choice of basis is arbitrary. We could instead measure $\ket{\psi}$ in the Hadamard basis, $\Hplus$ and $\Hminus$. Then, we can re-express the state $\ket{\psi}$ as follows
\begin{equation*}
\ket{\psi} = \alpha\ketz + \beta\keto = \alpha \frac{\Hplus + \Hminus}{\sqrt{2}} + \beta \frac{\Hplus - \Hminus}{\sqrt{2}} = \frac{\alpha + \beta}{\sqrt{2}} \Hplus + \frac{\alpha - \beta}{\sqrt{2}} \Hminus.
\end{equation*}
So, measuring $\ket{\psi}$ in the Hadamard basis yields $\Hplus$ with probability $\lvert \alpha + \beta \rvert^2/2$ and $\Hminus$ with probability $\lvert\alpha - \beta\rvert^2/2$.

More generally, given an orthonormal basis $\{\ket{a}, \ket{b}\}$, we can represent any state $\ket{\psi}$ as a unique linear combination of this basis, $\ket{\psi} = \alpha \ket{a}+ \beta \ket{b}$.Then, the probabilities of measuring $\ket{a}$ and $\ket{b}$ are $\lvert \alpha\rvert^2$ and $\lvert\beta\rvert^2$, respectively.
 

\section{Phase}
The term \textit{phase} has multiple meanings in quantum computation, so we will flesh these out. In fact, some of the original results in this thesis puzzled both student and adviser until we revisited the exact distinction between \textit{global phase} and \textit{relative phase}.

Consider the states $\ket{\psi}$ and $e^{i \theta}\ket{\psi}$, where $\ket{\psi}$ is a state vector and $\theta$ is a real number. These two states are equivalent up to their \textit{global phase shift}. This just means that the probability of observing some basis state $\ket{k}$ under measurement in any basis is the same for both states. Observationally, they are identical states, so we can ignore global phase shifts. 

The other notion of phase that will come up is \textit{relative phase}. Consider the states $\Hplus = (\ketz + \keto)/\sqrt{2}$ and $\Hminus = (\ketz- \keto)/\sqrt{2}$. These two states have amplitudes on basis state $\keto$ equal to $1/\sqrt{2}$ and $-1/\sqrt{2}$, respectively. So, although their magnitudes are equal, their signs are not. Now, observing both of these in the computational basis yields $\ketz$ with probability $1/2$ and $\keto$ with probability $1/2$ for both states. In the computational basis they are indistinguishable; however, measuring in the \textit{Hadamard basis}, with basis states $\Hplus$ and $\Hminus$, they are distinguishable with probability 1. Despite having the same relative phase (meaning that the magnitudes of each of their amplitudes are equal), the states are in fact computationally distinct.



\section{Quantum Oracles}

Within any computational model, some computations are ``expensive'' for any number of reasons: they require large amounts of resources such as time, space, or circuitry. For this reason, one may wish to outsource such computations to third parties, which we will call \textit{oracles}. As the name would suggest, oracles may be queried on inputs, and magically in one time step will output the result of the computation on the input, for a particular function. More concretely, an oracle $\mathcal{O}_f$ outputs $f(x)$ when queried on the input $x$.

Classical oracles are used throughout computer science theory to abstract computations and reason about algorithms and protocols. Quantum oracles differ  from classical oracles in significant ways. 

\subsection{Quantum Queries}
First, consider the action of the query. Suppose the function operates on the following spaces $f: \Z_2^n \rightarrow \Z_2^m$, and oracle $\mathcal{O}_f$ computes $f$. Clearly, if $n \neq m$, this function could not possibly be bijective. In order to ensure the reversibility of queries to $\mathcal{O}_f$, we must maintain two registers: a query register, and a response register. The query register contains the input $x$ on which we wish to query the oracle and remains unchanged by the query. The response register is initialized to some known basis state $\ket{r} \in \Z_2^m$. After the quantum query takes place, the oracle records the response by taking the bitwise exclusive OR, or addition modulo two of $f(x)$ and $r$. Thus, the response register has the post-query state $\ket{r\oplus f(x)}$. 

\begin{definition}[Quantum Oracle Query] Let $f: \Z_2^n \rightarrow \Z_2^m$ be a function. Then, the quantum oracle $\mathcal{O}_f : \Z_2^n \otimes \Z_2^m \rightarrow \Z_2^n \otimes \Z_2^m$ that computes $f$ has the following action on the query and response registers 
\begin{equation*}
\mathcal{O}_f : \ket{x, r} \rightarrow \ket{x, r \oplus f(x)}.
\end{equation*}
\end{definition}
In addition to the query and response registers, quantum oracles have the special ability to take as input a \textit{superposition of queries} and output a \textit{superposition of responses}. More precisely, suppose we want to query the oracle on the following valid superposition of points $S \subseteq \Z_2^n$.
\begin{equation*}
\ket{x} = \sum_{s \in S} \alpha_s \ket{s}, \quad \text{ where } \sum_{s \in S} \lvert \alpha_s\rvert^2 = 1.
\end{equation*}
Then, we need to append an $m$-qubit response register to each query register $\ket{s}$  and initialize the response registers to some known value $\ket{r_s}$. The pre-query state becomes
\begin{equation*}
\sum_{\substack{s \in S \\ r_s \in \Z_2^m}} \alpha_s \ket{s, r_s} .
\end{equation*}
Then, the query $\mathcal{O}_f:  \Z_2^n \otimes \Z_2^m \rightarrow \Z_2^n \otimes \Z_2^m$ produces the following unitary transformation
\begin{equation*}
\sum_{\substack{s \in S \\ r_s \in \Z_2^m}} \alpha_s \ket{s, r_s}  \xrightarrow{\mathcal{O}_f} \sum_{\substack{s \in S \\ r_s \in \Z_2^m}} \alpha_s \ket{s, r_s \oplus f(s)}.
\end{equation*}
It is important to note that this only constitutes \textit{one} quantum query to the oracle.


\section{Phase-kickback Trick}
 One common query method is using what is called the \textit{phase kickback trick}. The basic idea is that we initialize the response register to \Hminus so that both the query and response registers stay the same after the query, and the value of $f(x)$ is encapsulated by the phase of the state. More concretely, we have:
\begin{align*}
\ket{x} \otimes \Hminus 
& = \ket{x} \otimes \frac{1}{\sqrt{2}}(\ketz - \keto)\\
& = \frac{1}{\sqrt{2}}(\ket{x, 0} - \ket{x, 1}) \\
& \xrightarrow{f} \frac{1}{\sqrt{2}}(\ket{x, f(x)} - \ket{x, 1 \oplus f(x)} \\
& = \ket{x} \otimes \frac{1}{\sqrt{2}}\bigg(\ket{f(x)} - \ket{\overline{f(x)}}\bigg) \\
& = (-1)^{f(x)} \ket{x} \otimes \ket{ -}.
\end{align*}
Since the response register remains unchanged, we will generally omit this from future computations, though technically it must be present to preserve the reversibility of the query. 
\section{Bernstein-Vazirani Algorithm}

Now that we have seen a few tricks of the trade, we will dive into a quantum algorithm. The Bernstein-Vazirani algorithm forms the foundation for many of the algorithms in the rest of this thesis, so a solid grasp of this section will yield high dividends. 

The Bernstein-Vazirani algorithm solves the following problem: for $N = 2^n$, we are given a function $f: \Z_2^n \rightarrow \Z_2$ with the property that there exists some unknown $a \in \Z_2^n$ such that $f(i) = i \cdot a \pmod{2}$. The goal is to find $a$. 

First, an overview of the algorithm: we will start in the state $\ketz^{\otimes n}$, the $n$-qubit zero state, and apply a $n$-qubit Hadamard transform to get an equal superposition of the states, i.e.~ $\Hplus^{\otimes n}$. Next, we perform a quantum query using the phase kickback trick to store $f(i)$ in the phase of state $\ket{i}$. Then, another Hadamard transform on all $n$ qubits. With a simple measurement, we obtain $a$ with probability 1. Now, for the details.

\begin{align}
 \ketz^{\otimes n}
& \myrightarrow{H^{\otimes n}}\frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} \ket{i} \\
& \myrightarrow{\mathcal{O}_f} \frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} (-1)^{f(i)}\ket{i}\\
& \myrightarrow{H^{\otimes n}} \frac{1}{2^n} \sum_{i \in \Z_2^n} (-1)^{f(i)} \sum_{j \in \Z_2^n} (-1)^{i \cdot j} \ket{j} \label{confusingline}\\
& = \ket{a}
\end{align}
This computation looks a mess for the beginner, so let's try to make sense of it. The first three lines follow from the definitions of the functions (confused readers may need to review the defined action of $H$ and $\mathcal{O}$ with phase-kickback). The last step, however, is less clear-cut. Note that $(-1)^{f(i)} = (-1)^{(i \cdot a) \mod{2}}= (-1)^{(i \cdot a)}$, given by $f(i) = i \cdot a \pmod{2}$ in the problem statement. Thus, we may manipulate \ref{confusingline} as follows:
\begin{align}
 \frac{1}{2^n} \sum_{i \in \Z_2^n} (-1)^{f(i)} \sum_{j \in \Z_2^n} (-1)^{i \cdot j} \ket{j}
& = \frac{1}{2^n} \sum_{i \in \Z_2^n} (-1)^{i \cdot a} \sum_{j \in \Z_2^n} (-1)^{i \cdot j} \ket{j} \\
& = \frac{1}{2^n}  \sum_{j \in \Z_2^n}\sum_{i \in \Z_2^n} (-1)^{i \cdot(a + j)}  \ket{j} \label{equals_a}
\end{align}
Now, suppose $j=a$. Then, from \ref{equals_a} we have
\begin{align*}
\frac{1}{2^n}  \sum_{i \in \Z_2^n} (-1)^{i \cdot (2a)}  \ket{a} 
& = \frac{1}{2^n}  \sum_{i \in \Z_2^n} (-1)^{(i \cdot \mathbf{0})}  \ket{a} \\
& = \frac{1}{2^n}  \sum_{i \in \Z_2^n} 1 \ket{a}  \\
& = \ket{a}.
\end{align*}
Since the Hadamard transform is unitary, the amplitudes on all states $\ket{j}, j \neq a$ must be zero. So, \ref{equals_a} is simply the state $\ket{a}$; thus, measurement in the computational basis will yield $a$ with probability 1. This algorithm is the bread and butter of this thesis, so to speak, and is critical to the decoding algorithms we will study in a few chapters. 


\section{Advanced Topics}
In this section, we discuss some advanced topics that are not entirely necessary for understanding this thesis, but which are nonetheless vital to understanding quantum computation as a whole. 

\subsection{No-Cloning Theorem}

Suppose my friend has a secret state \ket{\psi} that she wants me to have a copy of.  Classically, we could easily build a circuit to copy her state $x = 0$ or $x = 1$ without measuring it. We would simply initialize a temporary register to 0, and our circuit would write $x \oplus 0$, the XOR of $x$ and 0, to the destination register. 

The quantum case is a bit trickier. Suppose \ket{\psi} is only known to our friend. We wish to copy this exact state into a target slot, which starts out in some standard known state \ket{s}. Thus, the initial state of our copying machine is $\ket{\psi} \otimes \ket{s}$. Now, we apply some unitary operation $U$ to these registers to obtain
\begin{equation}
\ket{\psi} \otimes \ket{s} \xrightarrow{U} U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
Now, suppose this quantum copying circuit works for two arbitrary states \ket{\psi} and \ket{\phi}. Then, we have
\begin{equation} \label{noclone1}
U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
and
\begin{equation} \label{noclone2}
U(\ket{\phi} \otimes \ket{s}) = \ket{\phi} \otimes \ket{\phi}
\end{equation}
Now, taking the inner product of the left-hand sides of equations \ref{noclone1} and \ref{noclone2} gives
\begin{align} 
(\bra{\psi} \otimes \bra{s})U^{\dagger}U (\ket{\phi} \otimes \ket{s})
& = (\bra{\psi} U^\dagger U \ket{\phi})(\bra{s} U^\dagger U \ket{s}) \\
& = \inner{\psi}{\phi} \label{noclone3}
\end{align}
However, taking the inner products of the right-hand sides of equations \ref{noclone1} and \ref{noclone2} gives
\begin{align} \label{noclone4}
(\bra{\psi} \otimes \bra{\psi})(\ket{\phi} \otimes \ket{\phi})
& = (\inner{\psi}{\phi})^2
\end{align}
Now, equations \ref{noclone3} and \ref{noclone4} give
\begin{equation*}
 \inner{\psi}{\phi} = (\inner{\psi}{\phi})^2.
\end{equation*}
However, this equation is only true if $\ket{\psi} = \ket{\phi}$ or if $\ket{\psi}$ is orthogonal to $\ket{\phi}$. Thus, a general cloning device can only clone states that are orthogonal, which means that we cannot clone two arbitrary states.

This is an important fundamental difference between the classical and quantum models of computation. We take for granted in classical computing that we can copy any unknown state, and much of classical computation relies upon this fact. Another fact of classical computing which we take for granted is the existence of universal gates.

\subsection{Universal Quantum Gate}
In classical computation, there are three basic logic gates: NOT, AND, and OR. We can combine these gates to represent any possible logical expression. Furthermore, the NAND gate and the NOR gate, which we get from taking the negation (NOT) of the output of AND and OR respectively, are said to be \textit{universal gates}. This means that we can construct the three basic logic gates just from NAND gates or just from NOR gates. So, NAND and NOR are universal in that any logical expression can be represented with just NAND or NOR circuits. This is very useful in practice because NAND gates are very cheap to construct, so using only NANDs can keep the cost of a computer chip down.

A natural question, then, is whether there exists a quantum analogue, a universal quantum gate. The short answer is that there is no single universal quantum gate, however the following three gates are enough to make an \textit{arbitrarily good approximation} of any quantum gate \citep{nielsenchuang}: Hadamard, CNOT, and phase gate. 

The phase gate is defined
\begin{align}
\begin{bmatrix}
1 & 0\\
0 & i 
\end{bmatrix}
\end{align}
And the CNOT, or controlled-NOT gate acts on two qubits by flipping the second qubit if and only if the first qubit is a 1. 
\begin{align}
\text{CNOT} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}
\end{align}
As you can see, by multiplying the CNOT matrix by the column vector $(a\quad b\quad c\quad d)^\intercal$ this takes $a\ket{00} + b\ket{01} + c\ket{10} + d\ket{11}$ and transforms it into $a\ket{00} + b\ket{01} + c\ket{11} + d\ket{10}$, thereby swapping the amplitudes on the states $\ket{10}$ and $\ket{11}$, or equivalently ``flipping'' the second bit of the state if the first bit is a 1. 

The \textit{gate complexity} of a unitary transformation is the minimum number of gates needed to implement the circuit. 




\chapter{Error-Correcting Codes}	

No real-life communication channel is perfect: internet packets occasionally get dropped, radio transmissions are drowned out by static interference, and data on storage media get corrupted. Distinguishing signal from noise is a problem as old as communication itself. One way of adapting to this problem is introducing redundancy into the transmission. If the information we want to transmit is encoded in more than one place, then there is a higher probability that at least one of the copies will make it through even if some of the copies are plagued with errors. This is precisely the approach that error-correcting codes take. 


Error-correcting codes were developed to enable the reliable transmission of messages over noisy communication channels. There are many examples of their use today, and many error-correcting codes have been developed for specific noise models. In CDs\footnote{If they still exist\ldots}, error-correcting codes prevent (small) scratches from leading to data loss. They are also used for transmitting photographs from rovers and telescopes throughout the solar system to earth. 

Error-correcting codes are important for the practical implementation of quantum computers because current physical implementations of quantum computers are plagued by \textit{quantum decoherence}. That is, the qubits are difficult to fully isolate from their environment and thus become entangled with their surroundings, leading to noisy computations. For now, we will forget about quantum computing and plunge into the code theory. Much of the material in this chapter was paraphrased from \cite{macwilliamssloane}.

\section{Preliminaries}
There are many kinds of communication channels, but we will focus on \textit{binary symmetric channels}. 

\begin{definition}[Binary Symmetric Channel] A binary symmetric channel is a classical channel that can transmit a string of 0's and 1's. If a bit $b$ is sent, $b$ will be flipped to $\lnot b$ with probability $p \in [0,1]$, and $b$ will be transmitted correctly with probability $1-p$ where $p < \frac{1}{2}$.
\end{definition}

This type of channel is \textit{binary} because we are transmitting bits, and it is \textit{symmetric} because there is an equal probability of a 0 flipping to a 1 and the reverse. Note that if our binary symmetric channel flips bits with probability $p > 1/2$, we could easily create a channel with $p <1/2$ by negating every bit on the receiving or sending end. Additionally, note that if $p=1/2$, it is  information-theoretically impossible to recover the message, so we don't consider this case and frankly it is dubious to even call that a communication channel.

In the following sections we will investigate how to mitigate the information loss from random errors in binary symmetric channels where $p<1/2$. In the following chapter, and throughout the thesis, we will use the terms ``noise'' and ``errors'' interchangeably.

\section{Overview of Transmission}

Thus far, we have only described the parts of the communication system in vague terms; now, we will assuage the reader's thirst for clarity and precision. We will, of course, delve further into each of these parts later.

There are five basic components in the communication system we consider. The \textit{message source}, or \textit{sender}, transfers a message $\mess = \m{1} \ldots \m{\mlen}$ to the \textit{encoder}. The \textit{encoder} outputs the \textit{codeword} $\cw = \cwb{1}\ldots \cwb{\clen}$ associated with message \mess and passes it along to the \textit{binary symmetric channel}. The \textit{channel} incorporates an \textit{error vector} $\ev = \evb{1} \ldots \evb{\clen}$ into the codeword and outputs the received vector $\rc = \ev + \cw = \rcb{1} \ldots \rcb{\clen}$ where the addition is bitwise and modulo 2. Then, the \textit{decoder} takes the received vector (also referred to as the ``noisy codeword'') $\rc$ and outputs $\mhat$, an estimate of message \mess to the \textit{receiver}. A schematic is provided below.
\begin{equation*}
\text{Sender} \xrightarrow{\text{Message }\mess} \text{Encoder} \xrightarrow{\text{Codeword }\cw} \text{Channel} \xrightarrow{\text{Rcvd.~Vect.~}\cw+\ev} \text{Decoder} \xrightarrow{\text{Est. }\mhat} \text{Receiver}
\end{equation*}

\section{Encoding}
The job of the encoder is to introduce well-defined redundancy into the transmission. We will describe a general encoding scheme that applies to all linear codes, though the more advanced encoding algorithms we will explore later will deviate from this general form.

In this general code $\mathscr{C}$, the message will be encoded as a codeword that consists of a series of \textit{check symbols} appended to the original message \mess. The check symbols encode redundancy into the codeword. More concretely, the bits of the codeword \cw associated with message \mess are defined
\begin{equation*}
\cwb{1} = \m{1}, \quad \cwb{2} = \m{2}, \quad \ldots,\quad \cwb{\mlen} = \m{\mlen}
\end{equation*}
and the last $\clen-\mlen$ bits of $\cw$ are all check symbols $\cwb{\mlen+1} \ldots \cwb{\clen}$. The check symbols are determined by the codeword, and they are chosen so that 
\begin{equation*}
P \begin{bmatrix}
\cwb{1}\\
\cwb{2}\\
\vdots\\
\cwb{\clen}
\end{bmatrix} = 
P \cw^{\bot}= 0
\end{equation*}
where $P$ is the \textit{parity check matrix} for the code. We denote such a code $\mathscr{C} = [\clen, \mlen]$.

\begin{definition}[Parity Check Matrix] The parity check matrix $P$ for a linear code $\mathscr{C} = [\clen, \mlen]$ is given by 
\begin{equation*}
P = [A \mid I_{\clen-\mlen}]
\end{equation*}
where $A$ is some fixed $(\clen-\mlen) \times \mlen$ binary matrix and $I_{\clen-\mlen}$ is the identity matrix of size $(\clen-\mlen)$.
\end{definition}
A code $\mathscr{C}\subset \Z_2^\clen$ is simply the set of all codewords $\cw$ that satisfy the equation $P\cw^{\bot} = 0$. In other words, $\mathscr{C} = \{c\in \Z_2^n \mid P\mathbf{c}^{\bot} = 0\}$.
\begin{example} \label{P_ex} Suppose our parity check matrix is
\begin{equation*}
P = 
\left[
\begin{array}{ccc|cc}
0 & 1 & 1 & 1 & 0 \\
1 & 0 & 1 & 0 & 1\\
\end{array}
\right]
\end{equation*}
This defines a code $\mathscr{C}$ with $\clen = 5$ and $\mlen = 3$. For this code,
\begin{equation*}
A = 
\begin{bmatrix}
0 & 1 & 1\\
1 & 0 & 1
\end{bmatrix}
\end{equation*}
Moreover, the message $\mess = \m{1}\m{2}\m{3}$ is encoded into a codeword $\cw = \cwb{1}\cwb{2}\cwb{3}\cwb{4}\cwb{5}$ such that $\cwb{1} = \m{1}$, $\cwb{2} = \m{2}$, and $\cwb{3} = \m{3}$ and then
\begin{align*}
& \cwb{4} = -(\cwb{2} + \cwb{3}) \equiv \cwb{2} + \cwb{3} \pmod 2\\
& \cwb{5} =  -(\cwb{1} + \cwb{3}) \equiv \cwb{1} + \cwb{3} \pmod 2
\end{align*}
\end{example}

Given a parity check matrix and a message, the encoder needs to output the corresponding codeword. This is best done with the \textit{generator matrix}.

\begin{definition}[Generator Matrix] Given a parity check matrix $P = [A\mid I_{\clen-\mlen}]$ for a binary code $\mathscr{C}$, the corresponding generator matrix is $G = [I_k \mid A^{\bot}]$.
To generate the codeword $\cw$ for a message $\mess$, simply multiply the matrices $\cw = \mess G$.
\end{definition}

\begin{remark}
Note that a given code $\mathscr{C}$ may have more than one matrix that generates it.
\end{remark}

\begin{example}
Continuing with the code defined in example \ref{P_ex}, we have that the corresponding generator matrix for this code is 
\begin{equation*}
G = 
\left[
\begin{array}{ccc|cc}
1 & 0 & 0 	& 0 & 1	  \\
0 & 1& 0	& 	1 & 0 \\
0 & 0 & 1 	& 1 & 1
\end{array}
\right]
\end{equation*}
\end{example}

\section{Decoding}
As we discussed earlier, the channel will (term-wise) add some unknown error vector $\ev=\evb{1} \ldots \evb{\clen}$ to the codeword so that the decoder receives a ``noisy codeword'' or ``received vector'' $\rc = \cw + \ev \pmod 2$. 

The decoder has a significantly harder job than the encoder. Namely, it must take this noisy codeword $\rc$ and return the message $\mess$ that produced the vector. Any given decoding algorithm will not always be successful; sometimes there will be so many errors in a given received codeword that it is impossible to parse which message was sent. That said, some decoding algorithms are more effective than others, and this distinction comes down to the properties of the code $\mathscr{C}$.

\subsection{Properties of Vectors}

In this subsection, we discuss properties of binary vectors generally, which apply to codewords, messages, error vectors, and received codewords (though we use notation for received codewords for generality). One of the key defining properties of a vector is its Hamming weight.
\begin{definition}[Hamming weight] The Hamming weight of a vector $\rc = \rcb{1}  \ldots \rcb{\clen}$ is the number of nonzero $\rcb{i}$'s, and is denoted by $wt(\rc)$.
\end{definition}

\begin{example}
For example, the vector $\rc = 0110100111$ has Hamming weight 6.
\end{example}

Another important property in relating two vectors is their \textit{Hamming distance}.
\begin{definition}[Hamming distance] The Hamming distance of two vectors $\rc = \rcb{1}  \ldots \rcb{\clen}$  and $\cw = \cwb{1} \ldots \cwb{\clen}$ is defined $dist(\rc,\cw) = wt(\rc \oplus \cw)$, where the XOR is bitwise.
\end{definition}

Note that one could write this definition in many ways (i.e.~addition or subtraction mod 2), but the heuristic idea behind computing the Hamming distance between two vectors is to add up how many of their bits differ. This concept is a fundamental building block for the decoding algorithms we will encounter.

\begin{example}
dist(010101, 100111) = 3
\end{example}

\subsection{Errors}
Earlier we defined the error vector $\ev = \evb{1} \ldots \evb{\clen}$ where $\evb{i} \in \Z_2$. The received vector is defined $\rc = \cw + \ev$, which implies that $\evb{i} = 1$ if and only if the $i$-th bit of the codeword was corrupted in transmission and $\evb{i} = 0$ otherwise. Since the binary symmetric channel corrupts a single bit with probability $p$, it follows that $P[\evb{i} = 1] = p$.  One may wonder: \textit{what is the most probable error vector?} This would certainly help the decoder to determine which codeword was sent given the noisy codeword.

Since the corruption of bits by the communication channel are independent of one another, we can compute the probability that the error vector $\ev$ is some particular vector $\mathbf{x}$:
\begin{equation} \label{p_error_vect}
P[\ev = \mathbf{x}] = \prod_{i = 1}^{\clen} P[\evb{i} = x_i] = p^{wt(\mathbf{x})}(1-p)^{\clen-wt(\mathbf{x})}.
\end{equation}
This perhaps goes without saying, but the error vector is entirely independent of the message.
\begin{example}
Suppose $\clen=5$. Then, $P[\ev = 10110] = p(1-p)p^2 (1-p) = p^3(1-p)^2$.
\end{example}
So, now we want to maximize the equation \ref{p_error_vect} to figure out the most likely error vector $\mathbf{x}$ and thus the best decoding algorithm. We have that $p < \frac{1}{2}$, so it follows that the maximum probability is achieved when $\clen-wt(\mathbf{x})$ is as large as possible. This occurs when $\mathbf{x} = \mathbf{0}$, the zero vector. In other words, \textit{the most likely error vector is one that indicates that none of the bits were corrupted}.
 
 In terms of decoding, this fact implies that the optimal decoding algorithm assumes that the weight of $\ev$ is as small as possible. If the received vector $\rc$ is not a codeword, then the decoder should output the nearest codeword $\cw$ with respect to the Hamming distance. This decoding method is called \textit{nearest neighbor decoding} and it is the strategy we will employ.

\subsection{Properties of Codes}
Now that we have defined the nearest neighbor decoding strategy, one may wonder how many errors a given code can correct. Before proceeding, note the distinction between \textit{error detection} whereby the code informs the decoder that $x$ number of errors occurred and \textit{error correction} whereby the decoder is able to correct the $x$ errors. Analyzing the error-correcting and error-detecting properties of linear codes will require some additional metrics on codes.

One of the traits of a code that is most pertinent to its error-correcting properties is called its \textit{minimum distance}. This captures the worst-case correction performance of a code on a nearest neighbor decoding algorithm.

\begin{definition}[Minimum Distance] The minimum distance $\mindist$ of a code $\mathscr{C}$ is $$d(\mathscr{C}) = \min_{\cw,\cw' \in \mathscr{C}} \{dist(\cw,\cw')\}.$$
\end{definition}

The minimum distance of a code $\mathscr{C}$ encapsulates how closely packed the codewords are in $\clen$-dimensional space. This determines how many errors we can correct with nearest-neighbor decoding.
\begin{theorem} A linear code $\mathscr{C}$ with minimum distance $\mindist$ can correct $\lfloor \frac{1}{2} (\mindist-1) \rfloor$ errors. 
\end{theorem}

\begin{proof}
Suppose our code $\mathscr{C}$ has minimum distance $\mindist$. Now, imagine each codeword has an $\clen$-dimensional sphere\footnote{Or, for the topologists among you, let's call it the closed $\clen$-ball $\overline{B(\cw, t)}$ where $\cw \in \Z_2^\clen$ is a codeword.} of radius $t$ around it. Now, if $t$ is small enough, none of the spheres will overlap. This means that a received vector $\rc$ with at most $t$ errors will be decoded correctly using nearest neighbor decoding. Now, the question is: what is the largest that the radius $t$ can be and still maintain the non-overlapping $\clen$-spheres? This is precisely how error-correcting codes can be translated to sphere packing problems.

Anyway, if $\mindist = 2t+1$, then spheres with radius $t$ will not overlap. Thus, we have that $t = \frac{1}{2}(\mindist-1)$ is the largest that $t$ may be. Now, we can correct up to $t$ errors since the $\clen$-spheres do not overlap, but the number of errors in a codeword are discrete integer values. Thus, we can only guarantee that $\lfloor t \rfloor$, or $\lfloor \frac{1}{2} (\mindist-1)\rfloor$ errors can be corrected.\\
\end{proof}

\subsection{Decoding Errors}
One may wonder what happens if there are more than $\lfloor \frac{1}{2}(\mindist-1)\rfloor$ errors occur during transmission. In this case, the received vector $\rc = \ev+\cw$ may be closer to some other codeword $\cw'$, so nearest neighbor decoding will decide that $\cw'$ was the original codeword rather than $\cw$, the actual codeword that was sent. This is called a \textit{decoding error} because the decoder erroneously returned $\cw'$ instead of $\cw$. This is not ideal, and code theorists study the probability of this occurrence, denoted $P_{err}$. 

For this thesis, we will only concern ourselves with $P_{succ}[\mathcal{A}]$, the probability of success of a decoding algorithm $\mathcal{A}$ on some code. 
\begin{definition}(Probability of Success) The probability of success for a decoding algorithm $\mathcal{A}$ in the presence of $\numerr$ errors for a code with generator matrix $G$ is 
\begin{equation*}
P_{succ}[\mathcal{A}] = P[\mathcal{A}(\mess G + \ev) = \mess]
\end{equation*}
where $wt(\ev) = \numerr$.
\end{definition}

We will study this property of the binary simplex code and prove some interesting results about it in chapter 4.

\chapter{Learning Theory}

Given oracle  access to some function $f$, how many queries would it take to determine some property of $f$? This is precisely the question that learning theory is concerned with, which was largely founded by Dana Angluin in \citep{angluin1988queries}. This field is motivated in part by the idea that the oracle computes some ``expensive'' function, i.e.~one that is time or space intensive. Minimizing the number of queries that an algorithm has to make to an oracle keeps the computational cost expended on oracle queries low. Furthermore, complexity theorists are very interested in the query complexity, or number of queries required to solve a problem, of quantum computers compared to classical computers\footnote{For example, see \citep{gortler2001quantum} and \citep{atici2005improved}}.

\section{Basics of Concept Learning}
Consider the oracle function $f$. This is a \textit{Boolean function}, as it takes inputs in $\Z_2^n $ for some fixed $n$ and outputs 0 or 1. In the field of learning theory, $f$ is called a \textit{concept}.

\begin{definition}[Concept] A concept $f$ over $\Z_2^n$ is a Boolean function $f: \Z_2^n \rightarrow \Z_2$. Equivalently, a concept can be viewed as a subset of $\Z_2^n$, namely $f = \{x \in \Z_2^n \mid f(x) = 1\}$.
\end{definition}

Computer scientists do not care how many queries it takes to learn the properties of some particular concept, but they are more interested in knowing how the number of queries \textit{scales asymptotically} with the size of the input. To take care of this, they came up with 
\textit{concept classes}. Traditionally, concept classes are denoted by $\mathcal{C}$, but we will instead call them $\mathcal{F}$ to avoid confusion with codes $\mathscr{C}$. 

\begin{definition}[Concept Class] A concept class $\mathcal{F} = \cup_{n \geq 1} f_n$ is a collection of concepts where $f_n = \{f \in \mathcal{F} \mid f \text{ is a concept over } \Z_2^n\}$.
\end{definition}

\begin{example}
Suppose the domain we are working with is sets of $2^n$ people instead of $\Z_2^n$. Then, we can define a concept like $f = \{ x\in 2^n \text{-set of people} \mid \text{ Marika has met } x\}$. Then, $f(\text{``Jamie Pommersheim''}) = 1$, but $f(\text{``Alan Turing''}) = 0$, sadly. Now, the concept class is just the union of these concepts over all possible domain sizes (ignoring the fact that the number of people that have existed is finite). This example is perhaps not mathematically rigid, but it should give the reader an intuition for what concept classes are.
\end{example}
The example we outlined refers to a specific kind of function called a \textit{membership oracle}.
\begin{definition}[Membership Oracle] A membership oracle $MQ_f$ is an oracle that returns 1 on input $x$ if and only if $x \in f$ or equivalently if $f(x)=1$. This is called a \textit{membership query}.
\end{definition}

Given some membership oracle $MQ_f$, the goal of the learning algorithm is to construct a \textit{hypothesis} $h: \Z_2^n \rightarrow \Z_2$ that is equivalent to $f$, i.e.~$\forall x \in \Z_2^n, f(x) = h(x)$. While their outputs are equal, $f$ and $h$ may compute their outputs quite differently, but this is irrelevant.

\chapter{The Binary Simplex Code}
One particularly interesting linear code is the simplex code. This is a special case of the Reed-Muller codes we will discuss later. The encoding and decoding algorithms are quite simple, so this is our starting point for further analyzing some of the error-correcting properties of the quantum decoding algorithm.
\section{Encoding}
We will denote the binary simplex code for messages of length $\mlen$ by \simpl{\mlen}. To encode a message $\mess$ with \mlen bits, simply append the inner product of each binary number of length \mlen (in order) with the message. The $2^\mlen$ bits of codeword \cw for message $\mess$ are: 
\begin{equation*}
\cwb{i}= \mathbf{i} \cdot \mess \pmod{2} \quad{\mathbf{i} \in \Z_2^\mlen}.
\end{equation*}
\begin{example}
Suppose $\mlen=3$. Then, the encoding of the message $\mess = 010$ is:
\begin{align*}
&\cwb{1} = (0,0,0) \cdot (0,1,0) = 0\\
&\cwb{2} = (0,0,1) \cdot (0,1,0) = 0\\
&\cwb 3 = (0,1,0) \cdot (0,1,0) = 1\\
&\cwb 4 = (0,1,1) \cdot (0,1,0) = 1\\
&\cwb 5 = (1,0,0) \cdot (0,1,0) = 0\\
&\cwb 6 = (1,0,1) \cdot (0,1,0) = 0\\
&\cwb 7 = (1,1,0) \cdot (0,1,0) = 1\\
&\cwb 8 = (1,1,1) \cdot (0,1,0) = 1\\
\end{align*}
Thus, $\cw = 00110011$ for that message. 
\end{example}
Now, the computer scientists reading this will notice that the length of the encoding of a message is exponential in the length of the message, i.e.~a message of length $\mlen$ has a codeword of length $2^\mlen$. This is far from an ideal representation of information. Indeed, the codeword for a 256-bit message would be $2^{253}$ bytes long, which is so large that our units of storage do not go that high. For some perspective, the largest unit of storage I could find is the yottabyte, or $2^{80}$ bytes, or one trillion terabytes. The length of the codeword, $2^{253}$ bytes, is far more information than could possibly be stored in the observable universe even if each bit were represented by a single atom. Clearly, this scheme is not ideal for long messages, but in the spirit of conducting unfettered computer science theory research, we will ignore this fact. 

\section{Quantum Decoding Algorithm}
The quantum decoding algorithm for the simplex code is very pleasing. The Bernstein-Vazirani algorithm is the essence of the algorithm. In this version of the algorithm, we will assume that no errors occurred during the transmission of the codeword; later, we will analyze the performance of the algorithm with errors.
\subsection{Reliable Quantum Oracle}
The quantum oracle we will use is specific to each message $\mess$. That is, the oracle $\mathcal{O}_\mess$ encodes the message $\mess$ into the corresponding codeword $\cw$. This oracle is ``reliable'' in the sense that it always gives $\cw$ with no errors, i.e~the received codeword is the actual codeword. 

To query the message oracle, the user initializes the query register with an indicator vector $\mathbf{i} \in \Z_2^\mlen$ denoting which bit of the message they would like to read in, and they initialize the response register to some known bit $z\in\Z_2$. Precisely, $\mathcal{O}_\mess$ is a bijection on $(\C^2)^\mlen \otimes \C^2$ and is defined as follows
\begin{equation*}
\mathcal{O}_\mess: \ket{\mathbf{i}, \rr} \longmapsto \ket{\mathbf{i}, \rr \oplus (\mathbf{i} \cdot \mess)}.
\end{equation*} 
This returns precisely the $i$-th bit of the codeword $\cw$ that corresponds to message $\mess$. Now, we may put $\Hminus$ in the response register and use the phase-kickback trick to store $\cwb{i}$ in the phase of the state:
\begin{equation*}
\mathcal{O}_m: \ket{\mathbf{i}, -} \longmapsto (-1)^{\mathbf{i} \cdot \mess}\ket{\mathbf{i},-}.
\end{equation*}
In the algorithm, we will omit the response register since it remains $\Hminus$ throughout the computation.
\subsection{Algorithm}

Now that our query model is defined, we give an overview of the algorithm. The $\mlen$-qubit query register will be initialized to $\ket{0}^{\otimes \mlen}$ and a $\mlen$-qubit Hadamard transform will put the query register into an equal superposition. Simultaneously, the $2^\mlen$ single-qubit response registers will each be set to $\Hminus$ so we can use the phase-kickback trick. The algorithm does one quantum query to the oracle $\mathcal{O}_m$ to read in the corresponding codeword's bits into the phase of each state in the query register. Then, an $n$-qubit Hadamard is applied to the query register. Upon measurement, we obtain the original message with probability 1.

Now, for the algorithm. Let $\ket{\psi_\mess}$ denote the query register.

\begin{align}
\ket{\psi_\mess}
& =  \frac{1}{\sqrt{2^\mlen}} \sum_{\mathbf{i} \in \Z_2^\mlen} \ket{\mathbf{i}} \\
& \xrightarrow{\mathcal{O}_\mess} \frac{1}{\sqrt{2^\mlen}} \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot \mess} \ket{\mathbf{i}} \\
& \xrightarrow{H^{\otimes \mlen}} \frac{1}{2^\mlen} \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot \mess} \sum_{\mathbf{j} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot \mathbf{j}} \ket{\mathbf{j}} \\
& = \frac{1}{2^\mlen} \sum_{\mathbf{j} \in \Z_2^\mlen} \left( \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i}\cdot (\mess +\mathbf{j})} \right) \ket{\mathbf{j}} \label{confuse}\\
& = \ket{\mess} \label{result}
\end{align}
Most of these transformations follow directly from definitions. The equality from line \ref{confuse} to \ref{result} is less apparent, but this equality follows the same logic as the analogous part of the Bernstein-Vazirani algorithm. Note that on line \ref{confuse} if $\mathbf{j} = \mess$, the inner sum becomes 
\begin{align*}
\sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot (2\mess)} 
&  = \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot \mathbf{0}} \\
&= \sum_{i \in \Z_2^\mlen} 1 \\
& = {2^\mlen}.
\end{align*}
Thus, the full state on line \ref{confuse} is 
\begin{align*}
\frac{1}{2^\mlen} \sum_{\mathbf{j} \in \Z_2^\mlen} \left( \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i}\cdot (\mess +\mathbf{j})} \right) \ket{\mathbf{j}}
& = \frac{1}{2^\mlen} \left (2^\mlen \ket{\mess}+  \sum\limits_{\substack{\mathbf{j} \in \Z_2^\mlen \\ \mathbf{j} \neq \mess}}
 (-1)^{\mathbf{i}\cdot (\mess +\mathbf{j})}\ket{\mathbf{j}} \right) \\
& = \ket{\mess}.
\end{align*}
The last equality follows because the coefficient on $\ket{\mess}$ is 1 and the Hadamard transform is unitary. These two facts imply that the coefficients on all other states must be zero. So, this algorithm yields the correct message $\mess$ with probability 1 using only one quantum query.
\section{Robustness to Errors}
Now we will consider a different scenario. Suppose that the oracle $\mathcal{O}_\mess$ is a little faulty. The oracle occasionally makes mistakes in transmitting the bits of the codeword $\cw$ that corresponds to $\mess$. Instead, it transmits a vector $\rc = \cw+\ev$ which may not even be a codeword in \simpl{\mlen}, the simplex code with messages of length $\mlen$. Since the simplex code is error-correcting, one would hope that the algorithm could still recover the correct $\mess$ with a high probability. In this section, we formalize the algorithm's robustness to errors, that is, the probability of producing the correct output even with a noisy oracle. 

\subsection{Noisy Quantum Oracle}
The reliable oracle is defined above. Now, we will define the unreliable, or noisy, quantum oracle. Again, it is a bijection on $(\C^2)^\mlen \otimes \C^2$. Now, define a set of indices $E \subseteq [N]$, where $N = 2^\mlen$, on which the oracle answers incorrectly. Then, the action of the noisy oracle $\widehat{\mathcal{O}}_\mess$ is defined as follows:

\begin{equation*}
\widehat{\mathcal{O}}_m: \ket{\mathbf{i}, -} \longmapsto (-1)^{\mess \cdot \mathbf{i}} - 2\delta_E (-1)^{\mess \cdot \mathbf{i}}
\end{equation*}
where $\delta_E$ is simply an indicator function on the set $E$. In other words, if $\mathbf{i} \notin E$ then the oracle $\widehat{\mathcal{O}}_\mess$ returns the $i$-th bit of the codeword associated with $\mess$ properly in the form $(-1)^{\mess \cdot \mathbf{i}}$. On the other hand, if $\mathbf{i} \in E$, the oracle returns $-(-1)^{\mess\cdot \mathbf{i}}$, which is the $i$-th bit of the codeword flipped.
\subsection{Algorithm with Noisy Oracle}

To the best of my knowledge, this is the first analysis of its kind. Given a fixed number of errors from the noisy oracle, I was able to compute the probability of success for the Bernstein-Vazirani algorithm (with an unreliable oracle) on the binary simplex code. I also state some corollaries that follow directly from the proof of the following theorem.

\begin{theorem} \label{cool_thm} The Bernstein-Vazirani quantum decoding algorithm for the binary simplex code $\mathcal{S}_\mlen$ has probability of success
\begin{equation*}
P_{succ} = \left(1 - \frac{\numerr}{2^{\mlen-1}}\right)^2
\end{equation*}
where $\numerr= \lvert E \rvert$, the number of errors introduced by the noisy quantum oracle and $\mlen$ is the length of the message.
\end{theorem}

\begin{proof}
We will begin by performing the algorithm with our noisy oracle $\widehat{\mathcal{O}}_\mess$.
\begin{align*}
\ket{\psi_\mess} 
& = \frac{1}{\sqrt{2^\mlen}} \sum_{\mathbf{i} \in \Z_2^\mlen} \ket{\mathbf{i}} \\
& \xrightarrow{\widehat{\mathcal{O}}_\mess} \frac{1}{\sqrt{2^\mlen}} \sum_{\mathbf{i} \in \Z_2^\mlen} (-1)^{\mathbf{i} \cdot \mess}\ket{\mathbf{i}} -2\left( \sum_{\mathbf{p} \in E} (-1)^{\mathbf{p} \cdot \mess} \ket{\mathbf{p}}\right) \\
& \xrightarrow{H^{\otimes \mlen}} \ket{\mess} - \frac{1}{2^{\mlen -1}} \left( \sum_{\mathbf{p} \in E}(-1)^{\mathbf{p} \cdot \mess} \sum_{\mathbf{j} \in Z_2^\mlen} (-1)^{\mathbf{j} \cdot \mathbf{p}} \ket{\mathbf{j}}\right)\\
& = \ket{\mess} - \frac{1}{2^{\mlen-1}} \left( \sum_{\mathbf{p} \in E}\sum_{\mathbf{j} \in Z_2^\mlen} (-1)^{\mathbf{p} \cdot(\mathbf{j}+\mess)} \ket{\mathbf{j}} \right)
\end{align*}
Now, we want to know $P_{succ}$, which is the probability of measuring $\mess$ in the computational basis. We have that for any quantum state, the probability of measuring a given result is the square of the coefficient on that state. So,
\begin{align*}
P_{succ} 
& = \left((1 - \frac{1}{2^{\mlen-1}} \left( \sum_{\mathbf{p} \in E} (-1)^{2(\mathbf{p} \cdot \mess)} \right)\right)^2 \\
& = \left(1 - \frac{1}{2^{\mlen-1}} \left( \sum_{\mathbf{p} \in E} 1 \right) \right)^2 \\
& = \left( 1 - \frac{\lvert E \rvert}{2^{\mlen-1}}\right)^2\\
& = \left( 1 - \frac{\numerr}{2^{\mlen-1}}\right)^2.
\end{align*}
\end{proof}
Interestingly, this formula shows that the quantum binary simplex decoding algorithm has $P_{succ} = 1$ if $\numerr = 2^\mlen$. In other words, if the quantum oracle is so noisy that it flips all $2^\mlen$ bits of the codeword, the decoding algorithm is still able to compute the correct message. This puzzled both the student and adviser on this thesis until we realized that flipping all of the bits leads to a \textit{global phase shift} in the quantum state post-query, which as was discussed in Chapter 1, is essentially an equivalent state. So, in this case the adage that \textit{it's so wrong it's right} holds true.

Another interesting property of the formula in theorem \ref{cool_thm} is that $\numerr/2^{\mlen-1}$ is twice the expected value of the number of errors in a codeword of length $\mlen$ when transmitted through a binary symmetric channel with a \emph{bitwise error rate} of $p = \numerr/2^\mlen$. This is because $\numerr/2^{\mlen}$ is the proportion of corrupted bits in the message. Of course, with independent and identically distributed random errors, the exact error rate for a given codeword transmission will follow a binomial distribution. That distribution will be centered around $p$, so a loose interpretation of the formula is that $P_{succ} \approx (1-2p)^2$, where $p$ is the bitwise error rate on the noisy oracle.


Some other notable features of the formula in theorem \ref{cool_thm}, 
\begin{corollary} \label{corollary}The probability of success for the quantum decoding algorithm for the binary simplex code with message length $\mlen$, $\simpl{\mlen}$:
\begin{enumerate}
\item Does not depend on the message $\mess$, and
\item Does not depend on $E$, only $\lvert E \rvert = \numerr$.
\end{enumerate} 
\end{corollary}
\begin{proof}
Corollary \ref{corollary} follows from the proof of Theorem \ref{cool_thm}.\\
\end{proof}


\chapter{Reed-Muller Codes}
Reed-Muller codes are error-correcting codes with fascinating mathematical properties.  They are used in deep space communications to reliably transmit messages across space and time. Reed-Muller codes may be one of the oldest and most well-studied codes; much of the material for this chapter can be found in \citep{macwilliamssloane}.

While the binary simplex code (singular) only had one parameter, namely the block length, the Reed-Muller codes (plural) have two parameters. In fact, Reed-Muller codes are a generalization of the binary simplex code (also called the Walsh-Hadamard code) and are closely related to other important codes. They belong to the family of \textit{geometric codes}, as evidenced by their close relationship with projective and Euclidean geometries (for which there is an appendix), and fortunately they are easier to decode than other geometrical codes. While they are used in some instances, such as for deep space communications, they have a smaller minimum distance than BCH (Bose-Chaudhuri-Hocquenghem) codes of the same block length \citep{macwilliamssloane}.


\section{Boolean Functions}
One interesting way to conceive of Reed-Muller (RM) codes is as polynomials of Boolean functions. Let the block length $\clen = 2^\mlen$ for some $\mlen\in \Z^+$. The input to a RM Boolean function will be $\numin$ variables taking binary values $\inv{1}, \ldots, \inv{\numin} \in \Z_2$. More precisely,

\begin{definition}(Boolean function) Any function $f: \Z_2^\numin \rightarrow \Z_2$ which takes $\numin$ binary input values $\inv{1}, \ldots, \inv{\numin}$ and outputs 0 or 1 is a Boolean function.
\end{definition}

A Boolean function $f$ can be defined in terms of its truth table, that is, a simple enumeration of what $f$ evaluates to on all $2^\numin$ inputs.

\begin{example}
Suppose $\numin = 3$. Then, we may represent a Boolean function $f: \Z_2^3 \rightarrow \Z_2$ with its truth table. For example, we could define the following function.
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
$ \inv{1}\inv{2}\inv{3}$ &$f(\inv{1}, \inv{2}, \inv{3})$  \\ 
\hline
000 & 0  \\ 
001 & 0 \\
010 & 0 \\
011 & 1\\
100 & 1\\
101 & 0\\
110 & 0\\
111 & 0 \\
 \hline
\end{tabular}
\end{center}
Now, a more compact way of representing the same function above is by simply writing $f = 00011000$. 
\end{example}
All of the normal Boolean operations apply to Boolean functions, with the following modifications: $f \oplus g = f+g, f \land g = fg, $ and $\lnot f = 1 + f$. In particular, Boolean functions can be written as a polynomial of some particular functions which we will denote $\bv{1}, \bv{2}, \ldots \bv{\numin}$. This notational overlap is not accidental, for $\bv{i}$ is a function which simply evaluates to whatever the $i$-th bit of the input is. In general, we can write $\bv{i} = (0^{2^{i-1}} 1^{2^{i-1}})^{2^{\numin-i}}$ where we use the computer science notation for concatenation as multiplication.
\begin{example}
For $\numin = 3$ again, the following are the special functions $\bv{1}, \bv{2}, \bv{3}$, which we will care about later,  written in the notation defined above :
\begin{align*}
& \bv{1} = 01010101 \\
& \bv{2} = 00110011\\
&\bv{3} = 00001111
\end{align*}
\end{example}
We can write any Boolean function $f: \Z_2^\numin \rightarrow \Z_2$ as a polynomial of the special functions $\bv{1}, \ldots \bv{\numin}$ together with the constant function \consto (which returns 1 on every input).  First, note that $\bv{i}^2 = \bv{i}$ since the values are binary. Now, there are $2^\numin$ possible monomials that we can make out of $\consto, \bv{1}, \bv{3},\ldots \bv{\numin}$. Namely, 
\begin{equation}
\consto, \bv{1}, \bv{3},\ldots \bv{\numin}, \bv{1}\bv{2}, \bv{1}\bv{3}, \ldots, \bv{\numin -1}\bv{\numin}, \ldots, \bv{1}\bv{2}\ldots\bv{\numin}.
\end{equation}
This is a little difficult to read, but essentially we begin by listing all monomials we can make with just one special function, then all monomials we can make by multiplying two of the special functions, and so on, until the last term which is just all of the special functions multiplied together.

There are $1 + { \numin \choose 1} + { \numin \choose 2}+ \ldots + {\numin \choose \numin} = 2^\numin$ such monomials. Now, note that the coefficient on a given monomial can be either 0 or 1. This implies that there are $2^{2^\numin}$ possible functions we can make by summing these tems. Since this is equal to the total number of Boolean functions with $\numin$ inputs, all of the sums must be distinct.
\section{Encoding}
Now, consider how one might connect these Boolean functions to error-correcting codes. As was mentioned, any Boolean function can be represented as a polynomial of the functions $\consto, \bv{1}, \ldots, \bv{\numin}$. That polynomial has a coefficient vector $(\m{1}, \ldots, \m{\mlen})$, which we will suggestively call the message, and the output of the polynomial $\cwb{1}\cwb{2} \ldots \cwb{\clen}$ which represents the codeword (note that $\clen = 2^\mlen$). 


\begin{definition} (Reed-Muller code) The $\degrm$th order Reed-Muller code $\mathscr{R}(\degrm, \numin)$ of length $\lenrm = 2^\numin$ , where $0 \leq \degrm \leq \numin$, is the set of all vectors $f$ where $f(\inv{1}, \ldots, \inv{\numin})$ is a Boolean function which is a polynomial of degree at most $\degrm$.
\end{definition}

As was mentioned, $\bv{i}^2 = \bv{i}$ since the values are binary. So, we can  assume every $\bv{i}$ has degree 0 or 1 in each monomial. Let \rmexp denote the set of valid monomials for vectors $f$ in $\mathscr{R}(\degrm, \numin)$ together with the constant function \consto. Then, we have
\begin{equation*}
\rmexp \coloneqq 
\left \{
	\prod_{i \in \sigma} \bv{i} \suchthat  \sigma \subseteq [\numin], \lvert \sigma \rvert \leq \degrm 
\right \} \cup \consto.
\end{equation*}
Then, all codewords in $\mathscr{R}(\degrm, \numin)$ can be expressed as a unique linear combination of elements in $\rmexp$\footnote{Our notation implicitly says that  the following are notationally equivalent $\bv{i}\bv{j} = \bv{ij}$ (if for example $\sigma = \{i, j\}$). We will allow this with the understanding that the meaning will be clear from context.}. Denote the elements of $\rmexp$ by $\bv{\sigma}$. That is,
\begin{equation} \label{reed_muller_poly}
\mathscr{R}(\degrm, \numin) = \sum_{\bv{\sigma} \in \rmexp} \m{\sigma} \bv{\sigma}  \quad \m{\sigma} \in \Z_2.
\end{equation}
Moreover, the $\mathscr{R}(\degrm, \numin)$ has a basis of size
\begin{equation}
\mlen = \sum_{i = 1}^{\degrm} {\numin \choose i}.
\end{equation}
This means that $\mlen = \lvert \rmexp \rvert$, which is our message length.
Hopefully a lengthy example will clarify this definition.
\begin{example}
The second order RM code of length 8, $\mathscr{R}(2,3)$ consists of the 128 codewords
\begin{equation*} 
\m{0}\consto + \m{1}\bv{1} + \m{2}\bv{2} + \m{3}\bv{3} + \m{12}\bv{1}\bv{2} + \m{13}\bv{1}\bv{3} + \m{23}\bv{2}\bv{3}, \quad \m{i} \in \Z_2. 
\end{equation*}
This code has order 2 because each term in the polynomial is a product of at most two Boolean functions. By varying the coefficients $\m{i}$ according to the bits of the message, we obtain a polynomial that corresponds to the codeword. 

For example, suppose we wanted to encode the message $\mess = 0110101$. We would obtain the following function as the codeword
\begin{equation*}
f_\cw = 0\consto + 1\bv{1} + 1\bv{2} + 0\bv{3} + 1\bv{1}\bv{2} + 0\bv{1}\bv{3} + 1\bv{2}\bv{3}.
\end{equation*}
Now, performing addition and multiplication of the Boolean functions in the terms of $f_c$, we obtain: 
\begin{align*}
f_\cw 
& =  \bv{1} + \bv{2} + \bv{1}\bv{2} + \bv{2}\bv{3}\\
& = 01010101 \oplus 00110011 \oplus 00010001 \oplus 00000011 \\
& = 01110100.
\end{align*}
So, for message $\mess = 0110101$ the corresponding codeword is $\cw=01110100$, which can also be thought of as the polynomial $f_\cw$ above.
\end{example}


\begin{theorem} 
$\mathscr{R}(\degrm,\numin)$ has minimum distance $\mindist = 2^{\numin-\degrm}$.
\end{theorem}
\begin{proof}
This induction proof will be left as an exercise. See \citep{macwilliamssloane}.\\
\end{proof}
\subsection{Generator Matrix}
For those who prefer to think of encoding functions in terms of generator matrices, the generator matrix for $\mathscr{R}(\degrm,\numin)$ has as its rows the terms of \rmexp.

\begin{theorem} The generator matrix $G$ for $\mathscr{R}(\degrm, \numin)$ has the following property
\begin{equation*}
G(\degrm, \numin) = 
\begin{bmatrix}
G(\degrm, \numin-1) & G(\degrm, \numin-1) \\
0 & G(\degrm-1,\numin-1)
\end{bmatrix} .
\end{equation*}
\end{theorem}

\begin{proof}
This follows from Theorem 2 in \S 13.3 of \citep{macwilliamssloane}.\\
\end{proof}

\begin{example} \label{gen_matrix_ex}
The generator matrix for $\mathscr{R}(4,4)$ is displayed in Table \ref{gen_matrix}.
\begin{table}
\begin{center}
\begin{tabular}{ r c }
 \consto & 1111111111111111 \\ 
 \hline
$\bv{4}$ & 0000000011111111 \\  
$\bv{3}$ & 0000111100001111 \\
$\bv{2}$ & 0011001100110011  \\
$\bv{1}$ & 0101010101010101 \\
\hline
$\bv{3}\bv{4}$ & 0000000000001111  \\
$\bv{2}\bv{4}$ & 0000000000110011 \\
$\bv{1}\bv{4}$ & 0000000001010101  \\
$\bv{2}\bv{3}$ & 0000001100000011  \\
$\bv{1}\bv{3}$ & 0000010100000101  \\
$\bv{1}\bv{2}$ & 0001000100010001  \\
\hline
$\bv{2}\bv{3}\bv{4}$ & 0000000000000011 \\
$\bv{1}\bv3\bv4$ & 0000000000000101 \\
$\bv1\bv2\bv4$ & 0000000000010001  \\
$\bv1\bv2\bv3$ & 0000000100000001  \\
\hline
$\bv1\bv2\bv3\bv4$ & 0000000000000001 
\end{tabular}
\caption{Generator matrix for $\mathscr{R}(4,4)$. \label{gen_matrix}}
\end{center} 
\end{table}
So, by simple matrix multiplication with our 16-bit message, we will obtain the corresponding codeword. Note that the generator matrix for $\mathscr{R}(3,4)$ is simply rows 1-15 of the above matrix, and the generator matrix for $\mathscr{R}(2,4)$ is just rows 1-11, and so on. 

The rows of the above matrix form a basis for the code, meaning any codeword in $\mathscr{R}(4,4)$ can be expressed uniquely as a linear combination of the rows in the generator matrix. That unique linear combination is, in fact, the message that corresponds to the codeword. Now, we will look at the classical decoding algorithm.
\end{example}
\section{Classical Decoding Algorithm}
There are a few ways to approach the decoding algorithm for Reed-Muller codes. By far the most elegant is to view RM codes through the lens of finite geometries and utilize the framework that geometries provide. The reader is encouraged to read Appendix \ref{finite_geom_chapter} before proceeding with the geometric description of the RM decoding algorithm.

\subsection{Geometric Connections}
Recall that the Euclidean geometry of dimension $\numin$ over $GF(2)$, denoted $EG(\numin,2)$, contains $2^\numin$ points whose coordinates are all of the binary vectors of length $\numin$, $(\bv1, \ldots, \bv{\numin})$. If the zero point is deleted, the projective geometry $PG(\numin-1, 2)$ is obtained. We can think of the codewords of $\mathscr{R}(\degrm,\numin)$ in this context, namely as subsets of $EG(\numin,2)$. However, we must have a way to specify the subsets. This is achieved by looking at the codewords as \textit{incidence vectors} for these subsets.

\begin{definition}[Incidence vector] Let $S \subseteq X$ where $\lvert X \rvert = 2^\numin$ and $X$ has some ordering. Then, $\mathbf{v} \in \Z_2^\numin$ is an incidence vector for $S$ if
\begin{equation*}
\inv{i} = 1 \iff x_i \in S,
\end{equation*}
where $x_i$ is the $i$-th element of the ordered set $X$ and \inv{i} is the $i$-th bit of $\mathbf{v}$. Incidence vectors are also called \textit{indicator vectors}.
\end{definition}

\begin{example}
$EG(3,2)$ consists of 8 points, call them $\proj{1}, \ldots, \proj{8}$, whose coordinates we may take to be the following column vectors:
\begin{center}
\begin{tabular}{ c | c c c c c c c c }
 & $\proj1$ & $\proj2$ & $\proj3$ &$ \proj4$ &$ \proj5$ &$ \proj6 $& $\proj7$ & $\proj8$ \\
 \hline
 $\nbv{3}$ & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\ 
 $\nbv{2}$ & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\
$\nbv{1}$ & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0 
\end{tabular}
\end{center}

The subset $S = \{\proj2, \proj3, \proj4, \proj5\}$ has incidence vector $\chi(S) = 00111100$. This is a codeword of $\mathscr{R}(1,3)$. So, there is a one-to-one correspondence between codewords in $\mathscr{R}(1,3)$ and subsets of $EG(3,2)$. This property will prove useful for decoding.
\end{example}

For any value $\numin$, let us take the complements of the vectors $\bv{\numin}, \ldots, \bv1$.

\begin{center}
\begin{tabular}{ l | c c c c c c c c c }
& $\proj1$ & $\proj2$ & $\proj3$ & $\proj4$ & $\ldots$ &$\proj{2^\numin-3}$ & $\proj{2^\numin-2}$ & $\proj{2^\numin-1}$ & $\proj{2^\numin}$\\
\hline
$\nbv{\numin}$ & 1 & 1 & 1 & 1 &$\ldots$ & 0 & 0 & 0 & 0\\
$\nbv{\numin-1}$ &1 & 1 & 1 & 1 &$\ldots$ & 0 & 0 & 0 & 0\\
$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\ddots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ \\
$\nbv2$ & 1 & 1 & 0 & 0 & $\ldots$ & 1 & 1 & 0 & 0\\   
$\nbv1$ & 1 & 0 & 1 & 0 &$\ldots$ & 1 & 0 & 1 & 0\\   
\end{tabular}
\end{center}
The columns of this matrix are the points in $EG(\numin,2)$. Again, there is a one-to-one correspondence between the subsets of $EG(\numin,2)$ and binary vectors of length $2^\numin$. In particular, any vector $x \in \Z_2^\numin$ is an incidence vector for a subset of $EG(\numin,2)$. The vectors $\bv{i}$ are themselves the characteristic vectors of hyperplanes which pass through the origin (subspaces of dimension $\numin-1$), the $\bv{i}\bv{j}$ describe subspaces of dimension $\numin-2$, and so on.

\subsection{Decoding Algorithm}
As with the other error-correcting codes we have studied thus far, RM decoding will rely on parity checks and \textit{majority logic decoding}, which is a slightly different term for \textit{maximum likelihood decoding}, or \textit{nearest neighbor decoding}. Finite geometries will inform the algorithm as to exactly which parity checks to compute.

The decoding algorithm is a little involved, so we will describe the general decoding algorithm followed by an example of encoding and decoding.
We will work with $\mathscr{R}(2,4)$ to illustrate the algorithm notation rather than using general notation (recall that rows 1-11 of Table \ref{gen_matrix} contains the generator matrix for this code).

Denote the eleven bits of the message as $\mess = \m0\m4\m3\m2\m1\m{34}\m{24}\m{14}\m{23}\m{13}\m{12}$. The notation here is taken to match the rows of Table \ref{gen_matrix}. When we apply $\mess \cdot G$ where $G$ is the generator matrix for $\mathscr{R}(2,4)$, this message gets encoded into the following codeword.

\begin{align*}
\mess\cdot G 
&= \m01 + \m4\bv4 + \ldots + \m1\bv1 + \ldots + \m{12}\bv1\bv2 \\
& \stackrel{\mathclap{call}}{=} \cwb0\cwb1\cwb2\ldots \cwb{15} \\
& = \cw.
\end{align*}
Now, the challenge is to decode $\rc$, the received codeword (note that $\rc$ may or may not be a codeword itself). In general, to decode a noisy codeword from $\mathscr{R}(\degrm,\numin)$, we construct a series of parity checks. We begin by decoding $\m\sigma$ where $\sigma = \sigma_1 \ldots \sigma_\degrm$. The corresponding row of the generator matrix $\bv{\sigma_1} \ldots \bv{\sigma_\degrm}$ is the incidence vector of an $(\numin-\degrm)$-dimensional subspace $S$ of $EG(\numin,2)$. To find $S$, use the row of the generator matrix as an indicator vector on the matrix of complements of the Reed-Muller monomial basis vectors. Now, we take $T$ to be the complementary subspace to $S$ with incidence vector $\bv{\tau_1}\ldots \bv{\tau_{\numin-\degrm}}$ where $\{\tau_1, \ldots \tau_{\numin-\degrm}\} = [\numin]\setminus \{\sigma_1, \ldots, \sigma_\degrm\}$. Then $T$ meets $S$ at a single point, namely the origin. Now, let $U_1, \ldots, U_{2^{\numin-\degrm}}$ be the \textit{translates} of $T$ by points in $S$, which includes $T$ itself. Each $U_i$ meets $S$ in exactly one point.

\begin{theorem} \label{error_correct_thm}
If there are no errors, $\m\sigma$ is given by
\begin{equation*}
\m\sigma = \sum_{P \in U_i} \rcb{P}, \quad i = 1, \ldots, 2^{\numin-\degrm}
\end{equation*}
\end{theorem}
Note that there are $2^{\numin-\degrm}$ equations that should all yield the same $\m{\sigma}$. If some of the equations do not agree, one can apply majority logic, meaning choosing the result based on what the majority of equations state.  This theorem implies that if no more than $\lfloor\frac{1}{2}(2^{\numin-\degrm} -1)\rfloor$ errors occur, majority logic decoding will recover each of the symbols $\m{\sigma}$ correctly, where $\sigma$ is a string of any $\degrm$ symbols. 
	After going through this process for all $\degrm$-length subscripts in $\mess$, we subtract the corresponding $\m{ij}\bv{i}\bv{j}$'s from $\rc$ to obtain a new received codeword $\rc' = \rcb0'\rcb1' \ldots \rcb{15}'$ and repeat the process for all $\degrm-1$-length subscripts. We continue this process until all we have left is $\rcb0\consto + \ev$ and $\rcb0 = 0$ or $\rcb0 =1$ according to the number of 1's in the remainder of the codeword.
	
\subsection{Geometric Decoding Example}
Again, we will work with $\mathscr{R}(2,4)$ since rows 1-11 of Table \ref{gen_matrix} are the generator matrix for the code. First, let us encode some message $$ \mess = \m0\m4\m3\m2\m1\m{34}\m{24}\m{14}\m{23}\m{13}\m{12} = 00001010100,$$ which is three zero's followed by the ascii value for `T' in binary (`T' for thesis). 

Now, we encode this message by matrix multiplication (modulo 2) to get $$\cw = \cwb0 \ldots \cwb{15} = 0101011001100101.$$ Now, suppose the channel flips a bit in the process of sending the codeword, and $$\rc = 0101011\underline{1}01100101$$ is what the decoder receives (without the flipped bit underlined, obviously).

We need to begin decoding with the last bit of the message, $\m{12}$. Applying the notation used in the general decoding, we have $\sigma = \{1,2\}$. Now, we look at the row corresponding to $\bv1\bv2$, which is row 11 of Table \ref{gen_matrix}: 0001000100010001. This is our indicator vector for the complements of $\bv1, \ldots,\bv4$, which are below:

\begin{center}
\begin{tabular}{ c | c  }
$\nbv4$ & 1111111100000000\\
$\nbv3$ & 1111000011110000 \\ 
$\nbv2$ & 1100110011001100\\
$\nbv1$ & 1010101010101010 
\end{tabular}
\end{center}
So, from this table we get $S =\{1100, 1000, 0100, 0000\}$. Now, the incidence vector for $T$ is the row of Table \ref{gen_matrix} corresponding to $\bv{34}$. Thus, the incidence vector for $T$ is 0000000001111. So, $T = \{0011,0010, 0001, 0000\}$. Indeed $S \cap T = \{0000\}$. 

Now, we take all translates of $T$ in $EG(4,2)$ (that is, we take each point in $S$ and translate $T$ by that value). The translates of $T$ are: 
\begin{align*}
& U_1 =  \{1100 + t \mid t \in T\} = \{1111, 1110, 1101, 1100\} \\
& U_2 = \{1000 + t \mid t \in T\} = \{1011, 1010, 1001, 1000\} \\
& U_3 = \{0100 + t \mid t \in T\} = \{0111, 0110, 01010, 0100\} \\
& U_4 =  \{0000 + t \mid t \in T\} = \{0011, 0010, 0001, 0000\} \\
\end{align*}
Now, we will use the bits of the received codeword $\rc$ to compute $\m{12}$, the last bit of the message. By applying Theorem \ref{error_correct_thm}, we have that 
\begin{equation*}
\m{12}  = \sum_{P \in U_i} \rcb{P}, \quad i = 1,2,3,4. 
\end{equation*}
This gives us the four following checksums, which should all be equal in the case of no errors.
\begin{align*}
\m{12} 
& = \cwb{15} + \cwb{14} + \cwb{13} + \cwb{12}\\
& = \cwb{11} + \cwb{10} + \cwb{9} + \cwb{8}\\
& = \cwb{7} + \cwb{6} + \cwb{5} + \cwb{4} \\
& = \cwb{3} + \cwb2 + \cwb1 + \cwb0
\end{align*}
So, these equations give us that $\m{12} =0 =0=1=0 $. These equations are \textit{not} equal because bit $\cwb7$ was flipped by the noisy channel. But, the decoder doesn't know which bit was flipped. Instead, it decides that $\m{12} = 0$ using majority logic. Indeed, this is correct! (Note how if two equations didn't match, the decoder would possibly make a decoding error).

Now, the decoder will continue using this process to decode $\m{13}, \m{23}, \m{14}, $ and $\m{34}$. Once it has figured out those five bits, it will subtract $\m{34}\bv3\bv4 + \m{14}\bv1\bv4 + \m{23}\bv2\bv3 + \m{13}\bv1\bv3$ from $\rc$ and proceed with the algorithm to decode $m_0 \ldots m_4$. We will spare the reader these details, but that at least conveys the tedium involved in decoding. In the chapters that follow, we will explore a quantum way of decoding RM codes.

\chapter{Quantum Multivariate Polynomial Interpolation}
Before we dive into quantum decoding algorithms for Reed-Muller codes, we must understand the mechanisms behind such an algorithm. The contents of this chapter are largely taken from the recent work in \citep{chen2018quantum} and \citep{childs2015optimal}. In this chapter, we set up the general algorithm that we will later tailor to our Reed-Muller decoding problem. 

It is worth noting that the papers upon which this chapter is based are far more extensive than what will be covered here. In fact, their quantum polynomial interpolation algorithm is much more closely related to quantum cryptanalysis and a concept in cryptography called \textit{secret sharing}. Alas, this is not a cryptography thesis.

\section{The Problem Definition}
Let $f(\inv1, \ldots, \inv\numin)\in \F_\fsize[\inv1, \ldots, \inv\numin]$, that is, $f$ is a multivariate polynomial over the finite field of size $\fsize$, and let $f$ be of degree $\degrm$. Suppose an algorithm $\mathcal{A}$ has oracle access to $f$, and $\degrm$ is known. The polynomial interpolation problem that $\mathcal{A}$ must solve is to determine the coefficients of $f$ by querying the oracle. In other words, $f$ must learn the coefficient vector of $f$.

The learning theory question that researchers then ask is: \textit{how many queries must any algorithm $\mathcal{A}$ make to the oracle before it can determine the coefficients of $f$?} They may wonder specifically about the minimum number of queries required for any algorithm.

Additionally, those interested in quantum complexity theory wonder how the answer to the above question changes depending on whether $\mathcal{A}$ is a quantum or classical algorithm. In their paper, \citep{chen2018quantum} show that 
\begin{equation*}
\left \lceil \frac{\degrm}{\numin + \degrm} { \numin + \degrm \choose \degrm} \right \rceil
\end{equation*}
queries suffice to achieve probability of success approaching 1 for large field order $\fsize$.

By contrast, the best classical algorithm for multivariate polynomial interpolation requires ${\numin+\degrm \choose \degrm}$ queries, using a well-chosen system of linear equations \citep{chen2018quantum}. So, the quantum algorithm has a speedup factor of $\frac{\numin + \degrm}{\degrm}$.

\section{Quantum Query Model}
Much like phase kickback, queries to the oracle will be encoded in the phase of the response register. However, the algorithm will perform queries in the Fourier basis using a technique that is a generalization of the phase kickback trick. Moreover, the algorithm will make $\numq$ quantum queries in parallel to extract all of this information at once. These $\numq$ queries are chosen \textit{non-adaptively}, that is, the algorithm prepares the questions all at once.

\subsection{Quantum Fourier Transform}
Defining the quantum Fourier transform requires some background. First, it is a well-known fact that the order of any finite field can be written as a prime to some power. In other words, the finite field $\F_\fsize$ has order $\fsize = p^w$ where $p$ is prime. Now, define the trace function $\tracef: \F_\fsize \rightarrow \F_\fsize$ by $x \mapsto x + x^p + x^{p^2} + \ldots + x^{p^{w-1}}$. Next, we will define the exponential function $\expf: \F_\fsize \rightarrow \C$ by $\expf(x) = e^{i2\pi \tracef(x)/p}$. Now we are ready to define the quantum Fourier transform (QFT).

\begin{definition}[QFT] The quantum Fourier transform over $\F_\fsize$ is a unitary transformation $QFT: \C^\fsize\rightarrow \C^\fsize$ defined as follows 
\begin{equation*}
\ket{x} \longmapsto \frac{1}{\sqrt{\fsize}} \sum_{y \in \F_\fsize} \expf(xy) \ket{y}.
\end{equation*}
The $\numq$-dimensional quantum Fourier transform $QFT: \C^{\fsize\numq}\rightarrow \C^{\fsize\numq}$ is given by
\begin{equation*}
\ket{x} \longmapsto \frac{1}{\fsize^{\numq/2}} \sum_{y\in \F_\fsize^\numq} \expf(x \cdot y) \ket{y}.
\end{equation*}
\end{definition}
This will be key to the phase query algorithm.
\subsection{Phase Query}
The authors of \citep{childs2015optimal} and \citep{chen2018quantum} describe what they call a \textit{phase query}. First, note that the query register $x$ is one set of inputs to $f$, $ (x_1, x_2, \ldots, x_\numin) \in \F_\fsize^\numin$ and the response register $y \in \F_\fsize$ is where the output of the query will be stored. Essentially, the algorithm involves doing an inverse quantum Fourier transform (which we will denote $QFT^{-1}$) on the query and response registers, performing a query to the oracle $\mathcal{O}_f: (\C^\fsize)^{\otimes \numin} \otimes \C^\fsize \rightarrow (\C^\fsize)^{\otimes \numin} \otimes \C^\fsize$,  and then doing a quantum Fourier transform (QFT) as follows
\begin{align}
\ket{x,y} 
 \myrightarrow{QFT^{-1}} &\frac{1}{\sqrt{\fsize}} \sum_{z\in \F_\fsize} \expf(-yz) \ket{x,z} \\
 \myrightarrow{\mathcal{O}_f} & \frac{1}{\sqrt{\fsize}} \sum_{z \in \F_\fsize} \expf(-yz) \ket{x, z + f(x)} \\
 \myrightarrow{QFT} & \frac{1}{\fsize} \sum_{z,w \in \F_\fsize} \expf(-yz + (z + f(x))w) \ket{x,w} \\
 & = \expf(yf(x)) \ket{x,y}.
\end{align}
So, overall the phase query has the following action $\ket{x,y} \mapsto \expf(yf(x)) \ket{x,y}$. Now, instead of just querying $f$ on one input $x \in \F_\fsize$, we want to query it on $\numq$ inputs. So, we choose a subset of $\F_\fsize^\numq$ on which to perform $\numq$ parallel queries, each in a separate register. For $x \in \F_\fsize^{\numin\numq}$ and $y \in \F_\fsize^\numq$, the $\numq$ parallel phase queries has the following action
\begin{equation*}
\ket{x,y} \longmapsto \expf\left(\sum_{i=1}^{\numq} y_i f(x_i) \right) \ket{x,y}
\end{equation*}
where $x_i$ and $y_i$ are the $i$-th bits of $x$ and $y$, respectively.
\section{More Preliminaries}
Next, we define a mapping $Z: \F_\fsize^{\numin\numq} \times \F_\fsize^{\numq} \rightarrow \F_\fsize^{\srmexp}$ by 
\begin{equation*}
Z(x,y)_j = \sum_{i = 1}^\numq y_ix_i^j \quad \text{for }j \in \mathbb{J}
\end{equation*}
so that
\begin{equation*}
Z(x,y) \cdot m = \sum_{i=1}^\numq y_i f(x_i)
\end{equation*}
where $\rmexp$ is the set of allowed exponents, and $\srmexp \coloneqq \lvert \rmexp \rvert$, and $m$ is the coefficient vector on $f$.

Now, we restrict the codomain of $Z$ so that it forms a bijection. Let $R_\numq \coloneqq Z(\F_\fsize^{\numin \numq}, \F_\fsize^\numq)$ be the image of $Z$. Now, for each $z \in R_\numq$, choose a unique $(x,y) \in \F_\fsize^{\numin \numq} \times \F_\fsize^{\numq}$ such that $Z(x,y) = z$. Call $T_\numq$ this set of unique representatives. Then, $Z: T_\numq \rightarrow R_\numq$ is a bijection.

\section{The Algorithm}
The algorithm has three steps. It starts in a superposition over $T_\numq$. First, it performs $\numq$ phase queries (denoted $\numq$-$\mathcal{PQ}$), then it simply computes $Z$ in place. Finally, we measure in the Fourier basis to get an approximation of the coefficients. More precisely,

\begin{align*}
\frac{1}{\sqrt{\lvert T_\numq \rvert}} \sum_{(x,y)\in T_\numq} \ket{x,y} 
& \myrightarrow{\numq\text{-}\mathcal{PQ}} \frac{1}{\sqrt{\lvert T_\numq \rvert}} \sum_{(x,y)\in T_\numq} \expf(Z(x,y) \cdot m) \ket{x,y} \\
& \myrightarrow{Z} \frac{1}{\sqrt{\lvert R_\numq \rvert}} \sum_{z\in R_\numq} \expf(z \cdot m) \ket{z}. 
\end{align*}
Measuring in the basis of Fourier states (defined below)
\begin{equation*}
\ket{\hat{m}} \coloneqq \frac{1}{\sqrt{\fsize^\srmexp}} \sum_{z \in \F_\fsize^\srmexp} \expf(z \cdot m) \ket{z},
\end{equation*}
where $m$ are the computational basis states, results in the correct vector of coefficients with probability $\lvert R_\numq \rvert / \fsize^\srmexp$.

In \citep{chen2018quantum}, they state that this success probability is $1-O(1/q)$ with at most $\left \lceil \frac{\degrm}{\numin + \degrm} { \numin + \degrm \choose \degrm} \right \rceil $ queries.




\chapter{Quantum Decoding Algorithm for Reed-Muller Codes}

In this chapter, we explore the possibility of using the quantum polynomial interpolation algorithm to decode Reed-Muller codes. The decoder will have oracle access to the received codeword $f_c$ and, through a well-chosen series of queries and computations, will need to output the correct message $m$. The task of this chapter is to yet again reframe the Reed-Muller decoding algorithm. This time, instead of thinking about decoding in terms of finite geometries, we will explain it in terms of polynomial interpolation. Furthermore, the algorithm described in the previous chapter will be tailored to the task of decoding.
 
\section{Polynomial : Coefficient Vector :: Codeword : Message}
As was previously discussed, we can view codewords in $\mathscr{R}(\degrm, \numin)$ as multivariate polynomials of degree at most $\degrm$ with coefficients in the field $\F_2$. The polynomials have the following form:
\begin{equation}
f_\cw = \sum_{\bv{\sigma} \in \rmexp} \m{\sigma} \bv{\sigma} .
\end{equation}
where $f_\cw$ is the $\mathscr{R}(\degrm,\numin)$ codeword that corresponds to message $\mess$. So, by applying the polynomial interpolation algorithm to $f_\cw$, we should be able to extract $\mess$. 


\subsection{Adapting Polynomial Interpolation}
A lot of nice things happen when we adapt the polynomial interpolation algorithm to our use case. For one, the notation gets substantially less painful to look at. More importantly, many components of the algorithm become simpler.

First, the polynomials in $\mathscr{R}(\degrm,\numin)$ are over the field $\F_2$. So, the order of the field is simply 2 and the trace function defined earlier $\tracef:\F_2 \rightarrow \F_2$ defined by $x \mapsto x + x^p + x^{p^2} + \ldots + x^{p^{\fsize-1}}$ is just the identity transformation. This simplifies the quantum Fourier transform. The function $\expf: \F_2 \rightarrow \C$ is now defined by $x \longmapsto e^{i\pi x}$.

 
\begin{theorem} The quantum Fourier transform over $\F_2$ is just the Hadamard transform.
\end{theorem}

\begin{proof}
The QFT acts on a state $\ket{x}$, $x\in \F_2$, as follows
\begin{equation*}
\ket{x}  \longmapsto \frac{1}{\sqrt{2}}\sum_{y\in \F_2} \expf(xy)\ket{y}
=  \frac{1}{\sqrt{2}}\sum_{y\in \F_2} e^{i\pi xy}\ket{y}.
\end{equation*}
But, since $x,y \in \F_2$, this is just the transformation
\begin{align*}
\ket{x}  \longmapsto 
&\frac{1}{\sqrt{2}}\bigg( \expf(x\cdot 0)\ket{0} + \expf(x\cdot 1)\ket{1} \bigg) \\
 = &  \frac{1}{\sqrt{2}}\bigg( e^{i\pi x\cdot 0}\ket{0} + e^{i\pi x\cdot 1}\ket{1} \bigg) \\
 = & \frac{1}{\sqrt{2}}\bigg( 1\ket{0} + e^{i\pi x}\ket{1} \bigg) \\
 = & \frac{1}{\sqrt{2}}\bigg( 1\ket{0} + (-1)^x\ket{1} \bigg). 
\end{align*}
This is the Hadamard transform. \\
\end{proof}

This simplifies the phase query. In the previous chapter we showed that the phase query has the following action $\ket{x,y} \mapsto \expf(yf(x))\ket{x,y}$. Given the simplified definition of the QFT and $\expf$, we have that the phase query ($\mathcal{PQ}$) in $\F_2$ has the action
\begin{equation}
\ket{x,y} \xrightarrow{\mathcal{PQ}} (-1)^{yf(x)} \ket{x,y}.
\end{equation}

Now, the only thing left to inspect is the mapping $Z: T_\numq \rightarrow R_\numq$ where $T_\numq \subseteq \F_2^{\numin \numq} \times \F_2^\numq$ and $R_\numq \subseteq \F_2^\srmexp$. In this mapping, the coordinate $(x,y) \in T_k$ is comprised of the query register ($x$) and response register ($y$). Each pair is a set of $\numq$ queries to the oracle and the corresponding results from the oracle of the $\numq$ queries. In other words, we have that

\begin{equation*}
(x, y) =
\begin{bmatrix}
x_1 & \mathcal{O}_f(x_1) \\
x_2 & \mathcal{O}_f(x_2)\\
\vdots & \vdots \\
x_\numq & \mathcal{O}_f(x_\numq)
\end{bmatrix}.
\end{equation*}
Now,  we may have that the response register $y$ stores $r \oplus \mathcal{O}_f(x_i)$  where $r$ is whatever was in the register prior to the query; these details are omitted because they yield equivalent definitions of $Z$.

The thing which $Z$ computes is fundamentally difficult. The question that $Z$ answers is: \textit{Given $\numq$ pairs of query inputs and query outputs, which polynomial is most likely?}. This problem is, in many ways, more difficult than plain polynomial interpolation. In order to even define $Z$, one must decide how it should behave given every possible $\numq$-set of input queries and all possible responses from the oracle. The one guarantee on $Z$ is that it computes the results in place without any oracle access. Since this thesis is concerned with quantum query complexity, we may leave $Z$ undefined with the understanding that the exact implementation of $Z$ will not affect the query complexity of the algorithm.

\subsection{The Algorithm}
We begin in a superposition over $T_\numq$, make $\numq$ parallel phase queries, compute $Z$ in place, and then instead of  measuring in the Fourier basis, we will take the Hadamard and measure in the computational basis. More precisely,

\begin{align*}
\frac{1}{\sqrt{\lvert T_\numq \rvert}} \sum_{(x,y) \in T_\numq} \ket{x,y}
 \myrightarrow{\numq-\mathcal{PQ}} 
 & \frac{1}{\sqrt{\lvert T_\numq \rvert}} \sum_{(x,y) \in T_\numq}  (-1)^{Z(x,y) \cdot m} \ket{x,y} \\
 \myrightarrow{Z}
 & \frac{1}{\sqrt{\lvert R_\numq \rvert}} \sum_{z \in R_\numq} (-1)^{z\cdot m} \ket{z} \\
\myrightarrow{H^{\otimes \numin}}
& \frac{1}{\sqrt{2^\numin \lvert R_\numq \rvert}} \sum_{z \in R_\numq} (-1)^{z \cdot m} \sum_{j \in \Z_2^\numin} (-1)^{z \cdot j} \ket{j} \\
 = &\frac{1}{\sqrt{2^\numin \lvert R_\numq \rvert}}  \sum_{z \in R_\numq} \sum_{j \in \Z_2^\numin}(-1)^{z(m + j)}\ket{j}. \\
\end{align*}
Now, measuring in the computational basis, we measure $\ket{m}$ with probability 
\begin{equation*}
\bigg(\frac{\lvert R_\numq \rvert}{\sqrt{2^\numin \lvert R_\numq \rvert}}\bigg )^2 = \bigg (\sqrt{\frac{\lvert R_\numq \rvert}{2^\numin}} \bigg)^2 = \frac{\lvert R_\numq \rvert}{2^\numin}
\end{equation*}
So, for $\numq$ queries, $P_{succ} = \lvert R_\numq \rvert /2^\numin$ for decoding $\mathscr{R}(\degrm, \numin)$.
\section{Two Types of Noisy Oracles}
One may wonder how the probability of success of the above algorithm is affected by an unreliable oracle, which is similar to the question that we answered about the binary simplex code. In the case of the simplex code, the algorithm only required one quantum query, but this algorithm makes $k$ quantum queries. After some thought about noise models, two kinds of unreliable or noisy oracles arise: one which we will call stateful or self-consistent and one which we call stateless or contradictory.

First, we define these two models intuitively and later rigorously. Both oracles are unreliable or noisy in some way. The stateful oracle, when queried multiple times on a given input, will always output the same answer. 
One can imagine that the oracle \textit{memoizes}\footnote{That is, it saves its answers.} computations to all of the queries it has recieved and always checks the table to see if it was previously queried on the given input. In this way, it remains self-consistent. 


The second model is the stateless, or contradictory, oracle. When the oracle is queried multiple times, it may give different answers to the same query. So, the query $\widehat{\mathcal{O}}_f(x)$ may yield $f(x)$ with some probability $p$ and $\lnot f(x)$ with probability $1-p$. We called this model \textit{stateless} because the oracle cannot store the results of its previous computations and thus must recompute queries every time.

\subsection{Connection to Message Transmission}

In the context of error-correction, this relates to how the oracle is connected to the communication channel. If the noisy oracle is stateless, this is analogous to the message being transmitted once and stored in the oracle, with errors. As the oracle is queried, it simply consults the noisy codeword that it stored in memory somewhere rather than requesting the codeword to be resent on every query. 

The stateless noisy oracle is analogous to the oracle requesting a retransmission of the codeword on every query. In other words, a new error vector is present on each query and this presumably comes from the noise in the communication channel as the codeword is retransmitted.




\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{8}
	\setcounter{section}{0}
	
Many questions arose during the course of the year. Some of them yielded satisfactory answers, while others turned up disappointing results and were not included. Yet more remained unanswered. The project of this thesis was to: 1) shed clarity on these confusing topics, and in doing so, 2) inspire further work in this area. Perhaps one day the topics that were left unpursued can be realized concretely. They are described below.

\section{Further Work}
In this thesis we presented the idea of algorithmic robustness to noisy quantum oracles. That is, for a given quantum algorithm $\mathcal{A}$ which relies on quantum queries to oracle $\mathcal{O}$, what is $P_{succ}$ of the algorithm if the oracle is replaced with a noisy oracle $\widehat{\mathcal{O}}$?

We were able to answer this question for the binary simplex decoding algorithm for simplex code of length $\mlen$, \simpl{\mlen}, if we had a specific known error rate $(\numerr/2^\mlen )$ for the noisy oracle. One may wonder if it is possible to improve this probability of success. Additionally, which algorithm is optimal for $\numq$ queries to the oracle?

Furthermore, this topic could be generalized by considering the probability of success given that the noise added to the oracle \emph{follows some particular distribution}, say a binomial distribution. This is more realistic because it is unlikely that one would know the exact number of errors that occurred in the query process.

A second way to expand upon this thesis is to answer questions about robustness to noisy oracles of a \emph{wide variety} of algorithms. For example, Grover's search algorithm \citep{grover1996fast} seems particularly suited for this line of inquiry. 

Lastly, for an algorithm which relies on \emph{multiple} oracle queries, one could analyze the difference in $P_{succ}$ when the oracle is replaced with a \emph{stateless} versus a \emph{stateful} noisy oracle (as described in chapter 7).




%If you feel it necessary to include an appendix, it goes here.
    \appendix

	
\chapter{Finite Geometries} \label{finite_geom_chapter}
Finite geometries are mathematical objects that codes map onto well, and they provide a new way of viewing codes. In this appendix, we will explore the properties of finite geometries that are vital to the classical Reed-Muller decoding algorithm with a focus on the projective geometry and affine or Euclidean geometry. This material is largely based on Appendix B in \citep{macwilliamssloane}.

\begin{definition}(Projective geometry) A finite projective geometry consists of a finite set $\Omega$ of \textit{points} $p, q, \ldots$ together with a collection of subsets $L, M, \ldots$ of $\Omega$ called \textit{lines}, which satisfies the following axioms (If $p \in L$ we say that $p$ lies on $L$ or $L$ passes through $p$.)
\begin{enumerate}
\item There is a unique line (denoted by $(pq)$) passing through any two distinct points $p$ and $q$.
\item Every line contains at least 3 points.
\item If distinct lines $L, M$ have a common point $p$, and if $q,  r\neq p$ are points of $L$ and $s, t\neq p$ are points of $M$, then the lines $(qt)$ and $(rs)$ also have a common point.
\item For any point $p$ there are at least two lines not containing $p$, and for any line $L$ there are at least two points not on $L$.
\end{enumerate}
A \textit{subspace} of the projective geometry is a subset $S$ of $\Omega$ such that
\begin{enumerate}[resume]
\item If $p,q$ are distinct points of $S$, then $S$ contains all of the points of $(pq)$
\end{enumerate}
A \textit{hyperplane} $H$ is a maximal proper subspace, so that $\Omega$ is the only subspace which properly contains $H$.
\end{definition}

\begin{definition}[Euclidean geometry] An affine or Euclidean geometry is obtained by deleting the points of a fixed hyperplane $H$ (called the hyperplane at infinity) from the subspaces of a projective geometry. The resulting sets are called the subspaces of the affine geometry.

A set $T$ of points in a projective or affine geometry is called \textit{independent} if, for every $x \in T, x$ does not belong to the smallest subspace which contains $T/\{x\}$. The \textit{dimension} of a subspace $S$ is $r-1$, where $r$ is the size of the largest set of independent points in $S$. In particular, if $S = \Omega$ this defines the dimension of the projective geometry.
\end{definition}

We denote Euclidean and projective geometries of dimension $m$ constructed from a finite field $GF(q)$ by $EG(m,q)$ and $PG(m,q)$, respectively. Now, we will discuss the projective and affine geometries that are obtained from finite fields, as they are most pertinent to Reed-Muller codes. (See \citep{macwilliamssloane} Appendix B for more information)

Let $GF(q)$ be a finite field and suppose $m \geq 2$. We will take the points of $\Omega$ to be the nonzero $(m+1)$-tuples in $GF(q)^{m+1}$ such that
\begin{equation*}
(a_0, \ldots, a_m) \equiv (\lambda a_0, \ldots \lambda a_m) \quad \lambda, a_i \in GF(q), \quad \lambda \neq 0.
\end{equation*}
These are called \textit{homogenous coordinates} for the points. 

A \textit{hyperplane} of subspace of dimension $m-1$ in $PG(m,q)$ consists of those points $(a_0, \ldots , a_m)$ which satisfy a homogenous linear equation 
\begin{equation*}
\lambda_0 a_0 + \lambda_1 a_1 + \ldots + \lambda_m a_m = 0, \quad \lambda_i \in GF(q)
\end{equation*}

The affine or projective geometry $EG(m,q)$ is obtained from $PG(m,q)$ by deleting the points of any hyperplane $H$. A subspace of $EG(m,q)$ of dimension $r$ is called an \textit{$r$-flat}. 

\chapter{Notation Guide}
It is an understatement to say that this thesis is \emph{notationally rich}. I have done my best to combine notations from three fields of computer science theory, two major textbooks on the subjects, and a dozen papers while resolving collisions. Undoubtedly, there were compromises. For the benefit of the reader, a description of all fixed variables are described below. One hopes that the reader will identify dummy variables and other notational conventions from context and good judgment. 

\subsection{Notation for Code Theory}
\begin{center}
\begin{tabular}{|c|l|}
\hline
Notation & Usage \\
\hline 
\mess & Message \\
\m{i} & $i$-th bit of message \\
\mlen & Message length \\
\cw & Codeword \\
\cwb{i} & $i$-th bit of codeword \\
\clen & Codeword length \\
\rc & Recieved codeword \\
\rcb{i} & $i$-th bit of received codeword \\
\ev & Error vector \\
\evb{i} & $i$-th bit of error vector \\
\mhat & Nearest message \\
\mindist & Minimum distance \\
\numerr & Number of errors in received codeword \\
\hline
\end{tabular}
\end{center}

\subsection{Notation for Reed-Muller Codes}
\begin{center}
\begin{tabular}{|c|l|}
\hline
Notation & Usage \\
\hline 
\degrm & Degree of Reed-Muller code\\
\lenrm & Length of Reed-Muller code ($\lenrm = 2^\numin$) \\
\numin & Number of inputs to vectors in RM code \\
\inv{i} & $i$-th input to $f$ in RM code \\
\bv{i} & Basis vector $i$ (``special'' function) \\
\nbv{i} & Negation of basis vector $i$ \\
\consto & Constant unit function \\
\rmexp & Set of valid monomials for codewords in $\mathscr{R}(\degrm, \numin)$ and \consto \\
\srmexp & $\lvert \rmexp \rvert$ \\
\hline
\end{tabular}
\end{center}

\subsection{Notation for Polynomial Interpolation}

\begin{center}
\begin{tabular}{|c|l|}
\hline
Notation & Usage \\
\hline 
\fsize & Size of field $\F_\fsize$ that polynomial's coefficients come from \\
\numq & Number of queries to oracle in interpolation algorithm \\
\tracef & Trace function defined in chapter 6 \\
\expf & Exponential function defined in chapter 6 \\
\hline
\end{tabular}
\end{center}

%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA 
% subfolder, e.g. 
% \bibliographystyle{APA/apa-good}  % or
 \bibliography{thesis}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.

\end{document}
