 % This is the Reed College LaTeX thesis template. Most of the work 
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a 
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment. 
% They won't show up in the document, and are useful for notes 
% to yourself and explaining commands. 
% Commenting also removes a line from the document; 
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in 
% the 2002-2003 Senior Handbook. Ask a librarian to check the 
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}


% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace} 
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{natbib}
\usepackage{mathtools}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

\usepackage{amssymb}
\usepackage{xspace}
\usepackage{color}
\usepackage[total={6in,8in}]{geometry}
\usepackage{amsthm}
\usepackage{mathrsfs}
\usepackage{enumitem}
\usepackage{centernot}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

% theorem, definition, remark, and example formatting
\theoremstyle{definition}
\newtheorem{theorem}{Theorem}[chapter]
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}

%Commands
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\K}{\mathbb{K}}
\newcommand{\F}{\mathbb{F}}
\newcommand{\sn}{\mathfrak{S}}
\newcommand{\ve}{\varepsilon}
\newcommand{\ketz}{\ensuremath{\lvert 0\rangle}\xspace}
\newcommand{\keto}{\ensuremath{\lvert 1\rangle}\xspace}
\newcommand{\ket}[1]{\ensuremath{\lvert #1\rangle}\xspace}
\newcommand{\ketpsi}{\ensuremath{|\psi\rangle}\xspace}
\newcommand{\bra}[1]{\ensuremath{\langle #1\vert}\xspace}
\newcommand{\Hplus}{\ensuremath{\lvert + \rangle}\xspace}
\newcommand{\Hminus}{\ensuremath{\lvert- \rangle}\xspace}
\newcommand{\inner}[2]{\ensuremath{\langle #1 \mid #2 \rangle}\xspace}
\newlength{\minuslength}
\settowidth{\minuslength}{$-$}
\newcommand{\hadamard}{
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & \hspace{\minuslength}1\\
1 & -1 
\end{bmatrix}
}

\title{Noisy Quantum Oracles: A Study of Algorithmic Robustness}
\author{Marika Swanberg}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2019}
\division{Mathematics and Natural Sciences}
\advisor{James D. Pommersheim}
%If you have two advisors for some reason, you can use the following
\altadvisor{Adam Groce}
%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%%
%% End Preamble
%%
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
    \chapter*{Acknowledgements}
%	I want to thank a few people.

% The preface is optional
% To remove it, comment it out or delete it.
   \chapter*{Preface}

 

    \tableofcontents
% if you want a list of tables, optional
 %   \listoftables
% if you want a list of figures, also optional
%    \listoffigures

% The abstract is not required if you're writing a creative thesis (but aren't they all?)
% If your abstract is longer than a page, there may be a formatting issue.
  \chapter*{Abstract}
My abstract
	
\chapter*{Dedication}

I dedicate this thesis to all of the individuals who funded my education at Reed College. Truly, I could not have done this without you.

  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

%The \introduction command is provided as a convenience.
%if you want special chapter formatting, you'll probably want to avoid using it altogether

	
\chapter{Introduction to Quantum Computing}


In the following chapter, we will delve into the basics of quantum computing. Before proceeding, it is important to realize that quantum computing describes a  fundamentally different \textit{model of computation} than the computer systems currently on the market. Much like the first programmable computer was realized by Alan Turing far before anyone built his remarkable invention, we too study quantum computing at a time when only the most rudimentary quantum computers are available in practice.


Due to the complex nature of quantum mechanics, there are a plethora of misconceptions about quantum computing. One of the most widespread is that quantum computers gain computational speedups by \textit{trying all possible solutions at once}. This is simply not true; the state of the quantum bits may be unknown, but they are still in only one state. We will go into this more later. Perhaps my favorite that I have heard upon starting my thesis is that \textit{quantum computers are like classical computers but in trinary}. False, we cannot efficiently simulate a quantum computer on a classical one. Lastly, probably the most optimistic is that \textit{quantum computers are faster at all tasks compared to classical computers}. Quantum computers are faster than classical computers at some tasks, such as searching an unstructured list, but they have the same asymptotic runtime\footnote{We will discuss what this means later} as classical computers for other tasks. I outline these common misconceptions not to criticize them, but rather to encourage the reader to exert patience and care with the following section, as quantum computing is so fundamentally different from classical computing.

The quantum mechanical properties upon which quantum computers are based have been studied extensively by physicists; we will avoid discussing such details and instead take these properties for granted in order to focus on the information-theoretic behavior of this new model of computation. 

\section{Information Representation}
Classical computers, i.e.~the computers that we all know and love, run on \textit{bits} or 1's and 0's. This is the fundamental unit of information in classical computers. In quantum computers, information is built upon an analagous concept, the \textit{quantum bit} or \textit{qubit}. We will discuss the defining properties of qubits, some of which may seem very different from those of bits.

In classical computing, each bit can be in one of two states--1 or 0.  We denote \textit{quantum states} by \ketz and \keto, pronounced ``\textit{ket zero}'' and ``\textit{ket one},'' respectively. This follows traditional \textit{Dirac notation}\footnote{See reference on notation--not yet written}. Unlike classical bits, qubits can be in a \textit{superposition} between these two states. That is, a qubit can lie in one of the infinite states ``between'' \ketz and \keto. We describe a quantum state as follows: $\ketpsi = \alpha \ketz + \beta \keto$, where $\alpha$ and $\beta$ are complex numbers and $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1$. This describes a probability distribution over the quantum states \ketz and \keto with \textit{amplitudes} $\alpha$ and $\beta$. We can also think of \ketpsi as a vector in a two-dimensional complex vector space, say $\C^2$. The states \ketz and \keto form an orthonomal basis in this complex vector space and are called the \textit{computational basis states}.

So how can we know which state a bit or qubit is in? In classical computing, we can simply read the bit to determine whether it's a 0 or a 1. Qubits are a little trickier. We cannot read \ketpsi to determine its exact amplitudes, $\alpha$ and $\beta$. As soon as we measure \ketpsi, the superposition will collapse to either \ketz with probability $ \lvert \alpha \rvert ^2$ or \keto with probability $ \lvert \beta \rvert ^2$. In other words, \ketpsi is like a weighted coin that we can flip once to get either heads or tails, but we have no way to directly measure the bias in the coin. Unlike a coin, as soon as \ketpsi has been measured to reveal some quantum state, \ketpsi will permanently collapse to that state, i.e.~$\ketpsi = 1\ketz + 0 \keto$ or $\ketpsi = 0\ketz + 1\keto$. Every time we measure it thereafter, we will observe the same state that we measured the first time. In the coin analogy, it's like we have a weighted coin that we can flip once to reveal heads or tails, and every subsequent flip always yields the initial state that we flipped to.

You may be wondering why this is useful. It seems impossible to build a model of computation on a fundamental unit of information that is unknowable, immeasurable. The beauty of quantum computation lies in the \textit{manipulation} of these immeasurable qubits such that by the time we measure them at the end, the result will inform us of the state they started in. Simply measuring the qubits before doing any transformations is fundamentally the same as  running a random number generator on a classical computer and trying a random possible solution. Obviously, this is not very useful, so we must \textit{transform} the qubits to say anything intelligent about the end result that we measure.

\section{Quantum Logic Gates}

As we already glossed over, $\ketz = 1 \ketz + 0  \keto$ and $\keto = 0 \ketz + 1 \keto$. We sometimes represent these quantum states as column vectors of the amplitudes: 
\begin{align*}
\ketz = \begin{bmatrix}
1\\
0
\end{bmatrix}
\text{ and }
\keto = \begin{bmatrix}
0\\
1
\end{bmatrix}
\end{align*}

Thus, we can represent a single-qubit transformation by a 2-by-2 matrix where the first column is the image of \ketz and the second column is the image of \keto under the transformation. For example, the quantum NOT gate can be realized as a matrix:
\begin{align*}
X = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\end{align*}
This takes a state $\ketpsi = \alpha \ketz + \beta \keto$ and transforms it into $X \ketpsi  = \beta \ketz + \alpha \keto$. Alternatively, by matrix multiplication we see that
\begin{align*}
X \ketpsi = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix} 
= 
\begin{bmatrix}
\beta \\
\alpha
\end{bmatrix}.
\end{align*}
So, any transformation on a single qubit can be represented by a 2-by-2 matrix. What about the transformation that, when given \ketz or \keto, outputs an \textit{equal superposition} of \ketz and \keto ? This would essentially give us a fair coin. We might represent that as follows:
\begin{align*}
\widehat{H} = \begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
There are a few problems with this transformation. First, applying this transformation, we get $\widehat{H} \ketz = 1\ketz + 1\keto$, which means that $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1^2 + 1^2 \neq 1.$ Since we view a quantum state as a probability distribution, the squares of the amplitudes must sum to 1. So, we must \textit{normalize} the matrix, to get 
\begin{align*}
\widehat{H} = 
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
Now, applying the transformation to our basis vectors, we get $\widehat{H} \ketz =(\ketz + \keto)/\sqrt{2}$ and $\widehat{H} \keto =(\ketz + \keto)/\sqrt{2}$. There is still a problem with this transformation. All quantum transformations must be \textit{reversible}, but we have mapped the two basis vectors to the same state, so the transformation is not reversible. To fix this, we will simply change the image of this transformation under \keto:
\begin{align*}
H = \hadamard
\end{align*}

The quantum gate above ($H$) is called the \textit{Hadamard transform} or \textit{Hadamard gate}. We will precisely define this quantum gate later in this chapter, as it is absolutely critical to work in quantum algorithms. It acts on the basis vectors as follows: $H\ketz = (\ketz + \keto)/\sqrt{2}$ and $H\keto = (\ketz - \keto)/\sqrt{2}$. 

Both of these states are in an equal superposition between \ketz and \keto, but they are distinct states. These states come up rather often, so they have been given the special shorthand notations \Hplus and \Hminus, respectively. 

\begin{definition}[Plus and Minus States] The following states (pronounced ``plus'' and ``minus'') will be used ubiquitously throughout this thesis in their shorthand form:
$$\Hplus = \frac{1}{\sqrt{2}}(\ketz + \keto)$$
$$\Hminus = \frac{1}{\sqrt{2}}(\ketz - \keto)$$
\end{definition}
The requirements that quantum transformations be reversible and normalized are encapsulated by the property that the matrix representation for any transformation must be \textit{unitary}. That is, $U^{\dagger} U = I$ where $U^{\dagger}$ is the transpose of the complex conjugate of $U$ and $I$ is the 2-by-2 identity matrix. Within those requirements, we can construct any transformation we like. What about transforming multiple qubits?

\section{Multiple Qubits}

To represent multiple qubits, we expand our two quantum states to many using tensor products \footnote{See appendix on tensor products--not yet written}. For example, in a two qubit system, we have the following basis states:
$$ \ketz \otimes \ketz,~\ketz \otimes \keto,~\keto \otimes \ketz, \text{ and } \keto \otimes \keto.$$ 
These four states can equivalently be written as:
$$ \ket{00},~\ket{01},~\ket{10}, \text{ and } \ket{11}.$$
Thus, any two-qubit state can be represented as a linear combination of these basis states, namely $\ket{\psi} = \alpha_{00}\ket{00}+ \alpha_{01}\ket{01} + \alpha_{10}\ket{10} + \alpha_{11}\ket{11}.$ In general, an $n$-qubit state can be represented as 
\begin{equation*}
\ket{\psi} = \sum_{i \in \Z_2^n} \alpha_{i}\ket{i},
\end{equation*}
where $i$ is the binary representation of the numbers $0, \ldots, n-1.$ The probability of observing a state \ket{i} upon measuring \ket{\psi} is $\lvert \alpha_{i} \rvert ^2.$ 

We may measure qubits individually as well. For example, measuring just the first qubit gives us 0 with probability 
\begin{equation*}
 \sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2,
\end{equation*}
leaving the post-measurement state
\begin{equation} \label{post_measurement}
 \ket{\psi'} = \frac{\sum_{i \in \Z_2^{n-1}} \alpha_{0i} \ket{\alpha_{0i}}}{\sqrt{\sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2}}.
\end{equation}

\section{Entangled States}

Something that follows from the post-measurement equation (\ref{post_measurement}) but which is not self-evident is the concept of \textit{entangled states}. Consider the following state:
\begin{equation*}
\ket{\beta_{00}} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}
\end{equation*}
Suppose we measured the first qubit. We will observe \ket{0} with probability 1/2, and the resulting post-measurement state is: \ket{\beta_{00}'} = \ket{00} (by the equation above). How can this be? We only measured the first qubit, and yet, we now have information about both qubits. This is precisely because the state is entangled. Another example of an entangled state is:
\begin{equation*}
\ket{\beta_{01}} = \frac{\ket{01} + \ket{10}}{\sqrt{2}}
\end{equation*}
These two states, \ket{\beta_{00}} and \ket{\beta_{01}}, are the first two \textit{Bell states} or \textit{EPR pairs}. Quantum entanglement is a powerful computational tool and will be used in many quantum algorithms in the rest of the text. 

Now that we have the means to represent an $n$-qubit state, the state transformations can be represented by $2^n$-by-$2^n$ matrices. In particular, the $n$-qubit Hadamard transform is $H^{\otimes n} = H \otimes H \otimes \ldots \otimes H$, i.e.~$n$ tensor products.

\section{Hadamard Transform}

As promised, a more precise definition of the Hadamard transform follows. This is by far the most important unitary transformation in the field of quantum algorithms.

\begin{definition}[Hadamard Transform] The n-qubit Hadamard transform, $H^{\otimes n}$ acts on a state $\ket{\psi}$ as follows:
\begin{equation*}
H^{\otimes n} \ket{\psi} = \frac{1}{\sqrt{2^n}} \sum_{j \in \{0,1\}^n} (-1)^{\psi \cdot j} \ket{j}
\end{equation*}
where $\psi$ and $j$ are $n$-bit binary numbers.
\end{definition}
Upon first glance, this formula is quite uninviting, so I will provide a quick guide for some common states. 
\begin{align*}
& H^{\otimes n} \ketz^{\otimes n} = \Hplus^{\otimes n} \\
& H^{\otimes n} \keto^{\otimes n} = \Hminus^{\otimes n} \\
\end{align*}
Since $H = H^{-1}$, we also have that
\begin{align*}
& H^{\otimes n} \Hplus^{\otimes n} =  \ketz^{\otimes n} \\
& H^{\otimes n} \Hminus^{\otimes n} = \keto^{\otimes n}. \\
\end{align*}
This concludes the section on the Hadamard transform, though the careful (or confused) reader will find that they will have to refer back to this section in the coming chapters as the Hadamard transform is central to this thesis. 

\section{Phase}
Up to this point, we have used the term \textit{phase} slightly ambiguously. The exact meaning of the word differs depending on context, so we will flesh these out. In fact, some of the original results in this thesis puzzled both student and adviser until we revisited the exact distinction between \textit{global phase} and \textit{relative phase}.

Consider the states $\ket{\psi}$ and $e^{i \theta}\ket{\psi}$,\footnote{HOW TO FORMAT EULERS NUMBER} where $\ket{\psi}$ is a state vector and $\theta$ is a real number. These two phases are equivalent up to their \textit{global phase shift}. As a thought experiment, one may wonder what happens upon measuring the two states. Suppose $M_x$ is our measurement operator. Then, the probabilities for $x$ occurring upon measurement are: $\bra{\psi}M_x^\dagger M_x \ket{\psi}$ and $\bra{\psi}e^{-i\theta}M_x^\dagger M_x e^{i\theta}\ket{\psi} = \bra{\psi}M_x^\dagger M_x \ket{\psi}.$ So, the two states are indistinguishable with respect to measurements. 

The other notion of phase that will come up is \textit{relative phase}. Consider the states $\Hplus = (\ketz + \keto)/\sqrt{2}$ and $\Hminus = (\ketz- \keto)/\sqrt{2}$. These two states have the same \textit{magnitude} on their amplitudes (i.e.~$1/\sqrt{2}$) on both $\ketz$ and $\keto$. Thus, they have the same \textit{relative phase}. However, we can distinguish between the states $\Hplus$ and $\Hminus$ if we apply a 45 degree clockwise rotation to the states, we may distinguish between the states with probability 1 upon measurement. So, despite having the same relative phase, the states are, in fact, computationally distinct.

\section{Universal Quantum Gate}
In classical computation, there are three basic logic gates: NOT, AND, and OR. We can combine these gates to represent any possible logical expression. Furthermore, the NAND gate and the NOR gate, which we get from taking the negation (NOT) of the output of AND and OR respectively, are said to be \textit{universal gates}. This means that we can construct the three basic logic gates just from NAND gates or just from NOR gates. So, NAND and NOR are universal in that any logical expression can be represented with just NAND or NOR circuits. This is very useful in practice because NAND gates are very cheap to construct, so using only NANDs can keep the cost of a computer chip down.

A natural question, then, is whether there exists a quantum analogue, a universal quantum gate. The short answer is that there is no one universal quantum gate, however the following three gates are enough to make an \textit{arbitrary approximation} of any quantum gate: Hadamard, CNOT, and phase gate. 

The phase gate is defined
\begin{align}
\begin{bmatrix}
1 & 0\\
0 & i 
\end{bmatrix}
\end{align}
And the CNOT, or controlled-NOT gate acts on two qubits by flipping the second qubit if and only if the first qubit is a 1. 
\begin{align}
\text{CNOT} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}
\end{align}
As you can see, by multiplying the CNOT matrix by the column vector $(a\quad b\quad c\quad d)^\intercal$ this takes $a\ket{00} + b\ket{01} + c\ket{10} + d\ket{11}$ and transforms it into $a\ket{00} + b\ket{01} + c\ket{11} + d\ket{10}$, thereby swapping the amplitudes on the states $\ket{10}$ and $\ket{11}$, or equivalently ``flipping'' the second bit of the state if the first bit is a 1. 

The \textit{gate complexity} of a unitary transformation is the minimum number of gates needed to implement the circuit. 

\section{No-Cloning Theorem}

Suppose my friend has a secret state \ket{\psi} that she wants me to have a copy of.  Classically, we could easily build a circuit to copy her state $x = 0$ or $x = 1$ without measuring it. We would simply initialize a temporary register to 0, and our circuit would write $x \oplus 0$, the XOR of $x$ and 0, to the destination register. 

The quantum case is a bit trickier. Suppose \ket{\psi} is only known to our friend. We wish to copy this exact state into a target slot, which starts out in some standard known state \ket{s}. Thus, the initial state of our copying machine is $\ket{\psi} \otimes \ket{s}$. Now, we apply some unitary operation $U$ to these registers to obtain
\begin{equation}
\ket{\psi} \otimes \ket{s} \xrightarrow{U} U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
Now, suppose this quantum copying circuit works for two arbitrary states \ket{\psi} and \ket{\phi}. Then, we have
\begin{equation}
U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
and
\begin{equation}
U(\ket{\phi} \otimes \ket{s}) = \ket{\phi} \otimes \ket{\phi}
\end{equation}
Now, taking the inner product of these two equations gives\footnote{Check this math???} 
\begin{align}
(\bra{\psi} \otimes \bra{s})U^{\dagger}U \ket{\phi} \otimes \ket{s}
& = \inner{\psi}{\phi} \otimes \inner{s}{s} \otimes I \\
 & = \inner{\psi}{\phi}\\
& = (\bra{\psi}\otimes \bra{\psi})(\ket{\phi}\otimes \ket{\phi}) \\
& = \inner{\psi}{\phi} \otimes \inner{\psi}{\phi} \\
& = (\inner{\psi}{\phi})^2
\end{align}
Line (5) holds because $s$ is normalized. Critically, lines (5) and (8) give
\begin{equation*}
 \inner{\psi}{\phi} = (\inner{\psi}{\phi})^2.
\end{equation*}
However, this equation is only true if $\ket{\psi} = \ket{\phi}$ or if $\ket{\psi}$ is orthogonal to $\ket{\phi}$. Thus, a general cloning device can only clone states that are orthogonal, which means that we cannot clone states which we know nothing about. 

This is an important fundamental difference between the classical and quantum models of computation. We take for granted in classical computing that we can copy any unknown states, and much of classical computation relies upon this fact. 

\section{Quantum Oracles}

Within any computational model, some computations are ``expensive'' for any number of reasons: they require large amounts of resources such as time, space, or circuitry. For this reason, one may wish to outsource such computations to third parties, which we will call \textit{oracles}. As the name would suggest, oracles may be queried on inputs, and magically in one time step will output the result of the computation on the input, for a particular function. More concretely, an oracle $\mathcal{O}_f$ outputs $f(x)$ when queried on the input $x$.

Classical oracles are used throughout computer science theory to abstract computations and reason about algorithms and protocols. Quantum oracles differ  from classical oracles in significant ways. 

\subsection{Quantum Queries}
First, consider the action of the query. In order to ensure the reversibility of quantum queries, we must maintain two registers: a query register, and a response register. The query register contains the input $x$ on which we wish to query the oracle. This register remains untouched by the oracle. The response register has some initial state $\ket{r}$ and after the quantum query takes place, has the state $\ket{r\oplus f(x)}$ where $f$ is the function that the oracle computed. 

\begin{definition}[Quantum Oracle Query] A quantum oracle has the following action on the query and response registers $\mathcal{O}_f : \ket{x, r} \rightarrow \ket{x, r \oplus f(x)}$.
\end{definition}

In addition to the query and response registers, quantum oracles have the special ability to take as input a \textit{superposition of queries} and output a \textit{superposition of responses}. More precisely, suppose we have a superposition of queries of length $n$ 
\begin{equation*}
\ket{x} = \sum_{i \in \Z_2^n} \alpha_i \ket{i}
\end{equation*}

and $n$ response registers $\ket{r} = \ket{r_1} \otimes \ldots \ket{r_n}$. Then, the query $\mathcal{O}(\ket{x, r})$ results in the following state
\begin{equation*}
\ket{x} = \sum_{i \in \Z_2^n} \alpha_i \ket{i, r_i \oplus f(i)}.
\end{equation*}
It is important to note that this only constitutes \textit{one} quantum query to the oracle.


\section{Phase-kickback Trick}

Suppose we have ``black-box access'' to some function $f$. That is, we can query $f$ on some input $x$ and it will compute $f(x)$. We record the output in a \textit{response register}, the second qubit in the example below; the first qubit is called the \textit{query register}. This can be modeled as the following unitary transformation:
\begin{equation*}
\ket{x, z} \xrightarrow{f} \ket{x, z \oplus f(x)}
\end{equation*}
Given $z$, we can deduce what $f(x)$ is by taking $z \oplus f(x) \oplus z = f(x)$, addition mod 2. This is essential to keeping the transformation unitary. 

One common query method is using what is called the \textit{phase kickback trick}. The basic idea is that we initialize the response register to \Hminus so that both the query and response registers stay the same after the query, and the value of $f(x)$ is encapsulated by the phase of the state. More concretely, we have:
\begin{align*}
\ket{x} \otimes \Hminus 
& = \ket{x} \otimes \frac{1}{\sqrt{2}}(\ketz - \keto)\\
& = \frac{1}{\sqrt{2}}(\ket{x, 0} - \ket{x, 1}) \\
& \xrightarrow{f} \frac{1}{\sqrt{2}}(\ket{x, f(x)} - \ket{x, 1 \oplus f(x)} \\
& = \ket{x} \otimes \frac{1}{\sqrt{2}}\bigg(\ket{f(x)} - \ket{\overline{f(x)}}\bigg) \\
& = (-1)^{f(x)} \ket{x} \otimes \ket{ -}.
\end{align*}
Since the response register remains unchanged, we will generally omit this from future computations, though technically it must be present to preserve the reversibility of the query. 
\section{Bernstein-Vazirani Algorithm}

Now that we have seen a few tricks of the trade, we will dive into a quantum algorithm. The Bernstein-Vazirani algorithm forms the foundation for many of the algorithms in the rest of this thesis, so a solid grasp of this section will yield high dividends. 

[CITATION TO VAZIRANI PAPER] The Bernstein-Vazirani algorithm solves the following problem: for $N = 2^n$, we are given $x \in \{0,1\}^N$ with the property that there exists some unknown $a \in \{0,1\}^n$ such that $x_i = i \cdot a$ mod 2. The goal is to find $a$. In other words, $x_i$ is the $i$th bit of the binary representation of $x$. 

First, an overview of the algorithm: we will start in the state $\ketz^{\otimes n}$, the $n$-qubit zero state, and apply a $n$-qubit Hadamard transform to get an equal superposition of the states, i.e.~ $\Hplus^{\otimes n}$. Next, we perform a quantum query using the phase kickback trick to store the bits of $x$ in the phase of our state. Then, another Hadamard transform on all $n$ qubits. With a simple measurement, we obtain $a$ with probability 1. Now, for the details.

\begin{align}
 \ketz^{\otimes n}
& \xrightarrow{H^{\otimes n}}\frac{1}{\sqrt{2^n}} \sum_{i \in \{0,1\}^n} \ket{i} \\
& \xrightarrow{\mathcal{O}_x} \frac{1}{\sqrt{2^n}} \sum_{i \in \{0,1\}^n} (-1)^{x_i}\ket{i}\\
& \xrightarrow{H^{\otimes n}} \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{x_i} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j} \label{confusingline}\\
& = \ket{a}
\end{align}
This computation looks a mess for the beginner, so let's try to make sense of it. The first three lines follow from the definitions of the functions (confused readers may need to review the defined action of $H$ and $\mathcal{O}$ with phase-kickback). The last step, however, is less clear-cut. Note that $(-1)^{x_i} = (-1)^{(i \cdot a) \text{ mod } 2} = (-1)^{(i \cdot a)}$, given by $x_i = (i \cdot a)$ mod 2 in the problem statement. Thus, we may manipulate \ref{confusingline} as follows:
\begin{align}
 \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{x_i} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j}
& = \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{(i \cdot a)} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j} \\
& = \frac{1}{2^n}  \sum_{j \in \{0,1\}^n}\sum_{i \in \{0,1\}^n} (-1)^{i \cdot(a + j)}  \ket{j} \label{equals_a}
\end{align}
Now, note that $(-1)^{i \cdot(a + j)}$ is equal to 1 if and only if $j = a$. Thus, the inner summation evaluates to $1/2^n$ when $j =a$ and zero otherwise. So, \ref{equals_a} is simply the state $\ket{a}$; thus, measurement in the computational basis will yield $a$ with probability 1. This algorithm is the bread and butter of this thesis, so to speak, and is critical to the decoding algorithms we will study in a few chapters. 

\section{Measuring in Non-Computational Bases}
Thus far, all of the measurements we have performed have been done in the \textit{computational basis}. Given a state $\ket{\psi} = \alpha\ketz + \beta\keto$, upon measurement in the computational basis ($\ketz$ and $\keto$) this state will collapse to $\ketz$ with probability $\lvert \alpha \rvert^2$ and $\keto$ with probability  $\lvert \beta \rvert^2.$ But, this choice of basis is arbitrary. We could instead measure $\ket{\psi}$ in the Hadamard basis, $\Hplus$ and $\Hminus$. Then, we can re-express the state $\ket{\psi}$ as follows
\begin{equation*}
\ket{\psi} = \alpha\ketz + \beta\keto = \alpha \frac{\Hplus + \Hminus}{\sqrt{2}} + \beta \frac{\Hplus - \Hminus}{\sqrt{2}} = \frac{\alpha + \beta}{\sqrt{2}} \Hplus + \frac{\alpha - \beta}{\sqrt{2}} \Hminus.
\end{equation*}
So, measuring $\ket{\psi}$ in the Hadamard basis yields $\Hplus$ with probability $(\alpha + \beta)^2/2$ and $\Hminus$ with probability $(\alpha - \beta)^2/2$.

More generally, we can represent any state $\ket{\psi}$ as a unique linear combination of any orthonormal basis, $\ket{\psi} = \alpha \ket{a}+ \beta \ket{b}$ (where $\ket{a}$ and $\ket{b}$ form an orthonormal basis). Then, the probabilities of measuring $\ket{a}$ and $\ket{b}$ are $\alpha^2$ and $\beta^2$, respectively.

\section{A Note on Asymptotic Complexity}

In this thesis, we will be studying the asymptotic complexity of various different metrics, including: runtime, query complexity, certificate complexity, block sensitivity, and perhaps circuit complexity\footnote{These complexity metrics will be defined later.}. Those who are unfamiliar with asymptotic complexity (also termed Landau-Bachmann notation in mathematics) should review the appendix on the topic. 


\chapter{Introduction to Code Theory and Error-Correcting Codes}	

No real-life communication channel is perfect: internet packets occasionally get dropped, radio transmissions are drowned out by static interference, and data on storage media get corrupted. Distinguishing signal from noise is a problem as old as communication itself. One way of adapting to this problem is introducing redundancy into the transmission. If the information we want to transmit is encoded in more than one place, then there is a higher probability that at least one of the copies will make it through even if some of the copies are plagued with errors. This is precisely the approach that error-correcting codes take. 


Error-correcting codes were developed to enable the reliable transmission of messages over noisy communication channels. There are many examples of their use today, and many error-correcting codes have been developed for specific noise models. In CDs, error-correcting codes prevent (small) scratches from leading to data loss. They are also used for transmitting photographs from rovers and telescopes throughout the solar system to earth. 

Error-correcting codes are important for the practical implementation of quantum computers because current physical implementations of quantum computers are plagued by \textit{quantum decoherence}. That is, the qubits are difficult to fully isolate from their environment and thus become entangled with their surroundings, leading to noisy computations. For now, we will forget about quantum computing and plunge into the code theory.

\section{Preliminaries}
There are many kinds of communication channels, but we will focus on \textit{binary symmetric channels}. 

\begin{definition}[Binary Symmetric Channel] A binary symmetric channel is a classical channel that can transmit a string of 0's and 1's. If a bit $b$ is sent, $b$ will be flipped to $\lnot b$ with probability $p \in [0,1]$, and $b$ will be transmitted correctly with probability $1-p$ where $p < \frac{1}{2}$.
\end{definition}

This type of channel is \textit{binary} because we are transmitting bits, and it is \textit{symmetric} because there is an equal probability of a 0 flipping to a 1 and the reverse. Note that if our binary symmetric channel flips bits with probability $p > 1/2$, we could easily create a channel with $p <1/2$ by negating every bit on the receiving or sending end. Additionally, note that if $p=1/2$, it is  information-theoretically impossible to recover the message, so we don't consider this case and frankly it is dubious to even call that a communication channel.

In the following sections we will investigate how to mitigate the information loss from random errors in binary symmetric channels where $p<1/2$. In the following chapter, and throughout the thesis, we will use the terms ``noise'' and ``errors'' interchangeably.

\section{Overview of Transmission}

Thus far, we have only described the parts of the communication system in vague terms; now, we will assuage the reader's thirst for clarity and precision. We will, of course, delve further into each of these parts later.

There are five basic components in the communication system we consider. The \textit{message source}, or \textit{sender}, transfers a message $m = m_1 \ldots m_k$ to the \textit{encoder}. The \textit{encoder} outputs the \textit{codeword} $c = c_1\ldots c_n$ associated with message $m$ and passes it along to the \textit{binary symmetric channel}. The \textit{channel} incorporates an \textit{error vector} $e = e_1 \ldots e_n$ into the codeword and outputs the received vector $r = e + c = r_1 \ldots r_n$ where the addition is bitwise and modulo 2. Then, the \textit{decoder} takes the received vector (also referred to as the ``noisy codeword'') $r$ and outputs $\hat{m}$, an estimate of message $m$ to the \textit{receiver}. A schematic is provided below.
\begin{equation*}
\text{Sender} \xrightarrow{\text{Message }m} \text{Encoder} \xrightarrow{\text{Codeword }c} \text{Channel} \xrightarrow{\text{Rcvd.~Vect.~}m+e} \text{Decoder} \xrightarrow{\text{Est. }\hat{m}} \text{Receiver}
\end{equation*}

\section{Encoding}
The job of the encoder is to introduce well-defined redundancy into the transmission. We will describe a general encoding scheme that applies to all linear codes, though the more advanced encoding algorithms we will explore later will deviate from this general form.

In this general code $\mathscr{C}$, the message will be encoded as a codeword that consists of a series of \textit{check symbols} appended to the original message $m$. The check symbols encode redundancy into the codeword. More concretely, the bits of the codeword $c$ associated with message $m$ are defined
\begin{equation*}
c_1 = m_1, \quad c_2 = m_2, \quad \ldots,\quad c_k = m_k
\end{equation*}
and the last $n-k$ bits of $c$ are all check symbols $c_{k+1} \ldots c_n$. The check symbols are determined by the codeword, and they are chosen so that 
\begin{equation*}
P \begin{bmatrix}
c_{1}\\
c_{2}\\
\vdots\\
c_n
\end{bmatrix} = 
P \mathbf{c}^{tr}= 0
\end{equation*}
where $P$ is the \textit{parity check matrix} for the code $\mathscr{C}$.

\begin{definition}[Parity Check Matrix] The parity check matrix $P$ for a linear code $\mathscr{C}$ is given by 
\begin{equation*}
P = [A \mid I_{n-k}]
\end{equation*}
where $A$ is some fixed $(n-k) \times n$ binary matrix and $I_{n-k}$ is the $(n-k)\times(n-k)$ identity matrix.
\end{definition}
A code $\mathscr{C}\subset \Z_2^n$ is simply the set of all codewords $c$ that satisfy the equation $P\mathbf{c}^{tr} = 0$. In other words, $\mathscr{C} = \{c\in \Z_2^n \mid P\mathbf{c}^{tr} = 0\}$.
\begin{example} \label{P_ex} Suppose our parity check matrix is
\begin{equation*}
P = 
\left[
\begin{array}{ccc|cc}
0 & 1 & 1 & 1 & 0 \\
1 & 0 & 1 & 0 & 1\\
\end{array}
\right]
\end{equation*}
This defines a code $\mathscr{C}$ with $n = 5$ and $k = 3$. For this code,
\begin{equation*}
A = 
\begin{bmatrix}
0 & 1 & 1\\
1 & 0 & 1
\end{bmatrix}
\end{equation*}
Moreover, the message $m = m_1m_2m_3$ is encoded into a codeword $c = c_1 c_2 c_3 c_4 c_5$ such that $c_1 = m_1$, $c_2 = m_2$, and $c_3 = m_3$ and then
\begin{align*}
& c_4 = -(c_2 + c_3) \equiv c_2 + c_3 \pmod 2\\
& c_5 =  -(c_1 + c_3) \equiv c_1 + c_3 \pmod 2
\end{align*}
\end{example}

Given a parity check matrix and a message, the encoder needs to output the corresponding codeword. This is best done with the \textit{generator matrix}.

\begin{definition}[Generator Matrix] Given a parity check matrix $P = [A\mid I_{n-k}]$ for a code $\mathscr{C}$, the corresponding generator matrix is $G = [I_k \mid A^{tr}]$.
To generate the codeword $c$ for a message $m$, simply multiply the matrices $c = mG$.
\end{definition}

\begin{remark}
Note that a given code $\mathscr{C}$ may have more than one matrix that generates it.
\end{remark}

\begin{example}
Continuing with the code defined in example \ref{P_ex}, we have that the corresponding generator matrix for this code is 
\begin{equation*}
G = 
\left[
\begin{array}{ccc|cc}
1 & 0 & 0 	& 0 & 1	  \\
0 & 1& 0	& 	1 & 0 \\
0 & 0 & 1 	& 1 & 1
\end{array}
\right]
\end{equation*}
\end{example}

\section{Decoding}
As was alluded to, the channel will (term-wise) add some unknown error vector $e = c_1 \ldots e_n$ to the codeword so that the decoder receives a ``noisy codeword'' or ``received vector'' $r = c + m \pmod 2$. 

The decoder has a significantly harder job than the encoder. Namely, it must take this noisy codeword $r$ and return the message $m$ that produced the vector. Any given decoding algorithm will not always be successful; sometimes there will be so many errors in a given received codeword that it is impossible to parse which message was sent. That said, some decoding algorithms are more effective than others, and this distinction comes down to the properties of the code $\mathscr{C}$.

\subsection{Properties of Vectors}

One of the key defining properties of a vector is its Hamming weight.
\begin{definition}[Hamming weight] The Hamming weight of a vector $x = x_1  \ldots x_n$ is the number of nonzero $x_i$, and is denoted by $wt(x)$.
\end{definition}

\begin{example}
For example, the vector $x = 0110100111$ has Hamming weight 6.
\end{example}

Another important property in relating two vectors is their \textit{Hamming distance}.
\begin{definition}[Hamming distance] The Hamming distance of two vectors $x = x_1  \ldots x_n$  and $y = y_1 \ldots y_n$ is defined $dist(x,y) = wt(x \oplus y)$, where the XOR is bitwise.
\end{definition}

Note that one could write this definition in many ways (i.e.~addition or subtraction mod 2), but the heuristic idea behind computing the Hamming distance between two vectors is to add up how many of their bits differ. This concept is a fundamental building block for the decoding algorithms we will encounter.

\begin{example}
dist(010101, 100111) = 3
\end{example}

\subsection{Errors}
Earlier we defined the error vector $e = e_1 \ldots e_n$ where $e_i \in \Z_2$. The received vector is defined $r = m + e$, which implies that $e_i = 1$ if and only if the $i$-th bit of the codeword was corrupted in transmission and $e_i = 0$ otherwise. Since the binary symmetric channel corrupts a single bit with probability $p$, it follows that $P[e_i = 1] = p$.  One may wonder: \textit{what is the most probable error vector?} This would certainly help the decoder to determine which codeword was sent given the noisy codeword.

Since the corruption of bits by the communication channel are independent of one another, we can compute the probability that the error vector $e$ is some particular vector $x$:
\begin{equation} \label{p_error_vect}
P[e = x] = \prod_{i = 1}^{n} P[e_i = x_i] = p^{wt(x)}(1-p)^{n-wt(x)}.
\end{equation}
This perhaps goes without saying, but the error vector is entirely independent of the message.
\begin{example}
Suppose $n=5$. Then, $P[e = 10110] = p(1-p)p^2 (1-p) p = p^3(1-p)^2$.
\end{example}
So, now we want to maximize the equation $$P[e = x] =  \prod_{i = 1}^{n} P[e_i = x_i] = p^{wt(x)}(1-p)^{n-wt(x)}$$ to figure out the most likely error vector $x$ and thus the best decoding algorithm. We have that $p < \frac{1}{2}$, so it follows that the maximum probability is achieved when $n-wt(x)$ is as large as possible. This occurs when $x = \mathbf{0}$, the zero vector. In other words, \textit{the most likely error vector is one that indicates that none of the bits were corrupted}.
 
 In terms of decoding, this fact implies that the optimal decoding algorithm assumes that the weight of $e$ is as small as possible. If the received vector $r$ is not a codeword, then the decoder should output the nearest codeword $c$ with respect to the Hamming distance $dist(r,c)$ In other words, $c = \min_{c' \in \mathscr{C}}\{dist(c', r)\}$. This decoding method is called \textit{nearest neighbor decoding} and it is the strategy we will employ.

\subsection{Properties of Codes}
Now that we have defined the nearest neighbor decoding strategy, one may wonder how many errors a given code can correct. Before proceeding, note the distinction between \textit{error detection} whereby the code informs the decoder that $x$ number of errors occurred and \textit{error correction} whereby the decoder is able to correct the $x$ errors. Analyzing the error-correcting and error-detecting properties of linear codes will require some additional metrics on codes.

One of the traits of a code that is most pertinent to its error-correcting properties is called its \textit{minimum distance}. This captures the worst-case correction performance of a code on a nearest neighbor decoding algorithm.

\begin{definition}[Minimum Distance] The minimum distance $d$ of a code $\mathscr{C}$ is $$d(\mathscr{C}) = \min_{x,y \in \mathscr{C}} \{dist(x,y)\}.$$
\end{definition}

The minimum distance of a code $\mathscr{C}$ encapsulates how closely packed the codewords are in $n$-dimensional space. This determines how many errors we can correct with nearest-neighbor decoding.
\begin{theorem} A linear code $\mathscr{C}$ with minimum distance $d$ can correct $\lfloor \frac{1}{2} (d-1) \rfloor$ errors. 
\end{theorem}

\begin{proof}
Suppose our code $\mathscr{C}$ has minimum distance $d$. Now, imagine each codeword has an $n$-dimensional sphere\footnote{Or, for the topologists among you, let's call it the closed $n$-ball $\overline{B(c, t)}$ where $c\in \Z_2^n$ is a codeword.} of radius $t$ around it. Now, if $t$ is small enough, none of the spheres will overlap. This means that a received vector $r$ with at most $t$ errors will be decoded correctly using nearest neighbor decoding. Now, the question is: what is the largest that the radius $t$ can be and still maintain the non-overlapping $n$-spheres? This is precisely how error-correcting codes can be translated to sphere packing problems.

Anyway, if $d = 2t+1$, then spheres with radius $t$ will not overlap. Thus, we have that $t = \frac{1}{2}(d-1)$ is the largest that $t$ may be. Now, we can correct up to $t$ errors since the $n$-spheres do not overlap, but the number of errors in a codeword are discrete integer values. Thus, we can only guarantee that $\lfloor t \rfloor$, or $\lfloor \frac{1}{2} (d-1)\rfloor$ errors can be corrected.\\
\end{proof}

\subsection{Decoding Errors}
One may wonder what happens if there are more than $\lfloor \frac{1}{2}(d-1)\rfloor$ errors occur during transmission. In this case, the received vector $r = e+c$ may be closer to some other codeword $c'$, so nearest neighbor decoding will decide that $c'$ was the original codeword rather than $c$, the actual codeword that was sent. This is called a \textit{decoding error} because the decoder erroneously returned $c'$ instead of $c$. This is not ideal, and code theorists study the probability of this occurrence, denoted $P_{err}$. 

[****If I want to talk about this then I have to introduce cosets and syndromes and I don't feel like doing that so let's just skip it]


\chapter{Introduction to Learning Theory}

Given oracle (``black box'') access to some function $f$, how many queries would it take to determine some property of $f$? This is precisely the question that learning theory is concerned with. This field is motivated in part by the idea that the oracle computes some ``expensive'' function, i.e.~one that is time or space intensive. Minimizing the number of queries that an algorithm has to make to an oracle keeps the computational cost expended on oracle queries low.

\section{Basics of Concept Learning}
Consider the oracle function $f$. This is a \textit{Boolean function}, as it takes inputs in $\{0,1\}^n$ for some fixed $n$ and outputs 0 or 1. In the field of learning theory, $f$ is called a \textit{concept}.\footnote{CITE Quantum vs classical learnability by Servedio!!!}

\begin{definition}[Concept] A concept $f$ over $\{0,1\}^n$ is a Boolean function $f: \{0,1\}^n \rightarrow \{0,1\}$. Equivalently, a concept can be viewed as a subset of $\{0,1\}^n$, namely $f = \{x \in \{0,1\}^n \mid f(x) = 1\}$.
\end{definition}

Computer scientists do not care how many queries it takes to learn the properties of some particular concept, but they are more interested in knowing how the number of queries \textit{scales asymptotically} with the size of the input. To take care of this, they came up with 
\textit{concept classes}. Traditionally, concept classes are denoted by $\mathcal{C}$, but we will instead call them $\mathcal{F}$ to avoid confusion with codes $\mathscr{C}$. 

\begin{definition}[Concept Class] A concept class $\mathcal{F} = \cup_{n \geq 1} f_n$ is a collection of concepts where $f_n = \{f \in \mathcal{F} \mid f \text{ is a concept over } \{0,1\}^n\}$.
\end{definition}

\begin{example}
Suppose the domain we are working with is sets of $2^n$ people instead of $\{0,1\}^n$. Then, we can define a concept like $f = \{ x\in 2^n \text{-set of people} \mid \text{ Marika has met } x\}$. Then, $f(\text{``Jamie Pommersheim''}) = 1$, but $f(\text{``Alan Turing''}) = 0$, sadly. Now, the concept class is just the union of these concepts over all possible domain sizes (ignoring the fact that the number of people that have existed is finite). This example is perhaps not mathematically rigid, but it should give the reader an intuition for what concept classes are.
\end{example}

The example we outlined refers to a specific kind of function called a \textit{membership oracle}.
\begin{definition}[Membership Oracle] A membership oracle $MQ_f$ is an oracle that returns 1 on input $x$ if and only if $x \in f$ or equivalently if $f(x)=1$. This is called a \textit{membership query}.
\end{definition}

Given some membership oracle $MQ_f$, the goal of the learning algorithm is to construct a \textit{hypothesis} $h: \{0,1\}^n \rightarrow \{0,1\}$ that is equivalent to $f$, i.e.~$\forall x \in \{0,1\}^n, f(x) = h(x)$. While their outputs are equal, $f$ and $h$ may compute their outputs quite differently, but this is irrelevant.

As with code theory, focusing on the properties of the concept classes can provide insight into the query complexity of that set of functions. We will look at two particular properties of concept classes: certificate complexity and block sensitivity. They provide some bounds on query complexities, however they are mostly discussed for fun rather than practical results.
\section{Certificate Complexity}

\begin{definition} (Certificate) For an input $x \in \{0,1\}^n$, a set $S \subseteq [n]$ is a certificate on $x$ if, for all $y \in \{0,1\}^n$ such that $x_i = y_i \forall i \in S$, we have $f(x) = f(y)$. Then, $C_x(f) = \min_{S\in\mathscr{S}_x(f)} \{\lvert S \rvert \}$ where $\mathscr{S}_x(f)$ is the set of all certificates on for input $x$ on function $f$.
\end{definition}
Now, obviously we would like to generalize this property to the whole function and not just particular inputs.
\begin{definition}[Certificate Complexity] The certificate complexity of a concept $f: \{0,1\}^n \rightarrow \{0,1\}$ is 
\begin{equation*}
C(f) = \max_{x\in \{0,1\}^n}\{C_x(f)\} = \max_{x\in \{0,1\}^n}\{\min_{S\in\mathscr{S}_x(f)} \{\lvert S \rvert \}\}
\end{equation*}
\end{definition}

This may make more sense with some examples.
\begin{example}
Let $f(x_1, x_2, \ldots, x_n) = x_1$ where $x_i \in \{0,1\}$. Suppose the input we are looking at is $x = (0, \ldots, 0)$. Then, the smallest certificate for $x$ is $S = \{1\}$, or just the first input since $f(y) = f(x)$ for all $y$ that have a zero in the first bit. Now, this property holds for all $x$ in the input, not just the input of all zeros. So, the overall certificate complexity of $f$ is 1. This makes sense intuitively because we only need to look at one bit of the input to determine what the output will be.
\end{example}
\begin{example}
Now, perhaps the other extreme example. Suppose $f(x_1, \ldots, x_n) = x_1 \oplus x_2 \oplus \ldots \oplus x_n$ i.e.~it outputs whether an even or odd number of the inputs are set to 1. Then, suppose our input is $x = (0, \ldots, 0)$ again. The minimum certificate size for the zero vector is $n$, since there are many inputs $y$ that have $n-1$ bits in common with the zero vector but where $f(x) \neq f(y)$ (e.g.~$y = (1, 0, \ldots, 0)$). So, the overall certificate complexity of $f$ is $n$. This also makes sense intuitively, since one must look at all input bits to evaluate $f$ accurately.
\end{example}
These two examples present somewhat extreme, or some may say trivial, evaluations of certificate complexities. Their purpose is merely to acquaint the reader with the idea of certificate complexity. What follows is a less trivial original proof about the certificate complexity of all linear codes.

\begin{theorem}(Swanberg) The certificate complexity for the nearest neighbor decoding algorithm $f$ on a linear code  $\mathscr{C} = [n,k]$ with minimum distance $d$ is 
\begin{equation*}
 C(f) = n - 2\bigg\lfloor \frac{1}{2} (d-1) \bigg\rfloor.
 \end{equation*}
\end{theorem}

Before ``proving'' this theorem, we must state a conjecture.

\begin{conjecture}\label{hope_its_true}
For any linear code $\mathscr{C} = [n,k]$ with minimum distance $d$, 
\begin{equation*}
\bigg\lfloor \frac{1}{2}(d-1)\bigg\rfloor < \frac{n}{2}.
\end{equation*}
\end{conjecture}

\begin{proof}
A code $\mathscr{C}= [n,k]$ with distance $d$ can correct $\lfloor\frac{1}{2}(d-1)\rfloor$ errors. Thus, there exist two codewords $c_1 = e_1 + c$ and $c_2 = e_2 +c$ where the nearest neighbor decoding function gives $f(c_1) = f(c) = f(c_2)$ provided that $dist(c, c_1), dist(c, c_2) \leq \lfloor\frac{1}{2}(d-1)\rfloor$. Let $dist(c, c_1) = dist(c, c_2) = \lfloor\frac{1}{2}(d-1)\rfloor$.

By the triangle inequality and the fact that inner product distance is in fact a metric on $\Z_2^n$, we have 
\begin{align*}
dist(c_1, c_2) 
& \leq dist(c, c_1) + dist(c, c_2) \\
& = 2 \bigg\lfloor\frac{1}{2}(d-1)\bigg\rfloor.
\end{align*}
So, we have an upper bound on the maximum distance between $c_1$ and $c_2$. Now, I will show that this upper bound is achievable. By Conjecture \ref{hope_its_true}, the number of errors we can correct is less than $n/2$. Thus, we have that $wt(e_1), wt(e_2) < n/2$. Given this fact, we may choose $e_1$ and $e_2$ such that $dist(e_1, e_2) = 2 wt(e_1) = dist(c, c_1) + dist(c, c_2)$ by choosing $e_1$ and $e_2$ that have at least
\end{proof}


\section{Block Sensitivity}
Block sensitivity is a lower bound on all query complexity measures (?)

\begin{definition}[Block sensitivity] For an input $x\in \{0,1\}^n$ and a subset of variables $S \subseteq [n]$, $x^{(S)}$ is the input obtained from $x$ by changing all $x_i, i \in S$ to opposite values. The block sensitivity $bs(f)$ is the maximum $k$ for which there is an input $x \in \{0,1\}^n$ and pairwise disjoint subsets $S_1, \ldots, S_k \subseteq [n]$ with $f(x) \neq f(x^{(S_i)})$ for all $1 \leq i \leq k$
\end{definition}

What does this mean in the context of error-correcting codes? We want to know the block sensitivity of $f$, our decoding algorithm. For linear decoding, we have $f: \{0,1\}^n \rightarrow \{0,1\}^k$ where the messages are of length $k$ and the codewords have $n-k$ check bits. 

\begin{theorem}
For linear codes $\mathscr{C} = [n,k]$ with minimum distance $d$, the nearest neighbor decoding function $f: \{0,1\}^n \rightarrow \{0,1\}^k$ has a block sensitivity $$
bs(f) \leq \frac{n}{\lfloor \frac{1}{2}(d-1)  \rfloor +1}$$
\end{theorem}

\begin{proof}
Our linear code has minimum distance $d$. By (theorem 2 in MacWilliams and Sloane), the code can correct $\lfloor \frac{1}{2}(d-1) \rfloor$ errors. Thus, $f(x) = f(x^{(S)})$ where $\lvert S \rvert \leq \lfloor \frac{1}{2}(d-1) \rfloor$. Correct decoding is not guaranteed when $\lvert S \rvert > \lfloor \frac{1}{2}(d-1) \rfloor$. There are at most $n/(\lfloor \frac{1}{2}(d-1) \rfloor +1)$ pairwise disjoint sets of size $\lfloor \frac{1}{2}(d-1) \rfloor+ 1$ in $\{1, \ldots, n\}$, which could all potentially lead to decoding errors. Thus, $$bs(f) \leq \frac{n}{\lfloor \frac{1}{2}(d-1)  \rfloor +1}.$$\\
\end{proof}


\chapter{The Binary Simplex Code}
One particularly interesting linear code is the simplex code. This is a special case of the Reed-Muller codes we will discuss later. The encoding and decoding algorithms are quite simple, so this is our starting point for further analyzing some of the error-correcting properties of the quantum decoding algorithm.
\section{Encoding}
To compute the $i$-th bit of the codeword for a message $m$, take $c_i =i_2 \cdot m$ mod 2, where $i_2$ is the binary representation of the number $i$. Thus, a codeword $c$ for a message $m$ is defined: 
\begin{equation*}
c_1 \ldots c_{2^n} = 1_2 \cdot m ||2_2 \cdot m || \ldots ||(2^n)_2 \cdot m
\end{equation*}
computing all of the inner products modulo 2.
\subsection{Example}
Suppose $n=3$. Then, the encoding of the message $m = 010$ is:
\begin{align*}
&c_1 = (0,0,0) \cdot (0,1,0) = 0\\
&c_2 = (0,0,1) \cdot (0,1,0) = 0\\
&c_3 = (0,1,0) \cdot (0,1,0) = 1\\
&c_4 = (0,1,1) \cdot (0,1,0) = 1\\
&c_5 = (1,0,0) \cdot (0,1,0) = 0\\
&c_6 = (1,0,1) \cdot (0,1,0) = 0\\
&c_7 = (1,1,0) \cdot (0,1,0) = 1\\
&c_8 = (1,1,1) \cdot (0,1,0) = 1\\
\end{align*}
Thus, $c = 00110011$ for that message. 

Now, the computer scientists reading this will notice that the length of the encoding of a message is exponential in the length of the message, i.e.~a message of length $n$ has a codeword of length $2^n$. This is far from an ideal representation of information. Indeed, the codeword for a 256-bit message would be $2^{253}$ bytes long, which is so large that our units of storage do not go that high. For some perspective, the largest unit of storage I could find is the yottabyte, or $2^{80}$ bytes, or one trillion terabytes. The length of the codeword, $2^{253}$ bytes, is far more information than could possibly be stored in the observable universe even if each bit were represented by a single atom. Clearly, this scheme is not ideal for long messages, but in the spirit of conducting unfettered computer science theory research, we will ignore this fact. 

\subsection{Generating Matrix}
Another way of encoding messages with the binary simplex code is to use a generating matrix. Recall the Hadamard matrix.
\begin{equation*}
H = \hadamard
\end{equation*}
Now, we will define a similar matrix transformation that is, in fact, the generating matrix for the binary simplex code.
\begin{equation} \label{2_simplex}
S = 
\frac{1}{\sqrt{2}}
\begin{bmatrix}
0 & 0 \\
0 & 1
\end{bmatrix}
\end{equation}
Note that the terms of $S$ are equivalent to the terms of $H$ with the following bijection: $h_{ij} = (-1)^{s_{ij}}$ (where $h_{ij}$ is the term of $H$ in row $i$ column $j$). This property is essential to the quantum decoding algorithm.

Now, to create the generating matrix for a simplex code of length $n = 2^N$ for some $N \in \Z_{>0}$, simply take $N$ tensor products of matrix \ref{2_simplex}, or $S^{\otimes N}$.
\begin{remark}
For the geometers among you, you will be pleased to know that the codewords of the binary simplex code do, in fact, lie on the vertices of an $n$-dimensional regular simplex as the name would suggest.
\end{remark}
\section{Quantum Decoding Algorithm}
The quantum decoding algorithm for the simplex code is very pleasing. The Bernstein-Vazirani algorithm is the essence of the algorithm. In this version of the algorithm, we will assume that no errors occurred during the transmission of the codeword; later, we will analyze the performance of the algorithm with errors.
\subsection{Reliable Quantum Oracle}
The quantum oracle we will use is specific to each message $m$. That is, the oracle $\mathcal{O}_m$ encodes the message $m$ into the corresponding codeword $c$. This oracle is ``reliable'' in the sense that it always gives $c$ with no errors, i.e~the received codeword is the actual codeword. 

Precisely, $\mathcal{O}_m$ is a bijection on $(\C^2)^n \otimes \C^2$ and is defined as follows
\begin{equation*}
\mathcal{O}_m: \ket{i, r} \longmapsto \ket{i, r \oplus (i\cdot m)}.
\end{equation*} 
This returns precisely the $i$-th bit of the codeword $c$ that corresponds to message $m$. Now, we may put $\Hminus$ in the response register and use the phase-kickback trick to store $c_i$ in the phase of the state:
\begin{equation*}
\mathcal{O}_m: \ket{i, -} \longmapsto (-1)^{i \cdot m}\ket{i,-}.
\end{equation*}
In the algorithm, we will omit the response register since it remains $\Hminus$ throughout the computation.
\subsection{Algorithm}

Now that our query model is defined, we give an overview of the algorithm. The $n$-qubit query register will be initialized with $\ket{0}^{\otimes n}$ and an $n$-qubit Hadamard transform will put the query register into an equal superposition. Simultaneously, the $n$-qubit response register will be set to $\Hminus^{\otimes n}$ so we can use the phase-kickback trick. The algorithm does one quantum query to the oracle $\mathcal{O}_m$ to read in the corresponding codeword's bits into the phase of each state in the query register. Then, an $n$-qubit Hadamard is applied to the query register. Upon measurement, we obtain the original message with probability 1.

Now, for the algorithm. Let $\ket{\psi_m}$ denote the query register.

\begin{align}
\ket{\psi_m}
& =  \frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} \ket{i} \\
& \xrightarrow{\mathcal{O}_m} \frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} (-1)^{i \cdot m} \ket{i} \\
& \xrightarrow{H^{\otimes n}} \frac{1}{2^n} \sum_{i \in \Z_2^n} (-1)^{i \cdot m} \sum_{j \in \Z_2^n} (-1)^{i \cdot j} \ket{j} \\
& = \frac{1}{2^n} \sum_{j \in \Z_2^n} \left( \sum_{i \in \Z_2^n} (-1)^{i\cdot (m +j)} \right) \ket{j} \label{confuse}\\
& = \ket{m} \label{result}
\end{align}
Most of these transformations follow directly from definitions. The equality from line \ref{confuse} to \ref{result} is less apparent, but this equality follows the same logic as the analogous part of the Bernstein-Vazirani algorithm. Note that on line \ref{confuse} if $j = m$, the inner sum becomes 
\begin{align*}
\sum_{i \in \Z_2^n} (-1)^{i \cdot (2m)} 
&= \sum_{i \in \Z_2^n} 1 \\
& = {2^n}.
\end{align*}
Thus, the full state on line \ref{confuse} is 
\begin{align*}
\frac{1}{2^n} \sum_{j \in \Z_2^n} \left( \sum_{i \in \Z_2^n} (-1)^{i\cdot (m +j)} \right) \ket{j}
& = \frac{1}{2^n} \left (2^n \ket{m}+  \sum\limits_{\substack{j \in \Z_2^n \\ j \neq m}}
 (-1)^{i\cdot (m +j)}\ket{j} \right) \\
 & = \frac{1}{2^n} \left (2^n \ket{m}+  \sum\limits_{\substack{j \in \Z_2^n \\ j \neq m}}
0\ket{j} \right) \\
& = \ket{m}.
\end{align*}
So, measuring in the computational basis gives us $c$ with probability 1. This algorithm yields the correct message $m$ with probability 1 using only one quantum query.
\section{Rosbustness to Errors}
Now we will consider a different scenario. Suppose that the oracle $\mathcal{O}_m$ is a little faulty. The oracle occasionally makes mistakes in transmitting the bits of the codeword $c$ that corresponds to $m$. Instead, it transmits a vector $r = c+ e$ which may not even be a codeword in the simplex code. Since the simplex code is error-correcting, one would hope that the algorithm could still recover the correct $m$ with a high probability. In this section, we formalize the algorithm's robustness to errors, that is, the probability of producing the correct output even with a noisy oracle. 
\begin{remark}
Since this algorithm only requires one query to the oracle, we will not distinguish between a stateless and stateful noisy quantum oracle; however, that is to come.
\end{remark}

\subsection{Noisy Quantum Oracle}
The reliable oracle is defined above. Now, we will define the unreliable, or noisy, quantum oracle. Again, it is a bijection on $(\C^2)^n \otimes \C^2$. Now, define a set of indices $E \subseteq [N]$, where $N = 2^n$, on which the oracle answers incorrectly. Then, the action of the noisy oracle $\widehat{\mathcal{O}}_m$ is defined as follows:

\begin{equation*}
\widehat{\mathcal{O}}_m: \ket{i, -} \longmapsto (-1)^{m \cdot i} - 2\delta_E (-1)^{m \cdot i}
\end{equation*}
where $\delta_E$ is simply an indicator function on the set $E$. In other words, if $i \notin E$ then the oracle $\widehat{\mathcal{O}}_m$ returns the $i$-th bit of the codeword associated with $m$ properly in the form $(-1)^{m \cdot i}$. On the other hand, if $i \in E$, the oracle returns $-(-1)^{m\cdot i}$, which is the $i$-th bit of the codeword flipped.
\subsection{Algorithm with Noisy Oracle}
\begin{theorem} \label{cool_thm}(Swanberg and Pommersheim) The Bernstein-Vazirani quantum decoding algorithm for the binary simplex code $\mathcal{S}_n$ has probability of success
\begin{equation*}
P_{succ} = \left(1 - \frac{k}{2^{n-1}}\right)^2
\end{equation*}
where $k= \lvert E \rvert$, the number of errors introduced by the noisy quantum oracle and $n$ is the length of the message.
\end{theorem}

\begin{proof}
We will begin by performing the algorithm with our noisy oracle $\widehat{\mathcal{O}}_m$.
\begin{align*}
\ket{\psi_m} 
& = \frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} \ket{i} \\
& \xrightarrow{\widehat{\mathcal{O}}_m} \frac{1}{\sqrt{2^n}} \sum_{i \in \Z_2^n} (-1)^{i \cdot m}\ket{i} -2\left( \sum_{p \in E} (-1)^{p \cdot m} \ket{p}\right) \\
& \xrightarrow{H^{\otimes n}} \ket{m} - \frac{1}{2^{n-1}} \left( \sum_{p \in E}(-1)^{p \cdot m} \sum_{j \in Z_2^n} (-1)^{j \cdot p} \ket{j}\right)\\
& = \ket{m} - \frac{1}{2^{n-1}} \left( \sum_{p \in E}\sum_{j \in Z_2^n} (-1)^{p \cdot(j+m)} \ket{j} \right)
\end{align*}
Now, we want to know $P_{succ}$, which is the probability of measuring $m$ in the computational basis. We have that for any quantum state, the probability of measuring a given result is the square of the coefficient on that state. So,
\begin{align*}
P_{succ} 
& = \left((1 - \frac{1}{2^{n-1}} \left( \sum_{p \in E} (-1)^{2(p \cdot m)} \right)\right)^2 \\
& = \left(1 - \frac{1}{2^{n-1}} \left( \sum_{p \in E} 1 \right) \right)^2 \\
& = \left( 1 - \frac{\lvert E \rvert}{2^{n-1}}\right)^2\\
& = \left( 1 - \frac{k}{2^{n-1}}\right)^2.
\end{align*}
\end{proof}
Interestingly, this formula shows that the quantum binary simplex decoding algorithm has $P_{succ} = 1$ if $k = 2^n$. In other words, if the quantum oracle is so noisy that it flips every single bit of the codeword, the decoding algorithm is still able to compute the correct message. This puzzled both the student and adviser on this thesis until we realized that flipping all of the bits leads to a \textit{global phase shift} in the quantum state post-query, which as was discussed in Chapter 1, is essentially an equivalent state. So, in this case the adage that \textit{it's so wrong it's right} holds true.

Additionally some features of the formula in theorem \ref{cool_thm}, 
\begin{corollary} \label{corollary}(Swanberg and Pommersheim) The probability of success for the quantum decoding algorithm for the binary simplex code $\mathcal{S}_n$:
\begin{enumerate}
\item Does not depend on the message $m$, and
\item Does not depend on $E$, only $\lvert E \rvert$.
\end{enumerate} 
\end{corollary}
\begin{proof}
Corollary \ref{corollary} follows from the proof of Theorem \ref{cool_thm}.
\end{proof}



\chapter{Reed-Muller Codes}
Reed-Muller codes are error-correcting codes with fascinating mathematical properties.  They are used in deep space communications to reliably transmit messages across space and time. Reed-Muller codes may be one of the oldest and most well-studied codes.

While the binary simplex code (singular) only had one parameter, namely the block length, the Reed-Muller codes (plural) have two parameters. In fact, Reed-Muller codes are a generalization of the binary simplex code (also called the Walsh-Hadamard code) and are closely related to other important codes. They belong to the family of \textit{geometrical codes}, as evidenced by their close relationship with projective and Euclidean geometries (for which there is an appendix), and fortunately they are easier to decode than other geometrical codes. While they are used in some instances, such as for deep space communications, they have a smaller minimum distance than BCH (Bose-Chaudhuri-Hocquenghem) codes of the same block length.


\section{Boolean Functions}
One interesting way to conceive of Reed-Muller (RM) codes is as polynomials of Boolean functions. Let the block length $N = 2^n$ for some $n\in \Z^+$. The input to a RM Boolean function will be $n$ variables taking binary values $v_1, \ldots, v_n \in \Z_2$. More precisely,

\begin{definition}(Boolean function) Any function $f: \Z_2^n \rightarrow \Z_2$ which takes $n$ binary input values $v_1, \ldots, v_n$ and outputs 0 or 1 is a Boolean function.
\end{definition}

A Boolean function $f$ can be defined in terms of its truth table, that is, a simple enumeration of what $f$ evaluates to on all $2^n$ inputs.

\begin{example}
Suppose $N = 2^3$. Then, we may represent a Boolean function $f: \Z_2^3 \rightarrow \Z_2$ with its truth table. For example, we could define the following function.
\begin{center}
\begin{tabular}{ |c|c| } 
 \hline
$ v_1v_2v_3$ &$f(v_1, v_2, v_3)$  \\ 
\hline
000 & 0  \\ 
001 & 0 \\
010 & 0 \\
011 & 1\\
100 & 1\\
101 & 0\\
110 & 0\\
111 & 0 \\
 \hline
\end{tabular}
\end{center}
Now, a more compact way of representing the same function above is by simply writing $f = 00011000$. 
\end{example}
All of the normal Boolean operations apply to Boolean functions, with the following modifications: $f \oplus g = f+g, f \land g = fg, $ and $\lnot f = 1 + f$. In particular, Boolean functions can be written as a polynomial of some particular functions which we will denote $v_1, v_2, \ldots v_n$. This notational overlap is not accidental, for $v_i$ is a function which simply evaluates to whatever the $i$-th bit of the input is. In general, we can write $v_i = (0^{2^{i-1}} 1^{2^{i-1}})^{2^{n-i}}$ where we use the computer science notation for concatenation as multiplication.
\begin{example}
For $N=2^3$ again, then the following are the special functions $v_1, v_2, v_3$, which we will care about later,  written in the notation defined above :
\begin{align*}
& v_1 = 01010101 \\
& v_2 = 00110011\\
&v_3 = 00001111
\end{align*}
\end{example}
We can write any Boolean function $f: \Z_2^n \rightarrow \Z_2$ as a polynomial of the special functions $v_1, \ldots v_n$ together with the constant function 1 (which always returns 1). 
\section{Encoding}
Now, consider how one might connect these Boolean functions to error-correcting codes. As was mentioned, any Boolean function can be represented as a polynomial of the functions $1, v_1, \ldots, v_n$. That polynomial has a coefficient vector $(a_1, \ldots, a_n)$, which we will suggestively call the message, and the output of the polynomial $c_1c_2 \ldots c_N$ which represents the codeword. 

\begin{definition} (Reed-Muller code) The $r$th order Reed-Muller code $\mathscr{R}(r,m)$ of length $n = 2^m$ , where $0 \leq r \leq m$, is the set of all vectors $f$ where $f(v_1, \ldots, v_m)$ is a Boolean function which is a polynomial of degree at most $r$.
\end{definition}

In other words, the $r$th order Reed-Muller code of length $2^m$ is the set of all linear combinations of products of $v_i$'s up to size $r$, i.e.
\begin{equation} \label{reed_muller_poly}
\sum_{\substack{S \subseteq [m] \\ \lvert S \rvert \leq r}} a_S \cdot \prod_{i \in S} v_i,  \quad a_S \in \Z_2.
\end{equation}

Hopefully a lengthy example will clarify this definition.
\begin{example}
The second order RM code of length 8, $\mathscr{R}(2,3)$ consists of the 128 codewords
\begin{equation*} 
a_0\mathbf{1} + a_1v_1 + a_2v_2 + a_3v_3 + a_{12}v_1v_2 + a_{13}v_1v_3 + a_{23}v_2v_3, \quad a_i \in \Z_2. 
\end{equation*}
This code has order 2 because each term in the polynomial is a product of at most two Boolean functions. By varying the coefficients $a_i$ according to the bits of the message, we obtain a polynomial that corresponds to the codeword. 

For example, suppose we wanted to encode the message $m = 0110101$. We would obtain the following function as the codeword
\begin{equation*}
f_c = 0\mathbf{1} + 1v_1 + 1v_2 + 0v_3 + 1v_1v_2 + 0v_1v_3 + 1v_2v_3.
\end{equation*}
Now, performing addition and multiplication of the Boolean functions in the terms of $f_c$, we obtain: 
\begin{align*}
f_c 
& = v_1 + v_2 + v_1v_2 + v_2v_3 \\
& = 01010101 \oplus 00110011 \oplus 00010001 \oplus 00000011 \\
& = 01110100.
\end{align*}
So, for message $m = 0110101$ the corresponding codeword is $c=01110100$, which can also be thought of as the polynomial $f_c$ above.
\end{example}

The set $\{\Pi_{i \in S}v_i \mid S \subseteq [m], \lvert S \rvert \leq r\} \cup \{\mathbf{1}\}$ forms a basis for the code, and it has size
\begin{equation*}
\sum_{i=0}^{r} {m \choose i}
\end{equation*}

\begin{theorem} 
$\mathscr{R}(r,m)$ has minimum distance $2^{m-r}$.
\end{theorem}
\begin{proof}
This induction proof will be left as an exercise.\\
\end{proof}
\subsection{Generator Matrix}
For those who prefer to think of encoding functions in terms of generator matrices, the generator matrix for $\mathscr{R}(r,m)$ has as its rows the terms of the polynomial in formula \ref{reed_muller_poly}.

\begin{theorem} The generator matrix $G$ for $\mathscr{R}(r,m)$ has the following property
\begin{equation*}
G(r, m) = 
\begin{bmatrix}
G(r, m-1) & G(r, m-1) \\
0 & G(r-1,m-1)
\end{bmatrix} .
\end{equation*}
\end{theorem}

\begin{example} \label{gen_matrix_ex}
The generator matrix for $\mathscr{R}(4,4)$ is displayed in Table \ref{gen_matrix}.
\begin{table}
\begin{center}
\begin{tabular}{ r c }
 1 & 1111111111111111 \\ 
 \hline
$v_4$ & 0000000011111111 \\  
$v_3$ & 0000111100001111 \\
$v_2$ & 0011001100110011  \\
$v_1$ & 0101010101010101 \\
\hline
$v_3v_4$ & 0000000000001111  \\
$v_2v_4$ & 0000000000110011 \\
$v_1v_4$ & 0000000001010101  \\
$v_2v_3$ & 0000001100000011  \\
$v_1v_3$ & 0000010100000101  \\
$v_1v_2$ & 0001000100010001  \\
\hline
$v_2v_3v_4$ & 0000000000000011 \\
$v_1v_3v_4$ & 0000000000000101 \\
$v_1v_2v_4$ & 0000000000010001  \\
$v_1v_2v_3$ & 0000000100000001  \\
\hline
$v_1v_2v_3v_4$ & 0000000000000001 
\end{tabular}
\caption{Generator matrix for $\mathscr{R}(4,4)$. \label{gen_matrix}}
\end{center} 
\end{table}
So, by simple matrix multiplication with our 16-bit message, we will obtain the corresponding codeword. Note that the generator matrix for $\mathscr{R}(3,4)$ is simply rows 1-15 of the above matrix, and the generator matrix for $\mathscr{R}(2,4)$ is just rows 1-11, and so on. 

The rows of the above matrix form a basis for the code, meaning any codeword in $\mathscr{R}(4,4)$ can be expressed uniquely as a linear combination of the rows in the generator matrix. That unique linear combination is, in fact, the message that corresponds to the codeword. Now, we will look at the classical decoding algorithm.
\end{example}
\section{Classical Decoding Algorithm}
There are a few ways to approach the decoding algorithm for Reed-Muller codes. By far the most elegant is to view RM codes through the lens of finite geometries and utilize the framework that geometries provide. The reader is encouraged to read Appendix \ref{finite_geom_chapter} before proceeding with the geometric description of the RM decoding algorithm.

\subsection{Geometric Connections}
Recall that the Euclidean geometry of dimension $m$ over $GF(2)$, denoted $EG(m,2)$, contains $2^m$ points whose coordinates are all of the binary vectors of length $m$, $(v_1, \ldots, v_m)$. If the zero point is deleted, the projective geometry $PG(m-1, 2)$ is obtained. We can think of the codewords of $\mathscr{R}(r,m)$ in this context, namely as subsets of $EG(m,2)$. However, we must have a way to specify the subsets. This is achieved by looking at the codewords as \textit{incidence vectors} for these subsets.

\begin{definition}[Incidence vector] Let $S \subseteq X$ where $\lvert X \rvert = 2^m$ and $X$ is well-ordered. Then, $v \in \Z_2^m$ is an incidence vector for $S$ if
\begin{equation*}
v_i = 1 \iff x_i \in S,
\end{equation*}
where $x_i$ is the $i$-th element of the ordered set $X$. Incidence vectors are also called \textit{indicator vectors}.
\end{definition}

\begin{example}
$EG(3,2)$ consists of 8 points, call them $P_0, \ldots, P_7$, whose coordinates we may take to be the following column vectors:
\begin{center}
\begin{tabular}{ c | c c c c c c c c }
 & $P_0$ & $P_1$ & $P_2$ &$ P_3$ &$ P_4$ &$ P_5 $& $P_6$ & $P_7$ \\
 \hline
$\bar{v_1}$ & 1 & 0 & 1 & 0 & 1 & 0 & 1 & 0\\
$\bar{v_2}$ & 1 & 1 & 0 & 0 & 1 & 1 & 0 & 0\\
$\bar{v_3}$ & 1 & 1 & 1 & 1 & 0 & 0 & 0 & 0\\   
\end{tabular}
\end{center}

The subset $S = \{P_2, P_3, P_4, P_5\}$ has incidence vector $\chi(S) = 00111100$. This is a codeword of $\mathscr{R}(1,3)$. So, there is a one-to-one correspondence between codewords in $\mathscr{R}(1,3)$ and subsets of $EG(3,2)$. This property will prove useful for decoding.
\end{example}

For any value $m$, let us take the complements of the vectors $v_m, \ldots, v_1$.

\begin{center}
\begin{tabular}{ l | c c c c c c c c c }
& $P_0$ & $P_1$ & $P_2$ & $P_3$ & $\ldots$ &$P_{2^m-4}$ & $P_{2^m-3}$ & $P_{2^m-2}$ & $P_{2^m-1}$\\
\hline
$\bar{v}_m$ & 1 & 1 & 1 & 1 &$\ldots$ & 0 & 0 & 0 & 0\\
$\bar{v}_{m-1}$ &1 & 1 & 1 & 1 &$\ldots$ & 0 & 0 & 0 & 0\\
$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\ddots$ &$\vdots$ &$\vdots$ &$\vdots$ &$\vdots$ \\
$\bar{v}_2$ & 1 & 1 & 0 & 0 & $\ldots$ & 1 & 1 & 0 & 0\\   
$\bar{v}_1$ & 1 & 0 & 1 & 0 &$\ldots$ & 1 & 0 & 1 & 0\\   
\end{tabular}
\end{center}
The columns of this matrix are the points in $EG(m,2)$. Again, there is a one-to-one correspondence between the subsets of $EG(m,2)$ and binary vectors of length $2^m$. In particular, any vector $x \in \Z_2^m$ is an incidence vector for a subset of $EG(m,2)$. The vectors $v_i$ are themselves the characteristic vectors of hyperplanes which pass through the origin (subspaces of dimension $m-1$), the $v_iv_j$ describe subspaces of dimension $m-2$, and so on.

\subsection{Decoding Algorithm}
As with the other error-correcting codes we have studied thus far, RM decoding will rely on parity checks and \textit{majority logic decoding}, which is a slightly different term for \textit{maximum likelihood decoding}, or \textit{nearest neighbor decoding}. Finite geometries will inform the algorithm as to exactly which parity checks to compute.

The decoding algorithm is a little involved, so we will describe the general decoding algorithm followed by an example of encoding and decoding.
We will work with $\mathscr{R}(2,4)$ to illustrate the algorithm notation rather than using general notation (recall that rows 1-11 of Table \ref{gen_matrix} contains the generator matrix for this code).

Denote the eleven bits of the message as $m = m_0m_4m_3m_2m_1m_{34}m_{24}m_{14}m_{23}m_{13}m_{12}$. The notation here is taken to match the rows of Table \ref{gen_matrix}. When we apply $m \cdot G$ where $G$ is the generator matrix for $\mathscr{R}(2,4)$, this message gets encoded into the following codeword.

\begin{align*}
m \cdot G 
&= m_01 + m_4v_4 + \ldots + m_1v_1 + \ldots + m_{12}v_1v_2 \\
& \stackrel{\mathclap{call}}{=} c_0c_1c_2\ldots c_{15} \\
& = c.
\end{align*}
Now, the challenge is to decode $\hat{c}$, some noisy version of a codeword (note that $\hat{c}$ may or may not be a codeword itself). In general, to decode a noisy codeword from $\mathscr{R}(r,m)$, we construct a series of parity checks. First, we will find $m_\sigma$ where $\sigma = \sigma_1 \ldots \sigma_r$, say. The corresponding row of the generator matrix $v_{\sigma_1} \ldots v_{\sigma_r}$ is the incidence vector of an $(m-r)$-dimensional subspace $S$ of $EG(m,2)$. To find $S$, use the row of the generator matrix as an indicator vector on the matrix of complements of the Reed-Muller monomial basis vectors. Now, we take $T$ to be the complementary subspace to $S$ with incidence vector $v_{\tau_1}\ldots v_{\tau_{m-r}}$ where $\{\tau_1, \ldots \tau_{m-r}\} = [m]\setminus \{\sigma_1, \ldots, \sigma_r\}$. Then $T$ meets $S$ at a single point, namely the origin. Now, let $U_1, \ldots, U_{2^{m-r}}$ be the \textit{translates} of $T$ in $EG(m,2)$, including $T$ itself. Each $U_i$ meets $S$ in exactly one point.

\begin{theorem} \label{error_correct_thm}
If there are no errors, $m_\sigma$ is given by
\begin{equation*}
m_\sigma = \sum_{P\in U_i} c_{P}, \quad i = 1, \ldots, 2^{m-r}
\end{equation*}
\end{theorem}
Note that there are $2^{m-r}$ equations that should all yield the same $m_{\sigma}$, and if not, one can apply majority logic. This theorem implies that if no more than $\lfloor\frac{1}{2}(2^{m-r} -1)\rfloor$ errors occur, majority logic decoding will recover each of the symbols $v_{\sigma}$ correctly, where $\sigma$ is a string of any $r$ symbols. 
	After going through this process for all $r$-length subscripts in $m$, we subtract the corresponding $a_{ij}v_iv_j$'s from $c$ to obtain a new codeword $c' = c_0'c_1' \ldots c_{15}'$ and repeat the process for all $r-1$-length subscripts. We continue this process until all we have left is $c_01 + \text{error}$ and $c_0 = 0$ or 1 according to the number of 1's in the remainder of the codeword.
	
\subsection{Geometric Decoding Example}
Again, we will work with $\mathscr{R}(2,4)$ since rows 1-11 of Table \ref{gen_matrix} are the generator matrix for the code. First, let us encode some message $ m_0m_4m_3m_2m_1m_{34}m_{24}m_{14}m_{23}m_{13}m_{12} = 00001010100$, which is three zero's followed by the ascii value for `T' in binary (`T' for thesis). 

Now, we encode this message by matrix multiplication (mod 2) to get $c_0 \ldots c_{15} = 0101011001100101$. Now, suppose the channel flips a bit in the process of sending the codeword, and $\hat{c} = 0101011\underline{1}01100101$ is what the decoder receives (without the flipped bit underlined, obviously).

The first bit of the message that we want to decode is $m_{12}$, so applying the notation used in the general decoding, we have $\sigma = 12$. Now, we look at the row corresponding to $v_1v_2$, which is row 11 of Table \ref{gen_matrix}: 0001000100010001. This is our indicator vector for the complements of $v_1, \ldots,v_4$, which is below:

\begin{center}
\begin{tabular}{ c | c  }
$\bar{v_4}$ & 1111111100000000\\
$\bar{v_3}$ & 1111000011110000 \\ 
$\bar{v_2}$ & 1100110011001100\\
$\bar{v_1}$ & 1010101010101010 
\end{tabular}
\end{center}
So, from this table we get $S =\{1100, 1000, 0100, 0000\}$. Now, the incidence vector for $T$ is the row of Table \ref{gen_matrix} corresponding to $v_{34}$. Thus, the incidence vector for $T$ is 0000000001111. So, $T = \{0011,0010, 0001, 0000\}$. Indeed $S \cap T = \{0000\}$. 

Now, we take all translates of $T$ in $EG(4,2)$ (that is, we take each point in $S$ and translate $T$ by that value). The translates of $T$ are: 
\begin{align*}
& U_1 =  \{1100 + t \mid t \in T\} = \{1111, 1110, 1101, 1100\} \\
& U_2 = \{1000 + t \mid t \in T\} = \{1011, 1010, 1001, 1000\} \\
& U_3 = \{0100 + t \mid t \in T\} = \{0111, 0110, 01010, 0100\} \\
& U_4 =  \{0000 + t \mid t \in T\} = \{0011, 0010, 0001, 0000\} \\
\end{align*}
Now, we will use the bits of the received codeword $\hat{c}$ to compute $m_{12}$, the last bit of the message. By applying Theorem \ref{error_correct_thm}, we have that 
\begin{equation*}
m_{12}  = \sum_{P \in U_i} c_P, \quad i = 1,2,3,4 
\end{equation*}
This gives us the four following checksums, which should all be equal in the case of no errors.
\begin{align*}
 m_{12} 
& = c_{15} + c_{14} + c_{13} + c_{12}\\
& = c_{11} + c_{10} + c_{9} + c_{8}\\
& = c_{7} + c_{6} + c_{5} + c_{4} \\
& = c_{3} + c_2 + c_1 + c_0
\end{align*}
So, these equations give us that $m_{12} =0,0,1,0 $. These equations are \textit{not} equal because bit $c_7$ was flipped by the noisy channel. But, the decoder doesn't know which bit was flipped. Instead, it decides that $m_{12} = 0$ using majority logic. Indeed, this is correct! (Note how if two equations didn't match, the decoder would possibly make a decoding error).

Now, the decoder will continue using this process to decode $m_{13}, m_{23}, m_{14}, $ and $m_{34}$. Once it has figured out those five bits, it will subtract $m_{34}v_3v_4 + m_{14}v_1v_4 + m_{23}v_2v_3 + m_{13}v_1v_3$ from $\hat{c}$ and proceed with the algorithm to decode $m_0 \ldots m_4$. We will spare the reader these details, but that at least conveys the tedium involved in decoding. In the chapters that follow, we will explore a quantum way of decoding RM codes.

\chapter{Quantum Multivariate Polynomial Interpolation}
Before we dive into quantum decoding algorithms for Reed-Muller codes, we must understand the mechanisms behind such an algorithm. The contents of this chapter are largely taken from the recent work [CITE MULTIVARIATE AND OTHER INTERPOLATION PAPER]. In this chapter, we set up the general algorithm that we will later tailor to our Reed-Muller decoding problem. 

It is worth noting that the papers upon which this chapter is based ([CITE THEM]) is far more extensive than what will be covered here. In fact, their quantum polynomial interpolation algorithm is much more closely related to quantum cryptanalysis and a concept in cryptography called \textit{secret sharing}. Alas, this is not a cryptography thesis.

\section{The Problem Definition}
Let $f(x_1, \ldots, x_n)\in \F_q[x_1, \ldots, x_n]$, that is, $f$ is a multivariate polynomial over the finite field of size $q$, and let $f$ be of degree $d$. Suppose an algorithm $\mathcal{A}$ has oracle access to $f$, and $d$ is known. The polynomial interpolation problem that $\mathcal{A}$ must solve is to determine the coefficients of $f$ by querying the oracle. In other words, $f$ must learn the coefficient vector of $f$.

The learning theory question that researchers then ask is: \textit{how many queries must any algorithm $\mathcal{A}$ make to the oracle before it can determine the coefficients of $f$?} In this case, researchers often care about some ``worst case,'' also called a \textit{lower bound}, over all possible algorithms $\mathcal{A}$ that solve the polynomial interpolation problem. Or, they may wonder what the optimal algorithm is. 

Additionally, those interested in quantum complexity theory wonder how the answer to the above question changes depending on whether $\mathcal{A}$ is a quantum or classical algorithm. The best classical algorithm for multivariate polynomial interpolation requires ${n+d \choose d}$ queries, using a well-chosen system of linear equations.

\section{Quantum Query Model}
Much like phase kickback, queries to the oracle will be encoded in the phase of the response register. However, the algorithm will perform queries in the Fourier basis using a technique that is a generalization of the phase kickback trick. Moreover, the algorithm will make $k$ quantum queries in parallel to extract all of this information at once. These $k$ queries are chosen \textit{non-adaptively}, that is, the algorithm prepares the questions all at once.

\subsection{Quantum Fourier Transform}
Defining the quantum Fourier transform requires some background. First, it is a well-known fact that the order of any finite field can be written as a prime to some power. In other words, the finite field $\F_q$ has order $q = p^r$ where $p$ is prime. Now, define the trace function $T: \F_q \rightarrow \F_q$ by $z \mapsto z + z^p + z^{p^2} + \ldots + z^{p^{r-1}}$. Next, we will define the exponential function $E: \F_q \rightarrow \C$ by $E(z) = e^{i2\pi T(z)/p}$. Now we are ready to define the quantum Fourier transform (QFT).

\begin{definition}[QFT] The quantum Fourier transform over $\F_q$ is a unitary transformation acting as 
\begin{equation*}
\ket{x} \longmapsto \frac{1}{\sqrt{q}} \sum_{y \in \F_q} E(xy) \ket{y}.
\end{equation*}
The $k$-dimensional quantum Fourier transform is given by
\begin{equation*}
\ket{x} \longmapsto \frac{1}{q^{k/2}} \sum_{y\in \F_q^k} E(x \cdot y) \ket{y}.
\end{equation*}
\end{definition}
This will be key to the phase query algorithm.
\subsection{Phase Query}
The authors of [CITE] describe what they call a \textit{phase query}. Essentially, the algorithm involves doing an inverse quantum Fourier transform (which we will denote $QFT^{-1}$), performing a query to oracle $\mathcal{O}$, and then doing a quantum Fourier transform (QFT) as follows
\begin{align}
\ket{x,y} 
& \xrightarrow{QFT^{-1}} \frac{1}{\sqrt{q}} \sum_{z\in \F_q} E(-yz) \ket{x,z} \\
& \xrightarrow{\mathcal{O}} \frac{1}{\sqrt{q}} \sum_{z \in \F_q} E(-yz) \ket{x, z + f(x)} \\
& \xrightarrow{QFT} \frac{1}{q} \sum_{z,w \in \F_q} E(-yz + (z + f(x))w) \ket{x,w} \\
& = E(yf(x)) \ket{x,y}.
\end{align}
So, overall the phase query has the following action $\ket{x,y} \mapsto E(yf(x)) \ket{x,y}$. Now, instead of just querying $f$ on one input $x \in \F_q$, we want to query it on $k$ inputs. So, we choose a subset of $\F_q^k$ on which to perform $k$ parallel queries, each in a separate register. For $x,y \in \F_q^k$, the $k$ parallel phase queries has the following action
\begin{equation*}
\ket{x,y} \longmapsto E\left(\sum_{i=1}^{k} y_i f(x_i) \right) \ket{x,y}
\end{equation*}
where $x_i$ and $y_i$ are the $i$-th bits of $x$ and $y$, respectively.
\section{More Preliminaries}
Next, we define a mapping $Z: \F_q^{nk} \times \F_q^{k} \rightarrow \F_q^{J}$ by 
\begin{equation*}
Z(x,y)_j = \sum_{i = 1}^k y_ix_i^j \quad \text{for }j \in \mathbb{J}
\end{equation*}
so that
\begin{equation*}
Z(x,y) \cdot c = \sum_{i=1}^k y_i f(x_i)
\end{equation*}
where $\mathbb{J}$ is the set of allowed exponents, and $J \coloneqq \lvert \mathbb{J} \rvert$.

Now, we restrict the codomain of $Z$ so that it forms a bijection. Let $R_k \coloneqq Z(\F_q^{nk}, \F_q^k)$ be the image of $Z$. Now, for each $z \in R_k$, choose a unique $(x,y) \in \F_q^{nk} \times \F_q^{k}$ such that $Z(x,y) = z$. Call $T_k$ this set of unique representatives. Then, $Z: T_k \rightarrow R_k$ is a bijection.

\section{The Algorithm}
The algorithm has three steps. It starts in a superposition over $T_k$. First, it performs $k$ phase queries (denoted $k$-$\mathcal{PQ}$), then it simply computes $Z$ in place. Finally, we measure in the Fourier basis to get an approximation of the coefficients. More precisely,

\begin{align*}
\frac{1}{\sqrt{\lvert T_k \rvert}} \sum_{(x,y)\in T_k} \ket{x,y} 
& \xmapsto{k\text{-}\mathcal{PQ}} \frac{1}{\sqrt{\lvert T_k \rvert}} \sum_{(x,y)\in T_k} E(Z(x,y) \cdot c) \ket{x,y} \\
& \xmapsto{Z} \frac{1}{\sqrt{\lvert R_k \rvert}} \sum_{z\in R_k} E(z \cdot c) \ket{z}. 
\end{align*}
Measuring in the basis of Fourier states defined as follows
\begin{equation*}
\ket{\hat{c}} \coloneqq \frac{1}{\sqrt{q^J}} \sum_{z \in \F_q^J} E(z \cdot c) \ket{z},
\end{equation*}
where $c$ are the computational basis states, results in the correct vector of coefficients with probability $\lvert R_k \rvert / q^J$.


\chapter{Quantum Decoding Algorithm for Reed-Muller Codes}

In this chapter, we explore the possibility of using the quantum polynomial interpolation algorithm to decode Reed-Muller codes. The decoder will have oracle access to the received codeword $f_c$ and, through a well-chosen series of queries and computations, will need to output the correct message $m$. The task of this chapter is to yet again reframe the Reed-Muller decoding algorithm. This time, instead of thinking about decoding in terms of finite geometries, we will explain it in terms of polynomial interpolation. Furthermore, the algorithm described in the previous chapter will be tailored to the task of decoding.
 
\section{Polynomial : Coefficient Vector :: Codeword : Message}
As was previously discussed, we can view codewords in $\mathscr{R}(r,m)$ as multivariate polynomials of degree at most $r$ with coefficients in the field $\F_2$. The polynomials have the following form:
\begin{equation*}
f_c = \sum_{\substack{S \subseteq [m] \\ \lvert S \rvert \leq r}} a_S \cdot \prod_{i \in S} v_i,  \quad a_S \in \Z_2.
\end{equation*}
where $f_c$ is the $\mathscr{R}(r,m)$ codeword that corresponds to message $a$. So, by applying the polynomial interpolation algorithm to $f_c$, we should be able to extract $a$. 


\subsection{Adapting Polynomial Interpolation}
A lot of nice things happen when we adapt the polynomial interpolation algorithm to our use case. For one, the notation gets substantially less painful to look at. More importantly, many components of the algorithm become simpler.

First, the polynomials in $\mathscr{R}(r,m)$ are over the field $\F_2$. So, the order of the field is simply 2 and the trace function defined earlier $T:\F_2 \rightarrow \F_2$ defined by $z \mapsto z + z^p + z^{p^2} + \ldots + z^{p^{r-1}}$ is just the identity transformation. This simplifies the quantum Fourier transform. The function $E: \F_2 \rightarrow \C$ is now defined by $z \longmapsto e^{i\pi z}$.

 
\begin{theorem} The quantum Fourier transform over $\F_2$ is just the Hadamard transform.
\end{theorem}

\begin{proof}
The QFT acts on a state $\ket{x}$, $x\in \F_2$, as follows
\begin{equation*}
\ket{x}  \longmapsto \frac{1}{\sqrt{2}}\sum_{y\in \F_2} E(xy)\ket{y}
=  \frac{1}{\sqrt{2}}\sum_{y\in \F_2} e^{i\pi xy}\ket{y}.
\end{equation*}
But, since $x,y \in \F_2$, this is just the transformation
\begin{align*}
\ket{x}  \longmapsto 
&\frac{1}{\sqrt{2}}\bigg( E(x\cdot 0)\ket{0} + E(x\cdot 1)\ket{1} \bigg) \\
 = &  \frac{1}{\sqrt{2}}\bigg( e^{i\pi x\cdot 0}\ket{0} + e^{i\pi x\cdot 1}\ket{1} \bigg) \\
 = & \frac{1}{\sqrt{2}}\bigg( 1\ket{0} + e^{i\pi x}\ket{1} \bigg) \\
 = & \frac{1}{\sqrt{2}}\bigg( 1\ket{0} + (-1)^x\ket{1} \bigg). 
\end{align*}
This is the Hadamard transform. \\
\end{proof}

This simplifies the phase query. In the previous chapter we showed that the phase query has the following action $\ket{x,y} \mapsto E(yf(x))\ket{x,y}$. Given the simplified definition of QFT and $E$, we have that the phase query ($\mathcal{PQ}$) in $\F_2$ has the action
\begin{equation}
\ket{x,y} \xrightarrow{\mathcal{PQ}} (-1)^{yf(x)} \ket{x,y}.
\end{equation}

Now, the only thing left to define is the mapping $Z: T_k \rightarrow R_k$ where $T_k \subseteq \F_2^{nk} \times \F_2^k$ and $R_k \subseteq \F_2^J$. In this mapping, the coordinate $(x,y) \in T_k$ is comprised of the query and response registers, respectively. As we've learned by now, the response register is essentially just an appendage of the ``unitary'' requirement, so we can ignore that. Now, $x \in \F_2^{nk}$ is a set of $k$ queries on the $n$ input bits of $f_c$. Ideally, those queries would uniquely identify one message; however, this may not be the case for sufficiently small $k$. 

The authors of [CITE PAPER] leave the exact mechanism of $Z$ ambiguous. It is tempting to simply assert that $Z$ will pick the most likely message given a received codeword, or it will choose uniformly among the most likely candidates if more than one are equally likely. This, I think, defeats the point of algorithms research; indeed, although the input and corresponding output are well-defined, the process of computing the output is yet to be discovered. This is one of the defining differences between mathematics and computer science theory. We will revisit $Z$.

\subsection{The Algorithm}
This algorithm is simplified by the query model. Suppose that $\lvert T_k \rvert = 2^d$ for some $d$. Again, we begin in a superposition over $T_k$, make $k$ parallel phase queries, compute $Z$ in place, and then measure in the Fourier basis over $\F_2$. We have

\begin{align*}
\frac{1}{\sqrt{\lvert T_k \rvert}} \sum_{(x,y) \in T_k} \ket{x,y}
& \xrightarrow{k-\mathcal{PQ}} \frac{1}{\sqrt{\lvert T_k \rvert}} \sum_{(x,y) \in T_k}  (-1)^{Z(x,y) \cdot c} \ket{x,y} \\
& \xrightarrow{Z} \frac{1}{\sqrt{\lvert R_k \rvert}} \sum_{z \in R_k} (-1)^{z\cdot c} \ket{z}
\end{align*}
Now, measuring
\section{Stateful Noisy Oracle}
\section{Stateless Noisy Oracle}

\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{6}
	\setcounter{section}{0}
	
Here's a conclusion, demonstrating the use of all that manual incrementing and table of contents adding that has to happen if you use the starred form of the chapter command. The deal is, the chapter command in \LaTeX\ does a lot of things: it increments the chapter counter, it resets the section counter to zero, it puts the name of the chapter into the table of contents and the running headers, and probably some other stuff. 

So, if you remove all that stuff because you don't like it to say ``Chapter 4: Conclusion'', then you have to manually add all the things \LaTeX\ would normally do for you. Maybe someday we'll write a new chapter macro that doesn't add ``Chapter X'' to the beginning of every chapter title.

\section{More info}
And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there. 


%If you feel it necessary to include an appendix, it goes here.
    \appendix
    	\chapter{Asymptotic Complexity}
    	Computer scientists are very interested in how long their algorithm or function will take to run on given inputs. The crudest, most theoretical, runtime analysis looks at the \textit{asymptotic runtime} of an algorithm. In practice, the actual runtime can be affected significantly by any number of factors like the operating system and memory  latency, but for our purposes the 
\textit{asymptotic} runtime is the only metric we will care about (particularly because current existent quantum computers are rather rudimentary).

The basic idea behind asymptotic runtime analysis is that we consider how long our algorithm takes to compute as a function of the \textit{length} (number of bits) of the input, where the time interval is calculated as the number of basic operations required. Classically, the basic operations are generally considered to be the ones that the Arithmetic Logic Unit (ALU) in the CPU can do. These include: addition, subtraction, multiplication, division, boolean comparisons, and variable assignment. Quantumly, the basic operations are the same as above in addition to arbitrary unitary transformations and oracle queries (which take one time step). 

Now that we have the basic idea, time to get technical. We define notions of relative asymptotic growth rates of functions: $O, \Omega, o, \omega$, and $\Theta$. We will define these notions rigorously below, but for all practical purposes, the following distinctions are sufficient. Suppose we have two functions $f(n)$ and $g(n)$. Then, 
\begin{itemize}
\item We say $f(n) = O(g(n))$,\textit{``$f$ is big-oh of $g$,''} if $f(n) \leq g(n)$ as $n$ gets large;
\item $f(n) = o(g(n))$,\textit{``$f$ is little-oh of $g$,''} if $f(n) < g(n)$ (strictly) as $n$ gets large;
\item $f(n) = \Omega(g(n))$,\textit{``$f$ is big-omega of $g$,''} if $f(n) \geq g(n)$ as $n \rightarrow \infty$;
\item $f(n) = \omega(g(n))$,\textit{``$f$ is little-omega of $g$,''} if $f(n) > g(n)$ (strictly) as $n$ gets large.
\item Lastly, $f(n) = \Theta(g(n))$, \textit{``$f$ is theta of $g$,''} if $f(n) = g(n)$ (up to constant factors) as $n$ gets large.
\end{itemize}
The above imprecise definitions are rigorous enough for our discussions; however, I present the formal definitions below for the overly pedantic or curious reader. 
 
\begin{theorem}
A function f(n) is O(g(n)), denoted f(n) = O(g(n)), if there exist $c>0, n_0>0$ such that
$$f(n) \leq c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is o(g(n)), denoted f(n) = o(g(n)), if there exists $n_0>0$ such that for all $c>0$,
$$f(n) < c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\Omega(g(n))$, denoted $f(n) = \Omega(g(n))$, if there exist $c>0, n_0>0$ such that
$$f(n) \geq c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\omega(g(n))$, denoted $f(n) = \omega(g(n))$, if there exists $n_0>0$ such that for all $c>0$,
$$f(n) > c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\Theta(g(n))$, denoted $f(n) = \Theta(g(n))$, if f(n) = O(g(n)) and g(n) = O(f(n)).
\end{theorem}

Determining the exact values of $n_0$ and $c$ is not necessary. Instead, we will present a few heuristics:
\begin{enumerate}
\item Constant factors don't matter. 
\item Only the largest complexity class matters if we are adding terms.
\end{enumerate}
For example, $f(n)= 25n^3 + 170000n$ is $O(n^{17})$ because $n^3 \leq n^{17} $ as $n$ grows, and $f(n) = \Theta(n^3)$ because the $170000n$ doesn't matter as $n$ grows and we don't care about the constant factor of 25 either, and $f(n) = \omega(n^2)$, because $n^2 < n^3$ as $n$ goes to infinity.

	
\chapter{Tensors}
\chapter{Finite Geometries} \label{finite_geom_chapter}
Finite geometries are mathematical objects that codes map onto well, and they provide a new way of viewing codes. In this appendix, we will explore the properties of finite geometries that are vital to the classical Reed-Muller decoding algorithm with a focus on the projective geometry and affine or Euclidean geometry. This material is largely based on Appendix B in [REFERENCE MACWILLIAMS AND SLOANE]

\begin{definition}(Projective geometry) A finite projective geometry consists of a finite set $\Omega$ of \textit{points} $p, q, \ldots$ together with a collection of subsets $L, M, \ldots$ of $\Omega$ called \textit{lines}, which satisfies the following axioms (If $p \in L$ we say that $p$ lies on $L$ or $L$ passes through $p$.)
\begin{enumerate}
\item There is a unique line (denoted by $(pq)$) passing through any two distinct points $p$ and $q$.
\item Every line contains at least 3 points.
\item If distinct lines $L, M$ have a common point $p$, and if $q,  r\neq p$ are points of $L$ and $s, t\neq p$ are points of $M$, then the lines $(qt)$ and $(rs)$ also have a common point.
\item For any point $p$ there are at least two lines not containing $p$, and for any line $L$ there are at least two points not on $L$.
\end{enumerate}
A \textit{subspace} of the projective geometry is a subset $S$ of $\Omega$ such that
\begin{enumerate}[resume]
\item If $p,q$ are distinct points of $S$, then $S$ contains all of the points of $(pq)$
\end{enumerate}
A \textit{hyperplane} $H$ is a maximal proper subspace, so that $\Omega$ is the only subspace which properly contains $H$.
\end{definition}

\begin{definition}[Euclidean geometry] An affine or Euclidean geometry is obtained by deleting the points of a fixed hyperplane $H$ (called the hyperplane at infinity) from the subspaces of a projective geometry. The resulting sets are called the subspaces of the affine geometry.

A set $T$ of points in a projective or affine geometry is called \textit{independent} if, for every $x \in T, x$ does not belong to the smallest subspace which contains $T/\{x\}$. The \textit{dimension} of a subspace $S$ is $r-1$, where $r$ is the size of the largest set of independent points in $S$. In particular, if $S = \Omega$ this defines the dimension of the projective geometry.
\end{definition}

We denote Euclidean and projective geometries of dimension $m$ constructed from a finite field $GF(q)$ by $EG(m,q)$ and $PG(m,q)$, respectively. Now, we will discuss the projective and affine geometries that are obtained from finite fields, as they are most pertinent to Reed-Muller codes.

Let $GF(q)$ be a finite field and suppose $m \geq 2$. We will take the points of $\Omega$ to be the nonzero $(m+1)$-tuples in $GF(q)^{m+1}$ such that
\begin{equation*}
(a_0, \ldots, a_m) \equiv (\lambda a_0, \ldots \lambda a_m) \quad \lambda, a_i \in GF(q), \quad \lambda \neq 0.
\end{equation*}
These are called \textit{homogenous coordinates} for the points. 

A \textit{hyperplane} of subspace of dimension $m-1$ in $PG(m,q)$ consists of those points $(a_0, \ldots , a_m)$ which satisfy a homogenous linear equation 
\begin{equation*}
\lambda_0 a_0 + \lambda_1 a_1 + \ldots + \lambda_m a_m = 0, \quad \lambda_i \in GF(q)
\end{equation*}

The affine or projective geometry $EG(m,q)$ is obtained from $PG(m,q)$ by deleting the points of any hyperplane $H$. A subspace of $EG(m,q)$ of dimension $r$ is called an \textit{$r$-flat}. 
%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA 
% subfolder, e.g. 
 \bibliographystyle{APA/apa-good}  % or
 \bibliography{thesis}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\end{document}
