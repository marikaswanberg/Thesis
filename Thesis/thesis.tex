 % This is the Reed College LaTeX thesis template. Most of the work 
% for the document class was done by Sam Noble (SN), as well as this
% template. Later comments etc. by Ben Salzberg (BTS). Additional
% restructuring and APA support by Jess Youngberg (JY).
% Your comments and suggestions are more than welcome; please email
% them to cus@reed.edu
%
% See http://web.reed.edu/cis/help/latex.html for help. There are a 
% great bunch of help pages there, with notes on
% getting started, bibtex, etc. Go there and read it if you're not
% already familiar with LaTeX.
%
% Any line that starts with a percent symbol is a comment. 
% They won't show up in the document, and are useful for notes 
% to yourself and explaining commands. 
% Commenting also removes a line from the document; 
% very handy for troubleshooting problems. -BTS

% As far as I know, this follows the requirements laid out in 
% the 2002-2003 Senior Handbook. Ask a librarian to check the 
% document before binding. -SN

%%
%% Preamble
%%
% \documentclass{<something>} must begin each LaTeX document
\documentclass[12pt,twoside]{reedthesis}


% Packages are extensions to the basic LaTeX functions. Whatever you
% want to typeset, there is probably a package out there for it.
% Chemistry (chemtex), screenplays, you name it.
% Check out CTAN to see: http://www.ctan.org/
%%
\usepackage{graphicx,latexsym} 
\usepackage{amssymb,amsthm,amsmath}
\usepackage{longtable,booktabs,setspace} 
\usepackage{chemarr} %% Useful for one reaction arrow, useless if you're not a chem major
\usepackage[hyphens]{url}
\usepackage{rotating}
\usepackage{natbib}
% Comment out the natbib line above and uncomment the following two lines to use the new 
% biblatex-chicago style, for Chicago A. Also make some changes at the end where the 
% bibliography is included. 
%\usepackage{biblatex-chicago}
%\bibliography{thesis}

% \usepackage{times} % other fonts are available like times, bookman, charter, palatino

\usepackage{amssymb}
\usepackage{xspace}
\usepackage{color}
\usepackage[total={6in,8in}]{geometry}
\usepackage{amsthm}

\usepackage{enumitem}
\usepackage{centernot}
\usepackage[utf8]{inputenc}
\usepackage[english]{babel}

\newtheorem{theorem}{Theorem}
\newtheorem{definition}{Definition}
\newcommand{\N}{\mathbb{N}}
\newcommand{\Z}{\mathbb{Z}}
\newcommand{\Q}{\mathbb{Q}}
\newcommand{\R}{\mathbb{R}}
\newcommand{\C}{\mathbb{C}}
\newcommand{\sn}{\mathfrak{S}}
\newcommand{\ve}{\varepsilon}
\newcommand{\ketz}{\ensuremath{\lvert 0\rangle}\xspace}
\newcommand{\keto}{\ensuremath{\lvert 1\rangle}\xspace}
\newcommand{\ket}[1]{\ensuremath{\lvert #1\rangle}\xspace}
\newcommand{\ketpsi}{\ensuremath{|\psi\rangle}\xspace}
\newcommand{\bra}[1]{\ensuremath{\langle #1\vert}\xspace}
\newcommand{\Hplus}{\ensuremath{\lvert + \rangle}\xspace}
\newcommand{\Hminus}{\ensuremath{\lvert- \rangle}\xspace}
\newcommand{\inner}[2]{\ensuremath{\langle #1 \mid #2 \rangle}\xspace}
\newlength{\minuslength}
\settowidth{\minuslength}{$-$}
\newcommand{\hadamard}{
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & \hspace{\minuslength}1\\
1 & -1 
\end{bmatrix}
}

\title{Stateful and Stateless Noisy Quantum Oracles}
\author{Marika Louise Swanberg}
% The month and year that you submit your FINAL draft TO THE LIBRARY (May or December)
\date{May 2019}
\division{Mathematics and Natural Sciences}
\advisor{James D. Pommersheim}
%If you have two advisors for some reason, you can use the following
\altadvisor{Adam Groce}
%%% Remember to use the correct department!
\department{Mathematics}
% if you're writing a thesis in an interdisciplinary major,
% uncomment the line below and change the text as appropriate.
% check the Senior Handbook if unsure.
%\thedivisionof{The Established Interdisciplinary Committee for}
% if you want the approval page to say "Approved for the Committee",
% uncomment the next line
%\approvedforthe{Committee}

\setlength{\parskip}{0pt}
%%
%% End Preamble
%%
%% The fun begins:
\begin{document}

  \maketitle
  \frontmatter % this stuff will be roman-numbered
  \pagestyle{empty} % this removes page numbers from the frontmatter

% Acknowledgements (Acceptable American spelling) are optional
% So are Acknowledgments (proper English spelling)
%    \chapter*{Acknowledgements}
%	I want to thank a few people.

% The preface is optional
% To remove it, comment it out or delete it.
   \chapter*{Preface}
Write some preface here.

A note to thesis students: If you are reading this thesis in order to expand upon my work or study a related topic, I recommend learning the basics of quantum computing from \textit{Quantum Computation and Quantum Information} by Nielsen and Chuang, and also looking at the definitive reference on code theory, \textit{The Theory of Error-Correcting Codes} by MacWilliams and Sloane. The introduction that I provide is not by any means exhaustive. Additionally, exercise patience and care with these topics, as they are not intuitive even to the seasoned mathematician or computer scientist. 

	
	

    \chapter*{List of Abbreviations}
		You can always change the way your abbreviations are formatted. Play around with it yourself, use tables, or come to CUS if you'd like to change the way it looks. You can also completely remove this chapter if you have no need for a list of abbreviations. Here is an example of what this could look like:

	\begin{table}[h]
	\centering % You could remove this to move table to the left
	\begin{tabular}{ll}
		\textbf{ABC}  	&  American Broadcasting Company \\
		\textbf{CBS}  	&  Columbia Broadcasting System\\
		\textbf{CDC}  	&  Center for Disease Control \\
		\textbf{CIA}  	&  Central Intelligence Agency\\
		\textbf{CLBR} 	&  Center for Life Beyond Reed\\
		\textbf{CUS}  	&  Computer User Services\\
		\textbf{FBI}  	&  Federal Bureau of Investigation\\
		\textbf{NBC}  	&  National Broadcasting Corporation\\
	\end{tabular}
	\end{table}
	

    \tableofcontents
% if you want a list of tables, optional
 %   \listoftables
% if you want a list of figures, also optional
%    \listoffigures

% The abstract is not required if you're writing a creative thesis (but aren't they all?)
% If your abstract is longer than a page, there may be a formatting issue.
 %   \chapter*{Abstract}
%	The preface pretty much says it all.
	
%	\chapter*{Dedication}
%	You can have a dedication here if you wish.

  \mainmatter % here the regular arabic numbering starts
  \pagestyle{fancyplain} % turns page numbering back on

%The \introduction command is provided as a convenience.
%if you want special chapter formatting, you'll probably want to avoid using it altogether

	
\chapter{Introduction to Quantum Computing}


In the following chapter, we will delve into the basics of quantum computing. Before proceeding, it is important to realize that quantum computing describes a  fundamentally different \textit{model of computation} than the computer systems currently on the market. Much like the first programmable computer was realized by Alan Turing far before anyone built his remarkable invention, we too study quantum computing at a time when only the most rudimentary quantum computers are available in practice.


Due to the complex nature of quantum mechanics, there are a plethora of misconceptions about quantum computing. One of the most widespread is that quantum computers gain computational speedups by \textit{trying all possible solutions at once}. This is simply not true; the state of the quantum bits may be unknown, but they are still in only one state. We will go into this more later. Perhaps my favorite that I have heard upon starting my thesis is that \textit{quantum computers are like classical computers but in trinary}. False, we cannot efficiently simulate a quantum computer on a classical one. Lastly, probably the most optimistic is that \textit{quantum computers are faster at all tasks compared to classical computers}. Quantum computers are faster than classical computers at some tasks, such as searching an unstructured list, but they have the same asymptotic runtime\footnote{We will discuss what this means later} as classical computers for other tasks. I outline these common misconceptions not to criticize them, but rather to encourage the reader to exert patience and care with the following section, as quantum computing is so fundamentally different from classical computing.

The quantum mechanical properties upon which quantum computers are based have been studied extensively by physicists; we will avoid discussing such details and instead take these properties for granted in order to focus on the information-theoretic behavior of this new model of computation. 

\section{Information Representation}
Classical computers, i.e.~the computers that we all know and love, run on \textit{bits} or 1's and 0's. This is the fundamental unit of information in classical computers. In quantum computers, information is built upon an analagous concept, the \textit{quantum bit} or \textit{qubit}. We will discuss the defining properties of qubits, some of which may seem very different from those of bits.

In classical computing, each bit can be in one of two states--1 or 0.  We denote \textit{quantum states} by \ketz and \keto, pronounced ``\textit{ket zero}'' and ``\textit{ket one},'' respectively. This follows traditional \textit{Dirac notation}\footnote{See reference on notation--not yet written}. Unlike classical bits, qubits can be in a \textit{superposition} between these two states. That is, a qubit can lie in one of the infinite states ``between'' \ketz and \keto. We describe a quantum state as follows: $\ketpsi = \alpha \ketz + \beta \keto$, where $\alpha$ and $\beta$ are complex numbers and $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1$. This describes a probability distribution over the quantum states \ketz and \keto with \textit{amplitudes} $\alpha$ and $\beta$. We can also think of \ketpsi as a vector in a two-dimensional complex vector space, say $\C^2$. The states \ketz and \keto form an orthonomal basis in this complex vector space and are called the \textit{computational basis states}.

So how can we know which state a bit or qubit is in? In classical computing, we can simply read the bit to determine whether it's a 0 or a 1. Qubits are a little trickier. We cannot read \ketpsi to determine its exact amplitudes, $\alpha$ and $\beta$. As soon as we measure \ketpsi, the superposition will collapse to either \ketz with probability $ \lvert \alpha \rvert ^2$ or \keto with probability $ \lvert \beta \rvert ^2$. In other words, \ketpsi is like a weighted coin that we can flip once to get either heads or tails, but we have no way to directly measure the bias in the coin. Unlike a coin, as soon as \ketpsi has been measured to reveal some quantum state, \ketpsi will permanently collapse to that state, i.e.~$\ketpsi = 1\ketz + 0 \keto$ or $\ketpsi = 0\ketz + 1\keto$. Every time we measure it thereafter, we will observe the same state that we measured the first time. In the coin analogy, it's like we have a weighted coin that we can flip once to reveal heads or tails, and every subsequent flip always yields the initial state that we flipped to.

You may be wondering why this is useful. It seems impossible to build a model of computation on a fundamental unit of information that is unknowable, immeasurable. The beauty of quantum computation lies in the \textit{manipulation} of these immeasurable qubits such that by the time we measure them at the end, the result will inform us of the state they started in. Simply measuring the qubits before doing any transformations is fundamentally the same as  running a random number generator on a classical computer and trying a random possible solution. Obviously, this is not very useful, so we must \textit{transform} the qubits to say anything intelligent about the end result that we measure.

\section{Quantum Logic Gates}

As we already glossed over, $\ketz = 1 \ketz + 0  \keto$ and $\keto = 0 \ketz + 1 \keto$. We sometimes represent these quantum states as column vectors of the amplitudes: 
\begin{align*}
\ketz = \begin{bmatrix}
1\\
0
\end{bmatrix}
\text{ and }
\keto = \begin{bmatrix}
0\\
1
\end{bmatrix}
\end{align*}

Thus, we can represent a single-qubit transformation by a 2-by-2 matrix where the first column is the image of \ketz and the second column is the image of \keto under the transformation. For example, the quantum NOT gate can be realized as a matrix:
\begin{align*}
X = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\end{align*}
This takes a state $\ketpsi = \alpha \ketz + \beta \keto$ and transforms it into $X \ketpsi  = \beta \ketz + \alpha \keto$. Alternatively, by matrix multiplication we see that
\begin{align*}
X \ketpsi = \begin{bmatrix}
0 & 1\\
1 & 0 
\end{bmatrix}
\begin{bmatrix}
\alpha \\
\beta
\end{bmatrix} 
= 
\begin{bmatrix}
\beta \\
\alpha
\end{bmatrix}.
\end{align*}
So, any transformation on a single qubit can be represented by a 2-by-2 matrix. What about the transformation that, when given \ketz or \keto, outputs an \textit{equal superposition} of \ketz and \keto ? This would essentially give us a fair coin. We might represent that as follows:
\begin{align*}
\widehat{H} = \begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
There are a few problems with this transformation. First, applying this transformation, we get $\widehat{H} \ketz = 1\ketz + 1\keto$, which means that $\lvert \alpha \rvert ^2+ \lvert \beta \rvert ^2= 1^2 + 1^2 \neq 1.$ Since we view a quantum state as a probability distribution, the squares of the amplitudes must sum to 1. So, we must \textit{normalize} the matrix, to get 
\begin{align*}
\widehat{H} = 
\frac{1}{\sqrt{2}}
\begin{bmatrix}
1 & 1\\
1 & 1 
\end{bmatrix}
\end{align*}
Now, applying the transformation to our basis vectors, we get $\widehat{H} \ketz =(\ketz + \keto)/\sqrt{2}$ and $\widehat{H} \keto =(\ketz + \keto)/\sqrt{2}$. There is still a problem with this transformation. All quantum transformations must be \textit{reversible}, but we have mapped the two basis vectors to the same state, so the transformation is not reversible. To fix this, we will simply change the image of this transformation under \keto:
\begin{align*}
H = \hadamard
\end{align*}

The quantum gate above ($H$) is called the \textit{Hadamard transform} or \textit{Hadamard gate}. We will precisely define this quantum gate later in this chapter, as it is absolutely critical to work in quantum algorithms. It acts on the basis vectors as follows: $H\ketz = (\ketz + \keto)/\sqrt{2}$ and $H\keto = (\ketz - \keto)/\sqrt{2}$. 

Both of these states are in an equal superposition between \ketz and \keto, but they are distinct states. These states come up rather often, so they have been given the special shorthand notations \Hplus and \Hminus, respectively. 

\begin{definition}[Plus and Minus States] The following states (pronounced ``plus'' and ``minus'') will be used ubiquitously throughout this thesis in their shorthand form:
$$\Hplus = \frac{1}{\sqrt{2}}(\ketz + \keto)$$
$$\Hminus = \frac{1}{\sqrt{2}}(\ketz - \keto)$$
\end{definition}
The requirements that quantum transformations be reversible and normalized are encapsulated by the property that the matrix representation for any transformation must be \textit{unitary}. That is, $U^{\dagger} U = I$ where $U^{\dagger}$ is the transpose of the complex conjugate of $U$ and $I$ is the 2-by-2 identity matrix. Within those requirements, we can construct any transformation we like. What about transforming multiple qubits?

\section{Multiple Qubits}

To represent multiple qubits, we expand our two quantum states to many using tensor products \footnote{See appendix on tensor products--not yet written}. For example, in a two qubit system, we have the following basis states:
$$ \ketz \otimes \ketz,~\ketz \otimes \keto,~\keto \otimes \ketz, \text{ and } \keto \otimes \keto.$$ 
These four states can equivalently be written as:
$$ \ket{00},~\ket{01},~\ket{10}, \text{ and } \ket{11}.$$
Thus, any two-qubit state can be represented as a linear combination of these basis states, namely $\ket{\psi} = \alpha_{00}\ket{00}+ \alpha_{01}\ket{01} + \alpha_{10}\ket{10} + \alpha_{11}\ket{11}.$ In general, an $n$-qubit state can be represented as 
\begin{equation*}
\ket{\psi} = \sum_{i \in \Z_2^n} \alpha_{i}\ket{i},
\end{equation*}
where $i$ is the binary representation of the numbers $0, \ldots, n-1.$ The probability of observing a state \ket{i} upon measuring \ket{\psi} is $\lvert \alpha_{i} \rvert ^2.$ 

We may measure qubits individually as well. For example, measuring just the first qubit gives us 0 with probability 
\begin{equation*}
 \sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2,
\end{equation*}
leaving the post-measurement state
\begin{equation} \label{post_measurement}
 \ket{\psi'} = \frac{\sum_{i \in \Z_2^{n-1}} \alpha_{0i} \ket{\alpha_{0i}}}{\sqrt{\sum_{i \in \Z_2^{n-1}} \lvert \alpha_{0i} \rvert ^2}}.
\end{equation}

\section{Entangled States}

Something that follows from the post-measurement equation (\ref{post_measurement}) but which is not self-evident is the concept of \textit{entangled states}. Consider the following state:
\begin{equation*}
\ket{\beta_{00}} = \frac{\ket{00} + \ket{11}}{\sqrt{2}}
\end{equation*}
Suppose we measured the first qubit. We will observe \ket{0} with probability 1/2, and the resulting post-measurement state is: \ket{\beta_{00}'} = \ket{00} (by the equation above). How can this be? We only measured the first qubit, and yet, we now have information about both qubits. This is precisely because the state is entangled. Another example of an entangled state is:
\begin{equation*}
\ket{\beta_{01}} = \frac{\ket{01} + \ket{10}}{\sqrt{2}}
\end{equation*}
These two states, \ket{\beta_{00}} and \ket{\beta_{01}}, are the first two \textit{Bell states} or \textit{EPR pairs}. Quantum entanglement is a powerful computational tool and will be used in many quantum algorithms in the rest of the text. 

Now that we have the means to represent an $n$-qubit state, the state transformations can be represented by $2^n$-by-$2^n$ matrices. In particular, the $n$-qubit Hadamard transform is $H^{\otimes n} = H \otimes H \otimes \ldots \otimes H$, i.e.~$n$ tensor products.

\section{Hadamard Transform}

As promised, a more precise definition of the Hadamard transform follows. This is by far the most important unitary transformation in the field of quantum algorithms.

\begin{definition}[Hadamard Transform] The n-qubit Hadamard transform, $H^{\otimes n}$ acts on a state $\ket{\psi}$ as follows:
\begin{equation*}
H^{\otimes n} \ket{\psi} = \frac{1}{\sqrt{2^n}} \sum_{j \in \{0,1\}^n} (-1)^{\psi \cdot j} \ket{j}
\end{equation*}
where $\psi$ and $j$ are $n$-bit binary numbers.
\end{definition}
Upon first glance, this formula is quite uninviting, so I will provide a quick guide for some common states. 
\begin{align*}
& H^{\otimes n} \ketz^{\otimes n} = \Hplus^{\otimes n} \\
& H^{\otimes n} \keto^{\otimes n} = \Hminus^{\otimes n} \\
\end{align*}
Since $H = H^{-1}$, we also have that
\begin{align*}
& H^{\otimes n} \Hplus^{\otimes n} =  \ketz^{\otimes n} \\
& H^{\otimes n} \Hminus^{\otimes n} = \keto^{\otimes n}. \\
\end{align*}
This concludes the section on the Hadamard transform, though the careful (or confused) reader will find that they will have to refer back to this section in the coming chapters as the Hadamard transform is central to this thesis. 

\section{Phase}
Up to this point, we have used the term \textit{phase} slightly ambiguously. The exact meaning of the word differs depending on context, so we will flesh these out. In fact, some of the original results in this thesis puzzled both student and adviser until we revisited the exact distinction between \textit{global phase} and \textit{relative phase}.

Consider the states $\ket{\psi}$ and $e^{i \theta}\ket{\psi}$,\footnote{HOW TO FORMAT EULERS NUMBER} where $\ket{\psi}$ is a state vector and $\theta$ is a real number. These two phases are equivalent up to their \textit{global phase shift}. As a thought experiment, one may wonder what happens upon measuring the two states. Suppose $M_x$ is our measurement operator. Then, the probabilities for $x$ occurring upon measurement are: $\bra{\psi}M_x^\dagger M_x \ket{\psi}$ and $\bra{\psi}e^{-i\theta}M_x^\dagger M_x e^{i\theta}\ket{\psi} = \bra{\psi}M_x^\dagger M_x \ket{\psi}.$ So, the two states are indistinguishable with respect to measurements. 

The other notion of phase that will come up is \textit{relative phase}. Consider the states $\Hplus = (\ketz + \keto)/\sqrt{2}$ and $\Hminus = (\ketz- \keto)/\sqrt{2}$. These two states have the same \textit{magnitude} on their amplitudes (i.e.~$1/\sqrt{2}$) on both $\ketz$ and $\keto$. Thus, they have the same \textit{relative phase}. However, we can distinguish between the states $\Hplus$ and $\Hminus$ if we apply a 45 degree clockwise rotation to the states, we may distinguish between the states with probability 1 upon measurement. So, despite having the same relative phase, the states are, in fact, computationally distinct.

\section{Universal Quantum Gate}
In classical computation, there are three basic logic gates: NOT, AND, and OR. We can combine these gates to represent any possible logical expression. Furthermore, the NAND gate and the NOR gate, which we get from taking the negation (NOT) of the output of AND and OR respectively, are said to be \textit{universal gates}. This means that we can construct the three basic logic gates just from NAND gates or just from NOR gates. So, NAND and NOR are universal in that any logical expression can be represented with just NAND or NOR circuits. This is very useful in practice because NAND gates are very cheap to construct, so using only NANDs can keep the cost of a computer chip down.

A natural question, then, is whether there exists a quantum analogue, a universal quantum gate. The short answer is that there is no one universal quantum gate, however the following three gates are enough to make an \textit{arbitrary approximation} of any quantum gate: Hadamard, CNOT, and phase gate. 

The phase gate is defined
\begin{align}
\begin{bmatrix}
1 & 0\\
0 & i 
\end{bmatrix}
\end{align}
And the CNOT, or controlled-NOT gate acts on two qubits by flipping the second qubit if and only if the first qubit is a 1. 
\begin{align}
\text{CNOT} = \begin{bmatrix}
1 & 0 & 0 & 0 \\
0 & 1 & 0 & 0 \\
0 & 0 & 0 & 1 \\
0 & 0 & 1 & 0
\end{bmatrix}
\end{align}
As you can see, by multiplying the CNOT matrix by the column vector $(a\quad b\quad c\quad d)^\intercal$ this takes $a\ket{00} + b\ket{01} + c\ket{10} + d\ket{11}$ and transforms it into $a\ket{00} + b\ket{01} + c\ket{11} + d\ket{10}$, thereby swapping the amplitudes on the states $\ket{10}$ and $\ket{11}$, or equivalently ``flipping'' the second bit of the state if the first bit is a 1. 

The \textit{gate complexity} of a unitary transformation is the minimum number of gates needed to implement the circuit. 

\section{No-Cloning Theorem}

Suppose my friend has a secret state \ket{\psi} that she wants me to have a copy of.  Classically, we could easily build a circuit to copy her state $x = 0$ or $x = 1$ without measuring it. We would simply initialize a temporary register to 0, and our circuit would write $x \oplus 0$, the XOR of $x$ and 0, to the destination register. 

The quantum case is a bit trickier. Suppose \ket{\psi} is only known to our friend. We wish to copy this exact state into a target slot, which starts out in some standard known state \ket{s}. Thus, the initial state of our copying machine is $\ket{\psi} \otimes \ket{s}$. Now, we apply some unitary operation $U$ to these registers to obtain
\begin{equation}
\ket{\psi} \otimes \ket{s} \xrightarrow{U} U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
Now, suppose this quantum copying circuit works for two arbitrary states \ket{\psi} and \ket{\phi}. Then, we have
\begin{equation}
U(\ket{\psi} \otimes \ket{s}) = \ket{\psi} \otimes \ket{\psi}
\end{equation}
and
\begin{equation}
U(\ket{\phi} \otimes \ket{s}) = \ket{\phi} \otimes \ket{\phi}
\end{equation}
Now, taking the inner product of these two equations gives\footnote{Check this math???} 
\begin{align}
(\bra{\psi} \otimes \bra{s})U^{\dagger}U \ket{\phi} \otimes \ket{s}
& = \inner{\psi}{\phi} \otimes \inner{s}{s} \otimes I \\
 & = \inner{\psi}{\phi}\\
& = (\bra{\psi}\otimes \bra{\psi})(\ket{\phi}\otimes \ket{\phi}) \\
& = \inner{\psi}{\phi} \otimes \inner{\psi}{\phi} \\
& = (\inner{\psi}{\phi})^2
\end{align}
Line (5) holds because $s$ is normalized. Critically, lines (5) and (8) give
\begin{equation*}
 \inner{\psi}{\phi} = (\inner{\psi}{\phi})^2.
\end{equation*}
However, this equation is only true if $\ket{\psi} = \ket{\phi}$ or if $\ket{\psi}$ is orthogonal to $\ket{\phi}$. Thus, a general cloning device can only clone states that are orthogonal, which means that we cannot clone states which we know nothing about. 

This is an important fundamental difference between the classical and quantum models of computation. We take for granted in classical computing that we can copy any unknown states, and much of classical computation relies upon this fact. 

\section{Quantum Oracles}

Within any computational model, some computations are ``expensive'' for any number of reasons: they require large amounts of resources such as time, space, or circuitry. For this reason, one may wish to outsource such computations to third parties, which we will call \textit{oracles}. As the name would suggest, oracles may be queried on inputs, and magically in one time step will output the result of the computation on the input, for a particular function. More concretely, an oracle $\mathcal{O}_f$ outputs $f(x)$ when queried on the input $x$.

Classical oracles are used throughout computer science theory to abstract computations and reason about algorithms and protocols. Quantum oracles differ  from classical oracles in significant ways. 

\subsection{Quantum Queries}
First, consider the action of the query. In order to ensure the reversibility of quantum queries, we must maintain two registers: a query register, and a response register. The query register contains the input $x$ on which we wish to query the oracle. This register remains untouched by the oracle. The response register has some initial state $\ket{r}$ and after the quantum query takes place, has the state $\ket{r\oplus f(x)}$ where $f$ is the function that the oracle computed. 

\begin{definition}[Quantum Oracle Query] A quantum oracle has the following action on the query and response registers $\mathcal{O}_f : \ket{x, r} \rightarrow \ket{x, r \oplus f(x)}$.
\end{definition}

In addition to the query and response registers, quantum oracles have the special ability to take as input a \textit{superposition of queries} and output a \textit{superposition of responses}. More precisely, suppose we have a superposition of queries of length $n$ 
\begin{equation*}
\ket{x} = \sum_{i \in \Z_2^n} \alpha_i \ket{i}
\end{equation*}

and $n$ response registers $\ket{r} = \ket{r_1} \otimes \ldots \ket{r_n}$. Then, the query $\mathcal{O}(\ket{x, r})$ results in the following state
\begin{equation*}
\ket{x} = \sum_{i \in \Z_2^n} \alpha_i \ket{i, r_i \oplus f(i)}.
\end{equation*}
It is important to note that this only constitutes \textit{one} quantum query to the oracle.


\section{Phase-kickback Trick}

Suppose we have ``black-box access'' to some function $f$. That is, we can query $f$ on some input $x$ and it will compute $f(x)$. We record the output in a \textit{response register}, the second qubit in the example below; the first qubit is called the \textit{query register}. This can be modeled as the following unitary transformation:
\begin{equation*}
\ket{x, z} \xrightarrow{f} \ket{x, z \oplus f(x)}
\end{equation*}
Given $z$, we can deduce what $f(x)$ is by taking $z \oplus f(x) \oplus z = f(x)$, addition mod 2. This is essential to keeping the transformation unitary. 

One common query method is using what is called the \textit{phase kickback trick}. The basic idea is that we initialize the response register to \Hminus so that both the query and response registers stay the same after the query, and the value of $f(x)$ is encapsulated by the phase of the state. More concretely, we have:
\begin{align*}
\ket{x} \otimes \Hminus 
& = \ket{x} \otimes \frac{1}{\sqrt{2}}(\ketz - \keto)\\
& = \frac{1}{\sqrt{2}}(\ket{x, 0} - \ket{x, 1}) \\
& \xrightarrow{f} \frac{1}{\sqrt{2}}(\ket{x, f(x)} - \ket{x, 1 \oplus f(x)} \\
& = \ket{x} \otimes \frac{1}{\sqrt{2}}\bigg(\ket{f(x)} - \ket{\overline{f(x)}}\bigg) \\
& = (-1)^{f(x)} \ket{x} \otimes \ket{ -}.
\end{align*}
Since the response register remains unchanged, we will generally omit this from future computations, though technically it must be present to preserve the reversibility of the query. 
\section{Bernstein-Vazirani Algorithm}

Now that we have seen a few tricks of the trade, we will dive into a quantum algorithm. The Bernstein-Vazirani algorithm forms the foundation for many of the algorithms in the rest of this thesis, so a solid grasp of this section will yield high dividends. 

[CITATION TO VAZIRANI PAPER] The Bernstein-Vazirani algorithm solves the following problem: for $N = 2^n$, we are given $x \in \{0,1\}^N$ with the property that there exists some unknown $a \in \{0,1\}^n$ such that $x_i = i \cdot a$ mod 2. The goal is to find $a$. In other words, $x_i$ is the $i$th bit of the binary representation of $x$. 

First, an overview of the algorithm: we will start in the state $\ketz^{\otimes n}$, the $n$-qubit zero state, and apply a $n$-qubit Hadamard transform to get an equal superposition of the states, i.e.~ $\Hplus^{\otimes n}$. Next, we perform a quantum query using the phase kickback trick to store the bits of $x$ in the phase of our state. Then, another Hadamard transform on all $n$ qubits. With a simple measurement, we obtain $a$ with probability 1. Now, for the details.

\begin{align}
 \ketz^{\otimes n}
& \xrightarrow{H^{\otimes n}}\frac{1}{\sqrt{2^n}} \sum_{i \in \{0,1\}^n} \ket{i} \\
& \xrightarrow{\mathcal{O}_x} \frac{1}{\sqrt{2^n}} \sum_{i \in \{0,1\}^n} (-1)^{x_i}\ket{i}\\
& \xrightarrow{H^{\otimes n}} \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{x_i} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j} \label{confusingline}\\
& = \ket{a}
\end{align}
This computation looks a mess for the beginner, so let's try to make sense of it. The first three lines follow from the definitions of the functions (confused readers may need to review the defined action of $H$ and $\mathcal{O}$ with phase-kickback). The last step, however, is less clear-cut. Note that $(-1)^{x_i} = (-1)^{(i \cdot a) \text{ mod } 2} = (-1)^{(i \cdot a)}$, given by $x_i = (i \cdot a)$ mod 2 in the problem statement. Thus, we may manipulate \ref{confusingline} as follows:
\begin{align}
 \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{x_i} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j}
& = \frac{1}{2^n} \sum_{i \in \{0,1\}^n} (-1)^{(i \cdot a)} \sum_{j \in \{0,1\}^n} (-1)^{i \cdot j} \ket{j} \\
& = \frac{1}{2^n}  \sum_{j \in \{0,1\}^n}\sum_{i \in \{0,1\}^n} (-1)^{i \cdot(a + j)}  \ket{j} \label{equals_a}
\end{align}
Now, note that $(-1)^{i \cdot(a + j)}$ is equal to 1 if and only if $j = a$. Thus, the inner summation evaluates to $1/2^n$ when $j =a$ and zero otherwise. So, \ref{equals_a} is simply the state $\ket{a}$; thus, measurement in the computational basis will yield $a$ with probability 1. This algorithm is the bread and butter of this thesis, so to speak, and is critical to the decoding algorithms we will study in a few chapters. 
\section{A Note on Asymptotic Complexity}

In this thesis, we will be studying the asymptotic complexity of various different metrics, including: runtime, query complexity, certificate complexity, block sensitivity, and perhaps circuit complexity\footnote{These complexity metrics will be defined later.}. Those who are unfamiliar with asymptotic complexity (also termed Landau-Bachmann notation in mathematics) should review the appendix on the topic. 


\chapter{Introduction to Coding Theory}	

Coding theory and error-correcting codes were developed to enable the reliable transmission of messages over noisy communication channels. When sending bits over the internet, suppose that occasionally that bits get flipped in transit. The receiver wants some means of correcting these errors, and this is precisely what error-correcting codes are for.

Error-correcting codes are used in the classical setting, for example in CDs (if anyone still uses CDs) to prevent data loss in the event of a scratch and long-distance space communications with rovers and satellites. An interesting aspect of coding theory that we will not cover is that different codes have been optimized for different noise models.

Error-correcting codes are important for the practical implementation of quantum computers because current physical implementations of quantum computers are plagued by \textit{quantum decoherence}. That is, the qubits are difficult to fully isolate from their environment and thus become entangled with their surroundings, leading to noisy computations. For now, we will forget about quantum computing and plunge into the theory of error-correcting codes.

\section{Preliminaries}
As was mentioned, we consider the scenario that we are transmitting a message over a noisy communication channel. More specifically, we will look at \textit{binary symmetric channels}.

\begin{definition}[Binary Symmetric Channel] A binary symmetric channel is a classical channel that can transmit a string of 0's and 1's. If a bit $b$ is sent, $b$ will be flipped to $\lnot b$ with probability $p$, and $b$ will be transmitted correctly with probability $1-p$ where $p < \frac{1}{2}$.
\end{definition}

This type of channel is \textit{binary} because we are transmitting bits, and it is \textit{symmetric} because there is an equal probability of a 0 flipping to a 1 and the reverse. Note that if our binary symmetric channel flips bits with probability $p > 1/2$, we could easily create a channel with $p <1/2$ by negating every bit on the receiving or sending end. Additionally, note that if $p=1/2$, it is  information-theoretically impossible to recover the message, so we don't consider this case and frankly it is dubious to even call that a communication channel.

In the following sections we will investigate how to mitigate the information loss from these random errors in binary symmetric channels where $p<1/2$.

\section{Encoding and Decoding}
The basic idea behind error-correcting codes is that we will systematically introduce redundancy to the message we want to send; this process is called \textit{encoding}.

\begin{definition}[Message, encoding] We will denote our message $m$ as $m_0m_1m_2 \ldots m_{k-1}$ where $m_i \in \{0,1\} \text{ for all }0 \leq i \leq k-1$. Next, we will denote the encoding of our message $m$, called a codeword, $E(m) = u_0u_1u_2 \ldots u_{n-1}$ where $n > k$ and $u_i \in \{0,1\} \text{ for all }0 \leq i \leq n-1$. 
\end{definition}


\chapter{Introduction to Learning Theory}
Read Ronald de Wolf's thesis section on query complexity! pg. 193
\section{Basics of Concept Learning}
\section{Certificate Complexity}
\section{Block Sensitivity}

\chapter{Simplex Code}
\section{Encoding}
\section{Quantum Decoding Algorithm}
\section{Rosbustness to Errors Metric}

\chapter{Reed-Muller Codes}
\section{Boolean Functions}
\section{Encoding}
\section{Classical Decoding Algorithm}
Projective geometry description


\chapter{Quantum Decoding Algorithm for Reed-Muller Codes}
\section{Multivariate Polynomial Interpolation}
\section{Stateful Noisy Oracle}
\section{Stateless Noisy Oracle}

\chapter*{Conclusion}
         \addcontentsline{toc}{chapter}{Conclusion}
	\chaptermark{Conclusion}
	\markboth{Conclusion}{Conclusion}
	\setcounter{chapter}{6}
	\setcounter{section}{0}
	
Here's a conclusion, demonstrating the use of all that manual incrementing and table of contents adding that has to happen if you use the starred form of the chapter command. The deal is, the chapter command in \LaTeX\ does a lot of things: it increments the chapter counter, it resets the section counter to zero, it puts the name of the chapter into the table of contents and the running headers, and probably some other stuff. 

So, if you remove all that stuff because you don't like it to say ``Chapter 4: Conclusion'', then you have to manually add all the things \LaTeX\ would normally do for you. Maybe someday we'll write a new chapter macro that doesn't add ``Chapter X'' to the beginning of every chapter title.

\section{More info}
And here's some other random info: the first paragraph after a chapter title or section head \emph{shouldn't be} indented, because indents are to tell the reader that you're starting a new paragraph. Since that's obvious after a chapter or section title, proper typesetting doesn't add an indent there. 


%If you feel it necessary to include an appendix, it goes here.
    \appendix
    	\chapter{Asymptotic Complexity}
    	Computer scientists are very interested in how long their algorithm or function will take to run on given inputs. The crudest, most theoretical, runtime analysis looks at the \textit{asymptotic runtime} of an algorithm. In practice, the actual runtime can be affected significantly by any number of factors like the operating system and memory  latency, but for our purposes the 
\textit{asymptotic} runtime is the only metric we will care about (particularly because current existent quantum computers are rather rudimentary).

The basic idea behind asymptotic runtime analysis is that we consider how long our algorithm takes to compute as a function of the \textit{length} (number of bits) of the input, where the time interval is calculated as the number of basic operations required. Classically, the basic operations are generally considered to be the ones that the Arithmetic Logic Unit (ALU) in the CPU can do. These include: addition, subtraction, multiplication, division, boolean comparisons, and variable assignment. Quantumly, the basic operations are the same as above in addition to arbitrary unitary transformations and oracle queries (which take one time step). 

Now that we have the basic idea, time to get technical. We define notions of relative asymptotic growth rates of functions: $O, \Omega, o, \omega$, and $\Theta$. We will define these notions rigorously below, but for all practical purposes, the following distinctions are sufficient. Suppose we have two functions $f(n)$ and $g(n)$. Then, 
\begin{itemize}
\item We say $f(n) = O(g(n))$,\textit{``$f$ is big-oh of $g$,''} if $f(n) \leq g(n)$ as $n$ gets large;
\item $f(n) = o(g(n))$,\textit{``$f$ is little-oh of $g$,''} if $f(n) < g(n)$ (strictly) as $n$ gets large;
\item $f(n) = \Omega(g(n))$,\textit{``$f$ is big-omega of $g$,''} if $f(n) \geq g(n)$ as $n \rightarrow \infty$;
\item $f(n) = \omega(g(n))$,\textit{``$f$ is little-omega of $g$,''} if $f(n) > g(n)$ (strictly) as $n$ gets large.
\item Lastly, $f(n) = \Theta(g(n))$, \textit{``$f$ is theta of $g$,''} if $f(n) = g(n)$ (up to constant factors) as $n$ gets large.
\end{itemize}
The above imprecise definitions are rigorous enough for our discussions; however, I present the formal definitions below for the overly pedantic or curious reader. 
 
\begin{theorem}
A function f(n) is O(g(n)), denoted f(n) = O(g(n)), if there exist $c>0, n_0>0$ such that
$$f(n) \leq c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is o(g(n)), denoted f(n) = o(g(n)), if there exists $n_0>0$ such that for all $c>0$,
$$f(n) < c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\Omega(g(n))$, denoted $f(n) = \Omega(g(n))$, if there exist $c>0, n_0>0$ such that
$$f(n) \geq c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\omega(g(n))$, denoted $f(n) = \omega(g(n))$, if there exists $n_0>0$ such that for all $c>0$,
$$f(n) > c g(n) \quad \forall n\geq n_0.$$
\end{theorem}

\begin{theorem}
A function f(n) is $\Theta(g(n))$, denoted $f(n) = \Theta(g(n))$, if f(n) = O(g(n)) and g(n) = O(f(n)).
\end{theorem}

Determining the exact values of $n_0$ and $c$ is not necessary. Instead, we will present a few heuristics:
\begin{enumerate}
\item Constant factors don't matter. 
\item Only the largest complexity class matters if we are adding terms.
\end{enumerate}
For example, $f(n)= 25n^3 + 170000n$ is $O(n^{17})$ because $n^3 \leq n^{17} $ as $n$ grows, and $f(n) = \Theta(n^3)$ because the $170000n$ doesn't matter as $n$ grows and we don't care about the constant factor of 25 either, and $f(n) = \omega(n^2)$, because $n^2 < n^3$ as $n$ goes to infinity.

	
\chapter{Tensors}
\chapter{Finite Geometries}
\chapter{TODO List}
      \section{Chapter 1: Quantum}
      Finish section on phase
      \section{Chapter 2: Coding Theory}
      Write this section
      \section{Chapter 3: Learning Theory}
      Write this section
      Include original proofs
      \section{Chapter 4: Simplex Code}
      Write this section
      Include cool formula/proof :)
      \section{Chapter 5: Reed-Muller Codes}
      Write this section (maybe write after section on finite geometries)
      \section{Chapter 6: Quantum Decoding for RM Codes}
      Write this section
      \section{Appendices}
      	\subsection{Asymptotic Complexity}
      	Write this to be more general for runtime and query complexity.
      	\subsection{Tensors}
      	Write this appendix
      	\subsection{Finite Geometries}
      	Write this appendix


%This is where endnotes are supposed to go, if you have them.
%I have no idea how endnotes work with LaTeX.

  \backmatter % backmatter makes the index and bibliography appear properly in the t.o.c...

% if you're using bibtex, the next line forces every entry in the bibtex file to be included
% in your bibliography, regardless of whether or not you've cited it in the thesis.
    \nocite{*}

% Rename my bibliography to be called "Works Cited" and not "References" or ``Bibliography''
% \renewcommand{\bibname}{Works Cited}

%    \bibliographystyle{bsts/mla-good} % there are a variety of styles available; 
%  \bibliographystyle{plainnat}
% replace ``plainnat'' with the style of choice. You can refer to files in the bsts or APA 
% subfolder, e.g. 
 \bibliographystyle{APA/apa-good}  % or
 \bibliography{thesis}
 % Comment the above two lines and uncomment the next line to use biblatex-chicago.
 %\printbibliography[heading=bibintoc]

% Finally, an index would go here... but it is also optional.
\end{document}
